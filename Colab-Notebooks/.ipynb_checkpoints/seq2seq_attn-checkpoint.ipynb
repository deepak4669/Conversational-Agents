{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/deepak4669/Conversational-Agents/blob/master/Colab-Notebooks/seq2seq_attn.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 41558,
     "status": "ok",
     "timestamp": 1590910123353,
     "user": {
      "displayName": "deepak goyal",
      "photoUrl": "https://lh5.googleusercontent.com/-3gfEZruD9Sk/AAAAAAAAAAI/AAAAAAAACvg/54H4lMsdOgA/s64/photo.jpg",
      "userId": "05164064759516400423"
     },
     "user_tz": -330
    },
    "id": "3JqhjAnVSaCY",
    "outputId": "8ae629bd-5e6a-415e-ae99-6558b1b95b07"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-d5df0069828e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdrive\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/content/drive'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4351,
     "status": "ok",
     "timestamp": 1590910132240,
     "user": {
      "displayName": "deepak goyal",
      "photoUrl": "https://lh5.googleusercontent.com/-3gfEZruD9Sk/AAAAAAAAAAI/AAAAAAAACvg/54H4lMsdOgA/s64/photo.jpg",
      "userId": "05164064759516400423"
     },
     "user_tz": -330
    },
    "id": "8LceekfUUWxL",
    "outputId": "6b1076e9-11d2-459c-bab9-00a4b86e1c8f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'nvidia-smi' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "# !nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CRbeP6r1VmTD"
   },
   "outputs": [],
   "source": [
    "# Pre-Processing\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "# Model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# For visualising metrics\n",
    "# from visdom import Visdom\n",
    "\n",
    "# For visualising gradients plot\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "import copy\n",
    "import math\n",
    "import time\n",
    "from collections import namedtuple, Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4029,
     "status": "ok",
     "timestamp": 1590910139201,
     "user": {
      "displayName": "deepak goyal",
      "photoUrl": "https://lh5.googleusercontent.com/-3gfEZruD9Sk/AAAAAAAAAAI/AAAAAAAACvg/54H4lMsdOgA/s64/photo.jpg",
      "userId": "05164064759516400423"
     },
     "user_tz": -330
    },
    "id": "5i5yWtmBH7LU",
    "outputId": "bb7535eb-56d6-40d8-87ac-81526969ab2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The device found: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device=torch.device(\"cpu\")\n",
    "print(\"The device found: \" + str(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9D390QjbICa9"
   },
   "outputs": [],
   "source": [
    "def plot_grad_flow(named_parameters):\n",
    "    \"\"\"\n",
    "        Plotting gradient flow across various layers\n",
    "        Thanks to: https://discuss.pytorch.org/t/check-gradient-flow-in-network/15063/2\n",
    "    \"\"\"\n",
    "    ave_grads = []\n",
    "    layers = []\n",
    "    for n, p in named_parameters:\n",
    "        if (p.requires_grad) and (\"bias\" not in n):\n",
    "            layers.append(n)\n",
    "            ave_grads.append(p.grad.abs().mean())\n",
    "    plt.plot(ave_grads, alpha=0.3, color=\"b\")\n",
    "    plt.hlines(0, 0, len(ave_grads) + 1, linewidth=1, color=\"k\")\n",
    "    plt.xticks(range(0, len(ave_grads), 1), layers, rotation=\"vertical\")\n",
    "    plt.xlim(xmin=0, xmax=len(ave_grads))\n",
    "    plt.xlabel(\"Layers\")\n",
    "    plt.ylabel(\"average gradient\")\n",
    "    plt.title(\"Gradient flow\")\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z565czS1IFdU"
   },
   "source": [
    "# Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3125,
     "status": "ok",
     "timestamp": 1590910139205,
     "user": {
      "displayName": "deepak goyal",
      "photoUrl": "https://lh5.googleusercontent.com/-3gfEZruD9Sk/AAAAAAAAAAI/AAAAAAAACvg/54H4lMsdOgA/s64/photo.jpg",
      "userId": "05164064759516400423"
     },
     "user_tz": -330
    },
    "id": "c9AkyMgfWetG",
    "outputId": "94019e74-0f1a-47e6-e73a-419b2c31ee2c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The final data corpus folder: /content/drive/My Drive/Data/cornell movie-dialogs corpus\n"
     ]
    }
   ],
   "source": [
    "path = \"/content/drive/My Drive/Data\"\n",
    "dataset = \"cornell movie-dialogs corpus\"\n",
    "\n",
    "data_folder = os.path.join(path, dataset)\n",
    "\n",
    "print(\"The final data corpus folder: \" + str(data_folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sNFBnhpYWhB4"
   },
   "outputs": [],
   "source": [
    "def get_lines_conversations():\n",
    "    \"\"\"\n",
    "    Loads movie lines and conversations from the dataset.\n",
    "    \n",
    "    data_folder: Destination where conversations and lines are stored.\n",
    "    \n",
    "    movie_lines: Consist of movie lines as given by the dataset.\n",
    "    movie_conversations: Consist of movie conversations as given by the dataset.\n",
    "    \n",
    "    \"\"\"\n",
    "    movie_lines = []\n",
    "    movie_conversations = []\n",
    "\n",
    "    with open(\n",
    "        os.path.join(data_folder, \"movie_lines.txt\"), \"r\", encoding=\"iso-8859-1\"\n",
    "    ) as f:\n",
    "        for line in f:\n",
    "            movie_lines.append(line)\n",
    "\n",
    "    with open(\n",
    "        os.path.join(data_folder, \"movie_conversations.txt\"), \"r\", encoding=\"iso-8859-1\"\n",
    "    ) as f:\n",
    "        for line in f:\n",
    "            movie_conversations.append(line)\n",
    "\n",
    "    return movie_lines, movie_conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4584,
     "status": "ok",
     "timestamp": 1590910141553,
     "user": {
      "displayName": "deepak goyal",
      "photoUrl": "https://lh5.googleusercontent.com/-3gfEZruD9Sk/AAAAAAAAAAI/AAAAAAAACvg/54H4lMsdOgA/s64/photo.jpg",
      "userId": "05164064759516400423"
     },
     "user_tz": -330
    },
    "id": "_j92fLQrW1pq",
    "outputId": "d1ea97b0-bacd-4f0f-e3a1-9a44afb6b3d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting movie lines and movie conversations...\n",
      "Number of distinct lines: 304713\n",
      "Number of conversations: 83097\n",
      "Average Number of lines per conversations: 3.6669554857576085\n",
      "L1045 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ They do not!\n",
      "\n",
      "u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L194', 'L195', 'L196', 'L197']\n",
      "\n",
      "Extracting took place in: 2.2381722927093506\n"
     ]
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "print(\"Extracting movie lines and movie conversations...\")\n",
    "movie_lines, movie_conversations = get_lines_conversations()\n",
    "\n",
    "print(\"Number of distinct lines: \" + str(len(movie_lines)))\n",
    "print(\"Number of conversations: \" + str(len(movie_conversations)))\n",
    "print(\n",
    "    \"Average Number of lines per conversations: \"\n",
    "    + str(len(movie_lines) / len(movie_conversations))\n",
    ")\n",
    "\n",
    "print(movie_lines[0])\n",
    "print(movie_conversations[0])\n",
    "\n",
    "print(\"Extracting took place in: \" + str(time.time() - t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eG0A5czeXz9H"
   },
   "outputs": [],
   "source": [
    "exceptions = []\n",
    "\n",
    "\n",
    "def loadLines(movie_lines, fields):\n",
    "    lines = {}\n",
    "    for lineid in range(len(movie_lines)):\n",
    "\n",
    "        line = movie_lines[lineid]\n",
    "        values = line.split(\" +++$+++ \")\n",
    "\n",
    "        lineVals = {}\n",
    "\n",
    "        # print(\"values\"+str(len(values)))\n",
    "        # print(\"fields\"+str(len(fields)))\n",
    "\n",
    "        for i, field in enumerate(fields):\n",
    "            try:\n",
    "                lineVals[field] = values[i]\n",
    "            except:\n",
    "                print(\"Exception: \" + str(len(values)))\n",
    "                exceptions.append(lineid)\n",
    "\n",
    "        lines[lineVals[\"lineID\"]] = lineVals\n",
    "\n",
    "    return lines\n",
    "\n",
    "\n",
    "def loadConversations(movie_conversations, lines, fields):\n",
    "    conversations = []\n",
    "\n",
    "    for convo in movie_conversations:\n",
    "        values = convo.split(\" +++$+++ \")\n",
    "        conVals = {}\n",
    "\n",
    "        for i, field in enumerate(fields):\n",
    "            conVals[field] = values[i]\n",
    "\n",
    "        lineIDs = eval(conVals[\"utteranceIDs\"])\n",
    "\n",
    "        conVals[\"lines\"] = []\n",
    "\n",
    "        for lineID in lineIDs:\n",
    "            conVals[\"lines\"].append(lines[lineID])\n",
    "        conversations.append(conVals)\n",
    "\n",
    "    return conversations\n",
    "\n",
    "\n",
    "def sentencePairs(conversations):\n",
    "    qr_pairs = []\n",
    "\n",
    "    for conversation in conversations:\n",
    "        for i in range(len(conversation[\"lines\"]) - 1):\n",
    "            query = conversation[\"lines\"][i][\"text\"].strip()\n",
    "            response = conversation[\"lines\"][i + 1][\"text\"].strip()\n",
    "\n",
    "            if query and response:\n",
    "                qr_pairs.append([query, response])\n",
    "\n",
    "    return qr_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5629,
     "status": "ok",
     "timestamp": 1590910143948,
     "user": {
      "displayName": "deepak goyal",
      "photoUrl": "https://lh5.googleusercontent.com/-3gfEZruD9Sk/AAAAAAAAAAI/AAAAAAAACvg/54H4lMsdOgA/s64/photo.jpg",
      "userId": "05164064759516400423"
     },
     "user_tz": -330
    },
    "id": "Bh37EXhxYQAJ",
    "outputId": "592e7cc8-c706-4187-8326-7ae4f3761489"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Separating meaningfull information for our model...\n",
      "The number of query-response pairs are: 221282\n",
      "Separation took place in: 2.472775459289551\n"
     ]
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "print(\"Separating meaningfull information for our model...\")\n",
    "\n",
    "lines = {}\n",
    "conversations = []\n",
    "qr_pairs = []\n",
    "\n",
    "movie_lines_fields = [\"lineID\", \"characterID\", \"movieID\", \"character\", \"text\"]\n",
    "movie_convo_fields = [\"charcaterID\", \"character2ID\", \"movieID\", \"utteranceIDs\"]\n",
    "\n",
    "lines = loadLines(movie_lines, movie_lines_fields)\n",
    "conversations = loadConversations(movie_conversations, lines, movie_convo_fields)\n",
    "qr_pairs = sentencePairs(conversations)\n",
    "\n",
    "print(\"The number of query-response pairs are: \" + str(len(qr_pairs)))\n",
    "print(\"Separation took place in: \" + str(time.time() - t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p5vYOofuzsaP"
   },
   "outputs": [],
   "source": [
    "PAD_Token = 0\n",
    "START_Token = 1\n",
    "END_Token = 2\n",
    "\n",
    "\n",
    "class Vocabulary:\n",
    "    def __init__(self):\n",
    "        self.trimmed = False\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_Token: \"PAD\", START_Token: \"SOS\", END_Token: \"EOS\"}\n",
    "        self.word2index = {\"PAD\": PAD_Token, \"SOS\": START_Token, \"EOS\": END_Token}\n",
    "        self.num_words = 3\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(\" \"):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.num_words\n",
    "            self.index2word[self.num_words] = word\n",
    "            self.word2count[word] = 1\n",
    "            self.num_words = self.num_words + 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "    def trim(self, min_count):\n",
    "\n",
    "        if self.trimmed:\n",
    "            return\n",
    "        self.trimmed = True\n",
    "\n",
    "        keep_words = []\n",
    "\n",
    "        for word, freq in self.word2count.items():\n",
    "            if freq >= min_count:\n",
    "                keep_words.append(word)\n",
    "\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_Token: \"PAD\", START_Token: \"SOS\", END_Token: \"EOS\"}\n",
    "        self.word2index = {\"PAD\": PAD_Token, \"SOS\": START_Token, \"EOS\": END_Token}\n",
    "        self.num_words = 3\n",
    "\n",
    "        for word in keep_words:\n",
    "            self.addWord(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11744,
     "status": "ok",
     "timestamp": 1590910151100,
     "user": {
      "displayName": "deepak goyal",
      "photoUrl": "https://lh5.googleusercontent.com/-3gfEZruD9Sk/AAAAAAAAAAI/AAAAAAAACvg/54H4lMsdOgA/s64/photo.jpg",
      "userId": "05164064759516400423"
     },
     "user_tz": -330
    },
    "id": "MEPrFn1h4sFp",
    "outputId": "9a764bcd-f007-406e-842c-c2541ac22d70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataset and corresponding vocabulary...\n",
      "Preparation took place in: 7.032042980194092\n"
     ]
    }
   ],
   "source": [
    "Max_Length = 10\n",
    "\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = s.lower().strip()\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
    "    return s\n",
    "\n",
    "\n",
    "def readVocs(qr_pairs):\n",
    "\n",
    "    for qr_pair in qr_pairs:\n",
    "        qr_pair[0] = normalizeString(qr_pair[0])\n",
    "        qr_pair[1] = normalizeString(qr_pair[1])\n",
    "\n",
    "    voc = Vocabulary()\n",
    "    return voc, qr_pairs\n",
    "\n",
    "\n",
    "def filterPair(pair):\n",
    "    return len(pair[0].split(\" \")) < Max_Length and len(pair[1].split(\" \")) < Max_Length\n",
    "\n",
    "\n",
    "def filterPairs(qr_pairs):\n",
    "    return [pair for pair in qr_pairs if filterPair(pair)]\n",
    "\n",
    "\n",
    "def prepareDataset(qr_pairs):\n",
    "    voc, qr_pairs = readVocs(qr_pairs)\n",
    "    qr_pairs = filterPairs(qr_pairs)\n",
    "\n",
    "    for pair in qr_pairs:\n",
    "        voc.addSentence(pair[0])\n",
    "        voc.addSentence(pair[1])\n",
    "    #     print(\"Number\"+str(voc.num_words))\n",
    "    return voc, qr_pairs\n",
    "\n",
    "\n",
    "t1 = time.time()\n",
    "print(\"Preparing dataset and corresponding vocabulary...\")\n",
    "voc, pairs = prepareDataset(qr_pairs)\n",
    "print(\"Preparation took place in: \" + str(time.time() - t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11197,
     "status": "ok",
     "timestamp": 1590910151110,
     "user": {
      "displayName": "deepak goyal",
      "photoUrl": "https://lh5.googleusercontent.com/-3gfEZruD9Sk/AAAAAAAAAAI/AAAAAAAACvg/54H4lMsdOgA/s64/photo.jpg",
      "userId": "05164064759516400423"
     },
     "user_tz": -330
    },
    "id": "wv1ruAcB4wxK",
    "outputId": "99741d64-db74-4ab7-e9a2-33593e7cf82a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trimming rare words from vocabulary and dataset..\n",
      "Trimming took place in: 0.14563655853271484\n"
     ]
    }
   ],
   "source": [
    "Min_Count = 3\n",
    "\n",
    "\n",
    "def trimRareWords(voc, qr_pairs):\n",
    "\n",
    "    voc.trim(Min_Count)\n",
    "    keep_pairs = []\n",
    "\n",
    "    for pair in qr_pairs:\n",
    "        input_sentence = pair[0]\n",
    "        output_sentence = pair[1]\n",
    "\n",
    "        keep_input = True\n",
    "        keep_output = True\n",
    "\n",
    "        for word in input_sentence.split(\" \"):\n",
    "            if word not in voc.word2index:\n",
    "                keep_input = False\n",
    "                break\n",
    "\n",
    "        for word in output_sentence.split(\" \"):\n",
    "            if word not in voc.word2index:\n",
    "                keep_output = False\n",
    "                break\n",
    "\n",
    "        if keep_input and keep_output:\n",
    "            keep_pairs.append(pair)\n",
    "\n",
    "    return keep_pairs\n",
    "\n",
    "\n",
    "t1 = time.time()\n",
    "print(\"Trimming rare words from vocabulary and dataset..\")\n",
    "\n",
    "pairs = trimRareWords(voc, pairs)\n",
    "\n",
    "print(\"Trimming took place in: \" + str(time.time() - t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YVoU5avw4zay"
   },
   "outputs": [],
   "source": [
    "# def indexesFromSentence(voc,sentence):\n",
    "#     tokenised_sentence=[]\n",
    "#     tokenised_sentence.append(START_Token)\n",
    "\n",
    "#     for word in sentence.split(\" \"):\n",
    "#         tokenised_sentence.append(voc.word2index[word])\n",
    "\n",
    "#     tokenised_sentence.append(END_Token)\n",
    "\n",
    "#     assert len(tokenised_sentence)<=Max_Length+2\n",
    "#     for _ in range(Max_Length+2-len(tokenised_sentence)):\n",
    "#         tokenised_sentence.append(PAD_Token)\n",
    "\n",
    "#     return tokenised_sentence\n",
    "\n",
    "# def binaryMatrix(l,value=PAD_Token):\n",
    "#     m=[]\n",
    "#     for i,seq in enumerate(l):\n",
    "#         m.append([])\n",
    "#         for token in seq:\n",
    "#             if token==value:\n",
    "#                 m[i].append(0)\n",
    "#             else:\n",
    "#                 m[i].append(1)\n",
    "\n",
    "#     return m\n",
    "\n",
    "# def inputVar(voc,l):\n",
    "\n",
    "#     indexes_batch=[indexesFromSentence(voc,sentence) for sentence in l]\n",
    "#     input_lengths=torch.tensor([len(index) for index in indexes_batch])\n",
    "#     padVar=torch.LongTensor(indexes_batch)\n",
    "#     return input_lengths,padVar\n",
    "\n",
    "# def outputVar(voc,l):\n",
    "#     indexes_batch=[indexesFromSentence(voc,sentence) for sentence in l]\n",
    "#     max_target_len=torch.tensor([len(index) for index in indexes_batch])\n",
    "#     mask=binaryMatrix(indexes_batch)\n",
    "#     mask=torch.ByteTensor(mask)\n",
    "#     padVar=torch.LongTensor(indexes_batch)\n",
    "#     return max_target_len, mask, padVar\n",
    "\n",
    "# def batch2TrainData(voc,pair_batch):\n",
    "#     #sort function see\n",
    "#     input_batch=[]\n",
    "#     output_batch=[]\n",
    "\n",
    "#     for pair in pair_batch:\n",
    "#         input_batch.append(pair[0])\n",
    "#         output_batch.append(pair[1])\n",
    "\n",
    "\n",
    "#     input_lengths,tokenised_input=inputVar(voc,input_batch)\n",
    "#     max_out_length,mask,tokenised_output=outputVar(voc,output_batch)\n",
    "#     return input_lengths,tokenised_input,max_out_length,mask,tokenised_output\n",
    "\n",
    "\n",
    "def indexesFromSentence(voc, sentence):\n",
    "    return [voc.word2index[word] for word in sentence.split(\" \")] + [END_Token]\n",
    "\n",
    "\n",
    "def zeroPadding(l, fillvalue=PAD_Token):\n",
    "    return list(itertools.zip_longest(*l, fillvalue=fillvalue))\n",
    "\n",
    "\n",
    "def binaryMatrix(l, value=PAD_Token):\n",
    "    m = []\n",
    "    for i, seq in enumerate(l):\n",
    "        m.append([])\n",
    "        for token in seq:\n",
    "            if token == PAD_Token:\n",
    "                m[i].append(0)\n",
    "            else:\n",
    "                m[i].append(1)\n",
    "    return m\n",
    "\n",
    "\n",
    "# Returns padded input sequence tensor and lengths\n",
    "def inputVar(l, voc):\n",
    "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
    "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
    "    padList = zeroPadding(indexes_batch)\n",
    "    padVar = torch.LongTensor(padList)\n",
    "    return padVar, lengths\n",
    "\n",
    "\n",
    "# Returns padded target sequence tensor, padding mask, and max target length\n",
    "def outputVar(l, voc):\n",
    "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
    "    max_target_len = max([len(indexes) for indexes in indexes_batch])\n",
    "    padList = zeroPadding(indexes_batch)\n",
    "    mask = binaryMatrix(padList)\n",
    "    mask = torch.BoolTensor(mask)\n",
    "    padVar = torch.LongTensor(padList)\n",
    "    return padVar, mask, max_target_len\n",
    "\n",
    "\n",
    "# Returns all items for a given batch of pairs\n",
    "def batch2TrainData(voc, pair_batch):\n",
    "    pair_batch.sort(key=lambda x: len(x[0].split(\" \")), reverse=True)\n",
    "    input_batch, output_batch = [], []\n",
    "    for pair in pair_batch:\n",
    "        input_batch.append(pair[0])\n",
    "        output_batch.append(pair[1])\n",
    "    inp, lengths = inputVar(input_batch, voc)\n",
    "    output, mask, max_target_len = outputVar(output_batch, voc)\n",
    "    return inp, lengths, output, mask, max_target_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 676
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 10026,
     "status": "ok",
     "timestamp": 1590910151118,
     "user": {
      "displayName": "deepak goyal",
      "photoUrl": "https://lh5.googleusercontent.com/-3gfEZruD9Sk/AAAAAAAAAAI/AAAAAAAACvg/54H4lMsdOgA/s64/photo.jpg",
      "userId": "05164064759516400423"
     },
     "user_tz": -330
    },
    "id": "d8-18tBuMZMx",
    "outputId": "38cf4841-2b55-4bd4-d626-7273aa075950"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query-response pairs after all the preprocessing: 53113\n",
      "Number of Unique Words in our vocabulary: 7816\n",
      "Input length: tensor([9, 8, 8, 7, 5]) Size: torch.Size([5])\n",
      "--------------------------------------------------------------------------------\n",
      "Tokenised Input: tensor([[  25,  248,  101, 1134,  218],\n",
      "        [ 200,   53,  112,   50,   12],\n",
      "        [  67, 1302,  113,   92,   84],\n",
      "        [6695,  144,   12,    7,   66],\n",
      "        [   7,   53, 1087,  278,    2],\n",
      "        [  24, 1133, 1205,    6,    0],\n",
      "        [  36,    4,  129,    2,    0],\n",
      "        [  66,    2,    2,    0,    0],\n",
      "        [   2,    0,    0,    0,    0]]) Size: torch.Size([9, 5])\n",
      "--------------------------------------------------------------------------------\n",
      "Max out length: 10 Size: \n",
      "--------------------------------------------------------------------------------\n",
      "Mask: tensor([[ True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True, False],\n",
      "        [ True,  True, False,  True, False],\n",
      "        [ True,  True, False,  True, False],\n",
      "        [ True,  True, False,  True, False],\n",
      "        [ True,  True, False,  True, False],\n",
      "        [False, False, False,  True, False]]) Size: torch.Size([10, 5])\n",
      "--------------------------------------------------------------------------------\n",
      "Tokenised Output: tensor([[  42,   33,   67,   25,  869],\n",
      "        [6694, 1599,    4,  102,    4],\n",
      "        [ 247,   76,   95,  112,    2],\n",
      "        [ 117,  177,    4,  512,    0],\n",
      "        [ 101,   53,    2,  188,    0],\n",
      "        [ 354,  108,    0, 5276,    0],\n",
      "        [ 283,  779,    0,   37,    0],\n",
      "        [  66,    4,    0,  834,    0],\n",
      "        [   2,    2,    0,    4,    0],\n",
      "        [   0,    0,    0,    2,    0]]) Size: torch.Size([10, 5])\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of query-response pairs after all the preprocessing: \" + str(len(pairs)))\n",
    "print(\"Number of Unique Words in our vocabulary: \" + str(voc.num_words))\n",
    "\n",
    "# Sample batch\n",
    "batch = [random.choice(pairs) for _ in range(5)]\n",
    "(\n",
    "    tokenised_input,\n",
    "    input_lengths,\n",
    "    tokenised_output,\n",
    "    mask,\n",
    "    max_out_length,\n",
    ") = batch2TrainData(voc, batch)\n",
    "\n",
    "print(\"Input length: \" + str(input_lengths) + \" Size: \" + str(input_lengths.shape))\n",
    "print(\"-\" * 80)\n",
    "print(\n",
    "    \"Tokenised Input: \" + str(tokenised_input) + \" Size: \" + str(tokenised_input.shape)\n",
    ")\n",
    "print(\"-\" * 80)\n",
    "print(\"Max out length: \" + str(max_out_length) + \" Size: \")\n",
    "print(\"-\" * 80)\n",
    "print(\"Mask: \" + str(mask) + \" Size: \" + str(mask.shape))\n",
    "print(\"-\" * 80)\n",
    "print(\n",
    "    \"Tokenised Output: \"\n",
    "    + str(tokenised_output)\n",
    "    + \" Size: \"\n",
    "    + str(tokenised_output.shape)\n",
    ")\n",
    "print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Oppo7HKtKMxW"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ra7_TfQv42Ra"
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, embedding, n_layers=1, dropout=0):\n",
    "        \"\"\"\n",
    "        Encoder module for seq2seq architechture.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = embedding\n",
    "        self.gru = nn.GRU(\n",
    "            hidden_size,\n",
    "            hidden_size,\n",
    "            n_layers,\n",
    "            dropout=(0 if n_layers == 1 else dropout),\n",
    "            bidirectional=True,\n",
    "        )\n",
    "\n",
    "    def forward(self, input_seq, input_lengths, hidden=None):\n",
    "\n",
    "        embedded_input = self.embedding(input_seq)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(embedded_input, input_lengths)\n",
    "        outputs, hidden = self.gru(packed, hidden)\n",
    "\n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs)\n",
    "\n",
    "        outputs = outputs[:, :, : self.hidden_size] + outputs[:, :, self.hidden_size :]\n",
    "\n",
    "        return outputs, hidden\n",
    "\n",
    "    def hidden_init(self, batch_size):\n",
    "        return torch.zeros(\n",
    "            self.n_layers * 2, batch_size, self.hidden_size, device=device\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2ZsVL85g49Er"
   },
   "outputs": [],
   "source": [
    "# Luong attention layer\n",
    "class Attn(torch.nn.Module):\n",
    "    def __init__(self, method, hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        self.method = method\n",
    "        if self.method not in [\"dot\", \"general\", \"concat\"]:\n",
    "            raise ValueError(self.method, \"is not an appropriate attention method.\")\n",
    "        self.hidden_size = hidden_size\n",
    "        if self.method == \"general\":\n",
    "            self.attn = torch.nn.Linear(self.hidden_size, hidden_size)\n",
    "        elif self.method == \"concat\":\n",
    "            self.attn = torch.nn.Linear(self.hidden_size * 2, hidden_size)\n",
    "            self.v = torch.nn.Parameter(torch.FloatTensor(hidden_size))\n",
    "\n",
    "    def dot_score(self, hidden, encoder_output):\n",
    "        return torch.sum(hidden * encoder_output, dim=2)\n",
    "\n",
    "    def general_score(self, hidden, encoder_output):\n",
    "        energy = self.attn(encoder_output)\n",
    "        return torch.sum(hidden * energy, dim=2)\n",
    "\n",
    "    def concat_score(self, hidden, encoder_output):\n",
    "        energy = self.attn(\n",
    "            torch.cat(\n",
    "                (hidden.expand(encoder_output.size(0), -1, -1), encoder_output), 2\n",
    "            )\n",
    "        ).tanh()\n",
    "        return torch.sum(self.v * energy, dim=2)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        # Calculate the attention weights (energies) based on the given method\n",
    "        if self.method == \"general\":\n",
    "            attn_energies = self.general_score(hidden, encoder_outputs)\n",
    "        elif self.method == \"concat\":\n",
    "            attn_energies = self.concat_score(hidden, encoder_outputs)\n",
    "        elif self.method == \"dot\":\n",
    "            attn_energies = self.dot_score(hidden, encoder_outputs)\n",
    "\n",
    "        # Transpose max_length and batch_size dimensions\n",
    "        attn_energies = attn_energies.t()\n",
    "\n",
    "        # Return the softmax normalized probability scores (with added dimension)\n",
    "        return F.softmax(attn_energies, dim=1).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S6Vv95PR5TBu"
   },
   "outputs": [],
   "source": [
    "class LuongAttnDecoderRNN(nn.Module):\n",
    "    def __init__(\n",
    "        self, attn_model, embedding, hidden_size, output_size, n_layers=1, dropout=0.1\n",
    "    ):\n",
    "        super(LuongAttnDecoderRNN, self).__init__()\n",
    "\n",
    "        # Keep for reference\n",
    "        self.attn_model = attn_model\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Define layers\n",
    "        self.embedding = embedding\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "        self.gru = nn.GRU(\n",
    "            hidden_size,\n",
    "            hidden_size,\n",
    "            n_layers,\n",
    "            dropout=(0 if n_layers == 1 else dropout),\n",
    "        )\n",
    "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "        self.attn = Attn(attn_model, hidden_size)\n",
    "\n",
    "    def forward(self, input_step, last_hidden, encoder_outputs):\n",
    "        # Note: we run this one step (word) at a time\n",
    "        # Get embedding of current input word\n",
    "        embedded = self.embedding(input_step)\n",
    "        embedded = self.embedding_dropout(embedded)\n",
    "        # Forward through unidirectional GRU\n",
    "\n",
    "        rnn_output, hidden = self.gru(embedded, last_hidden)\n",
    "\n",
    "        # Calculate attention weights from the current GRU output\n",
    "        attn_weights = self.attn(rnn_output, encoder_outputs)\n",
    "        # Multiply attention weights to encoder outputs to get new \"weighted sum\" context vector\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))\n",
    "        # Concatenate weighted context vector and GRU output using Luong eq. 5\n",
    "        rnn_output = rnn_output.squeeze(0)\n",
    "        context = context.squeeze(1)\n",
    "        concat_input = torch.cat((rnn_output, context), 1)\n",
    "        concat_output = torch.tanh(self.concat(concat_input))\n",
    "        # Predict next word using Luong eq. 6\n",
    "        output = self.out(concat_output)\n",
    "        # output = F.softmax(output, dim=1)\n",
    "        # Return output and final hidden state\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PybGzLUrPRGD"
   },
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bqc_v6v09aN3"
   },
   "outputs": [],
   "source": [
    "def make_model(\n",
    "    vocabulary_size,\n",
    "    d_model=500,\n",
    "    num_encoders=1,\n",
    "    num_decoders=1,\n",
    "    dropout_encoder=0.1,\n",
    "    dropout_decoder=0.1,\n",
    "    attn_model=\"general\",\n",
    "):\n",
    "\n",
    "    embedding = nn.Embedding(vocabulary_size, d_model)\n",
    "    embedding.weight.requires_grad = False\n",
    "\n",
    "    encoder = EncoderRNN(d_model, embedding, num_encoders, dropout_encoder)\n",
    "    decoder = LuongAttnDecoderRNN(\n",
    "        attn_model, embedding, d_model, vocabulary_size, num_decoders, dropout_decoder\n",
    "    )\n",
    "\n",
    "    for p in encoder.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "\n",
    "    for p in decoder.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "\n",
    "    num_parameters = 0\n",
    "\n",
    "    num_parameters += count_parameters(encoder)\n",
    "    num_parameters += count_parameters(decoder)\n",
    "\n",
    "    return encoder, decoder, num_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nxlGp-uOKbLD"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RIMnSo1Q5WKP"
   },
   "outputs": [],
   "source": [
    "def maskNLLLoss(inp, target, mask):\n",
    "    nTotal = mask.sum()\n",
    "    crossEntropy = -torch.log(torch.gather(inp, 1, target.view(-1, 1)).squeeze(1))\n",
    "    loss = crossEntropy.masked_select(mask).mean()\n",
    "    loss = loss.to(device)\n",
    "    return loss, nTotal.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G5RqXDqbpp5g"
   },
   "outputs": [],
   "source": [
    "def f1_score(predictions, targets, average=True):\n",
    "    predictions = predictions.tolist()\n",
    "    targets = targets.tolist()\n",
    "\n",
    "    def f1_score_items(pred_items, gold_items):\n",
    "        common = Counter(gold_items) & Counter(pred_items)\n",
    "        num_same = sum(common.values())\n",
    "\n",
    "        if num_same == 0:\n",
    "            return 0\n",
    "\n",
    "        precision = num_same / len(pred_items)\n",
    "        recall = num_same / len(gold_items)\n",
    "        f1 = (2 * precision * recall) / (precision + recall)\n",
    "\n",
    "        return f1\n",
    "\n",
    "    scores = [f1_score_items(p, t) for p, t in zip(predictions, targets)]\n",
    "\n",
    "    if average:\n",
    "        return sum(scores) / len(scores)\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1xk1Eu8y5cYn"
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 10\n",
    "\n",
    "\n",
    "def train(\n",
    "    input_variable,\n",
    "    lengths,\n",
    "    target_variable,\n",
    "    mask,\n",
    "    max_target_len,\n",
    "    encoder,\n",
    "    decoder,\n",
    "    encoder_optimizer,\n",
    "    decoder_optimizer,\n",
    "    batch_size,\n",
    "    clip,\n",
    "    max_length=MAX_LENGTH,\n",
    "    teacher_forcing=True,\n",
    "):\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    input_variable = torch.tensor(input_variable).to(device)\n",
    "    lengths = lengths.to(device)\n",
    "\n",
    "    target_variable = torch.tensor(target_variable).to(device)\n",
    "    mask = mask.to(device)\n",
    "\n",
    "    loss = 0\n",
    "    print_losses = []\n",
    "    n_totals = 0\n",
    "    encoder_hidden = encoder.hidden_init(input_variable.size()[1])\n",
    "    encoder_outputs, encoder_hidden = encoder(input_variable, lengths, encoder_hidden)\n",
    "\n",
    "    decoder_input = torch.LongTensor([[START_Token for _ in range(batch_size)]])\n",
    "    decoder_input = decoder_input.to(device)\n",
    "    use_teacher_forcing = (\n",
    "        teacher_forcing  # if random.random()<teacher_forcing_ratio else False\n",
    "    )\n",
    "\n",
    "    decoder_hidden = encoder_hidden[: decoder.n_layers]\n",
    "\n",
    "    predicted = torch.ones(max_target_len, input_variable.size()[1])\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "\n",
    "        for t in range(max_target_len):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "\n",
    "            decoder_input = target_variable[t].view(1, -1)\n",
    "\n",
    "            # mask_loss=F.cross_entropy(decoder_output,target_variable[t])\n",
    "            # mask_loss,nTotal=maskNLLLoss(decoder_output,target_variable[t],mask)\n",
    "            mask_loss = loss_fn(decoder_output, target_variable[t])\n",
    "            loss += mask_loss\n",
    "            nTotal = (mask.sum()).item()\n",
    "            print_losses.append(mask_loss.item() * nTotal)\n",
    "            n_totals += nTotal\n",
    "            predicted[t] = torch.argmax(decoder_output, dim=-1)\n",
    "\n",
    "    else:\n",
    "\n",
    "        for t in range(max_target_len):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "\n",
    "            _, topi = decoder_output.topk(1)\n",
    "            decoder_input = torch.LongTensor([[topi[i][0] for i in range(batch_size)]])\n",
    "            decoder_input = decoder_input.to(device)\n",
    "\n",
    "            mask_loss = loss_fn(decoder_output, target_variable[t])\n",
    "            loss += mask_loss\n",
    "            nTotal = (mask.sum()).item()\n",
    "            print_losses.append(mask_loss.item() * nTotal)\n",
    "            n_totals += nTotal\n",
    "\n",
    "    F1 = f1_score(predicted, target_variable.transpose(0, 1))\n",
    "    if teacher_forcing:\n",
    "        loss.backward()\n",
    "\n",
    "        _ = nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
    "        _ = nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
    "\n",
    "        # plot_grad_flow(encoder.named_parameters())\n",
    "        # plot_grad_flow(decoder.named_parameters())\n",
    "\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "\n",
    "    return sum(print_losses) / n_totals, F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6-GQ9XmH5fLl"
   },
   "outputs": [],
   "source": [
    "def trainIters(\n",
    "    model_name,\n",
    "    voc,\n",
    "    pairs,\n",
    "    encoder,\n",
    "    decoder,\n",
    "    encoder_optimizer,\n",
    "    decoder_optimizer,\n",
    "    encoder_n_layers,\n",
    "    decoder_n_layers,\n",
    "    save_dir,\n",
    "    n_batches,\n",
    "    batch_size,\n",
    "    save_every,\n",
    "    clip,\n",
    "    corpus_name,\n",
    "    loadFileName,\n",
    "    n_epochs,\n",
    "    training_batches,\n",
    "    teacher_forcing,\n",
    "    save_want,\n",
    "):\n",
    "\n",
    "    start_epoch = 0\n",
    "    loss = 0\n",
    "    perplexity = 0\n",
    "    time_taken = 0\n",
    "    f1_score = 0\n",
    "\n",
    "    if loadFileName:\n",
    "        start_epoch = checkpoint[\"epoch\"] + 1\n",
    "        time_taken = checkpoint[\"time\"]\n",
    "\n",
    "    for epoch in range(start_epoch, n_epochs):\n",
    "\n",
    "        t1 = time.time()\n",
    "        loss = 0\n",
    "        perplexity = 0\n",
    "        f1_score = 0\n",
    "\n",
    "        for i in range(n_batches):\n",
    "\n",
    "            training_batch = training_batches[i]\n",
    "\n",
    "            (\n",
    "                input_variable,\n",
    "                lengths,\n",
    "                target_variable,\n",
    "                mask,\n",
    "                max_target_len,\n",
    "            ) = training_batch\n",
    "\n",
    "            curr_loss, F1 = train(\n",
    "                input_variable,\n",
    "                lengths,\n",
    "                target_variable,\n",
    "                mask,\n",
    "                max_target_len,\n",
    "                encoder,\n",
    "                decoder,\n",
    "                encoder_optimizer,\n",
    "                decoder_optimizer,\n",
    "                batch_size,\n",
    "                clip,\n",
    "                10,\n",
    "                teacher_forcing,\n",
    "            )\n",
    "\n",
    "            loss += curr_loss\n",
    "            perplexity += math.exp(curr_loss)\n",
    "            f1_score += F1\n",
    "\n",
    "        loss = loss / n_batches\n",
    "        perplexity = perplexity / n_batches\n",
    "        f1_score = f1_score / n_batches\n",
    "\n",
    "        if epoch % save_every == 0 and save_want:\n",
    "\n",
    "            directory = os.path.join(save_dir, model_name, corpus_name)\n",
    "            if not os.path.exists(directory):\n",
    "                os.makedirs(directory)\n",
    "\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"epoch\": epoch,\n",
    "                    \"encoder\": encoder.state_dict(),\n",
    "                    \"decoder\": decoder.state_dict(),\n",
    "                    \"loss\": loss,\n",
    "                    \"encoder_opt\": encoder_optimizer.state_dict(),\n",
    "                    \"decoder_opt\": decoder_optimizer.state_dict(),\n",
    "                    \"ppl\": perplexity,\n",
    "                    \"time\": time_taken,\n",
    "                    \"F1\": f1_score,\n",
    "                },\n",
    "                os.path.join(directory, \"{}_{}.tar\".format(epoch, \"checkpoint\")),\n",
    "            )\n",
    "\n",
    "        print(\"=\" * 100)\n",
    "        print(\n",
    "            \"| End of epoch : \"\n",
    "            + str(epoch)\n",
    "            + \"| Loss Value: \"\n",
    "            + str(loss)\n",
    "            + \"| PPL: \"\n",
    "            + str(perplexity)\n",
    "            + \"| F1: \"\n",
    "            + str(f1_score)\n",
    "            + \"| Time Took: \"\n",
    "            + str(time.time() - t1)\n",
    "            + \" |\"\n",
    "        )\n",
    "        print(\"=\" * 100)\n",
    "        time_taken += time.time() - t1\n",
    "\n",
    "    print(\"| Training Finished | Took:\" + str(time_taken))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z3yEoRzsIsPN"
   },
   "outputs": [],
   "source": [
    "def data_generation(pairs, batch_size, n_batches, start=0):\n",
    "\n",
    "    # sample_batches=[batch2TrainData(voc,[random.choice(pairs) for _ in range(batch_size)]) for _ in range(n_batches)]\n",
    "    sample_batches = []\n",
    "\n",
    "    for i in range(n_batches):\n",
    "        curr_batch = []\n",
    "        for j in range(batch_size):\n",
    "            curr_id = i * batch_size + j + start\n",
    "            curr_batch.append(pairs[curr_id])\n",
    "\n",
    "        sample_batches.append(batch2TrainData(voc, curr_batch))\n",
    "\n",
    "    return sample_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9D6B5hoG5mfA"
   },
   "outputs": [],
   "source": [
    "model_name = \"seq2seq_attn\"\n",
    "corpus_name = \"cornell-movie\"\n",
    "\n",
    "attn_model = \"dot\"\n",
    "num_encoder = 2\n",
    "num_decoder = 2\n",
    "d_model = 500\n",
    "dropout_encoder = 0.1\n",
    "dropout_decoder = 0.1\n",
    "\n",
    "batch_size = 10\n",
    "n_batches = 4000\n",
    "\n",
    "clip = 50.0\n",
    "teacher_forcing_ratio = 1.0\n",
    "learning_rate = 0.0001\n",
    "decoder_learning_ratio = 5.0\n",
    "\n",
    "\n",
    "save_every = 10\n",
    "n_epochs = 201\n",
    "\n",
    "loadFile = (\n",
    "    \"/content/drive/My Drive/Model Data/seq2seq_attn/cornell-movie/200_checkpoint.tar\"\n",
    ")\n",
    "\n",
    "save_dir = \"/content/drive/My Drive/Model Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 12392,
     "status": "ok",
     "timestamp": 1590910212072,
     "user": {
      "displayName": "deepak goyal",
      "photoUrl": "https://lh5.googleusercontent.com/-3gfEZruD9Sk/AAAAAAAAAAI/AAAAAAAACvg/54H4lMsdOgA/s64/photo.jpg",
      "userId": "05164064759516400423"
     },
     "user_tz": -330
    },
    "id": "LkB9JkW15omn",
    "outputId": "a837f531-d73b-4046-da33-5122af408fb5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making models....\n",
      "Model building finished, the model has: 14934316 parameters..\n",
      "| Training Finished | Took:28300.82805275917\n"
     ]
    }
   ],
   "source": [
    "training_batches = data_generation(pairs, 10, 4000, 0)\n",
    "print(\"Making models....\")\n",
    "encoder, decoder, num_parameters = make_model(\n",
    "    voc.num_words,\n",
    "    d_model,\n",
    "    num_encoder,\n",
    "    num_decoder,\n",
    "    dropout_encoder,\n",
    "    dropout_decoder,\n",
    "    attn_model,\n",
    ")\n",
    "print(\n",
    "    \"Model building finished, the model has: \" + str(num_parameters) + \" parameters..\"\n",
    ")\n",
    "\n",
    "encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = torch.optim.Adam(\n",
    "    decoder.parameters(), lr=learning_rate * decoder_learning_ratio\n",
    ")\n",
    "\n",
    "if loadFile:\n",
    "    checkpoint = torch.load(loadFile)\n",
    "\n",
    "    encoder.load_state_dict(checkpoint[\"encoder\"])\n",
    "    decoder.load_state_dict(checkpoint[\"decoder\"])\n",
    "    encoder_optimizer.load_state_dict(checkpoint[\"encoder_opt\"])\n",
    "    decoder_optimizer.load_state_dict(checkpoint[\"decoder_opt\"])\n",
    "\n",
    "\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "encoder.train()\n",
    "decoder.train()\n",
    "\n",
    "for state in encoder_optimizer.state.values():\n",
    "    for k, v in state.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            state[k] = v.cuda()\n",
    "\n",
    "for state in decoder_optimizer.state.values():\n",
    "    for k, v in state.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            state[k] = v.cuda()\n",
    "\n",
    "\n",
    "trainIters(\n",
    "    model_name,\n",
    "    voc,\n",
    "    pairs,\n",
    "    encoder,\n",
    "    decoder,\n",
    "    encoder_optimizer,\n",
    "    decoder_optimizer,\n",
    "    num_encoder,\n",
    "    num_decoder,\n",
    "    save_dir,\n",
    "    n_batches,\n",
    "    batch_size,\n",
    "    save_every,\n",
    "    clip,\n",
    "    corpus_name,\n",
    "    loadFile,\n",
    "    n_epochs,\n",
    "    training_batches,\n",
    "    True,\n",
    "    True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OA5TjKYt5hhA"
   },
   "outputs": [],
   "source": [
    "class GreedySearchDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, input_seq, input_length, max_length, target=None):\n",
    "\n",
    "        encoder_outputs, encoder_hidden = self.encoder(input_seq, input_length)\n",
    "        decoder_hidden = encoder_hidden[: decoder.n_layers]\n",
    "        decoder_input = torch.ones(1, 1, device=device, dtype=torch.long) * START_Token\n",
    "\n",
    "        all_tokens = torch.zeros([0], device=device, dtype=torch.long)\n",
    "        all_scores = torch.zeros([0], device=device)\n",
    "        loss = 0\n",
    "\n",
    "        for i in range(max_length):\n",
    "\n",
    "            decoder_output, decoder_hidden = self.decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            if target != None:\n",
    "                loss += F.cross_entropy(decoder_output, target[i]).item()\n",
    "            decoder_scores, decoder_input = torch.max(decoder_output, dim=1)\n",
    "            all_scores = torch.cat((all_scores, decoder_scores), dim=0)\n",
    "            all_tokens = torch.cat((all_tokens, decoder_input), dim=0)\n",
    "\n",
    "            decoder_input = torch.unsqueeze(decoder_input, 0)\n",
    "\n",
    "        return all_tokens, all_scores, loss / max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "66bZDfiW0U6W"
   },
   "outputs": [],
   "source": [
    "def testing(encoder, decoder, searcher, pairs, starting_point):\n",
    "\n",
    "    loss = 0\n",
    "    F1 = 0\n",
    "    num = 0\n",
    "\n",
    "    for i in range(starting_point, len(pairs)):\n",
    "        test_data = data_generation(pairs, 1, 1, i)\n",
    "        input_variable, lengths, target_variable, mask, max_target_len = test_data[0]\n",
    "        input_variable = input_variable.to(device)\n",
    "        lengths = lengths.to(device)\n",
    "        target_variable = target_variable.to(device)\n",
    "\n",
    "        tokens, scores, curr_loss = searcher(\n",
    "            input_variable, lengths, max_target_len, target_variable\n",
    "        )\n",
    "        loss += curr_loss\n",
    "        F1 += f1_score(tokens.view(-1, max_target_len), target_variable)\n",
    "        num = num + 1\n",
    "\n",
    "    return loss / num, F1 / num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 393710,
     "status": "ok",
     "timestamp": 1588011958332,
     "user": {
      "displayName": "deepak goyal",
      "photoUrl": "https://lh5.googleusercontent.com/-3gfEZruD9Sk/AAAAAAAAAAI/AAAAAAAACvg/54H4lMsdOgA/s64/photo.jpg",
      "userId": "05164064759516400423"
     },
     "user_tz": -330
    },
    "id": "5EMWVrmC3uud",
    "outputId": "edff01c8-0844-4093-9a33-584b99fabdfa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17.45475177370771, 0.0887326835004401)"
      ]
     },
     "execution_count": 33,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.eval()\n",
    "decoder.eval()\n",
    "searcher = GreedySearchDecoder(encoder, decoder)\n",
    "testing(encoder, decoder, searcher, pairs, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EhpBX3QT5kCf"
   },
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, searcher, voc, sentence, max_length=MAX_LENGTH):\n",
    "\n",
    "    index_batch = [indexesFromSentence(voc, sentence)]\n",
    "    lengths = torch.tensor([len(index) for index in index_batch])\n",
    "    input_batch = torch.LongTensor(index_batch).transpose(0, 1)\n",
    "\n",
    "    input_batch = input_batch.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "\n",
    "    tokens, scores, loss = searcher(input_batch, lengths, max_length)\n",
    "    decoded_words = [voc.index2word[token.item()] for token in tokens]\n",
    "    return decoded_words\n",
    "\n",
    "\n",
    "def evaluateInput(encoder, decoder, searcher, voc):\n",
    "    input_sentence = \"\"\n",
    "    while True:\n",
    "        try:\n",
    "            input_sentence = input(\"Human> \")\n",
    "\n",
    "            if input_sentence == \"q\" or input_sentence == \"quit\":\n",
    "                break\n",
    "            input_sentence = normalizeString(input_sentence)\n",
    "            output_words = evaluate(encoder, decoder, searcher, voc, input_sentence)\n",
    "\n",
    "            output_words[:] = [\n",
    "                x for x in output_words if not (x == \"PAD\" or x == \"EOS\")\n",
    "            ]\n",
    "            print(\"Bot:\", \" \".join(output_words))\n",
    "\n",
    "        except KeyError:\n",
    "            print(\"Unknown Word\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1195,
     "status": "ok",
     "timestamp": 1590910256537,
     "user": {
      "displayName": "deepak goyal",
      "photoUrl": "https://lh5.googleusercontent.com/-3gfEZruD9Sk/AAAAAAAAAAI/AAAAAAAACvg/54H4lMsdOgA/s64/photo.jpg",
      "userId": "05164064759516400423"
     },
     "user_tz": -330
    },
    "id": "Bk193gyI54Iv",
    "outputId": "5b5de870-25b9-4759-d621-61a454f7d550"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LuongAttnDecoderRNN(\n",
       "  (embedding): Embedding(7816, 500)\n",
       "  (embedding_dropout): Dropout(p=0.1, inplace=False)\n",
       "  (gru): GRU(500, 500, num_layers=2, dropout=0.1)\n",
       "  (concat): Linear(in_features=1000, out_features=500, bias=True)\n",
       "  (out): Linear(in_features=500, out_features=7816, bias=True)\n",
       "  (attn): Attn()\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.eval()\n",
    "decoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 65609,
     "status": "ok",
     "timestamp": 1590910673435,
     "user": {
      "displayName": "deepak goyal",
      "photoUrl": "https://lh5.googleusercontent.com/-3gfEZruD9Sk/AAAAAAAAAAI/AAAAAAAACvg/54H4lMsdOgA/s64/photo.jpg",
      "userId": "05164064759516400423"
     },
     "user_tz": -330
    },
    "id": "VoxgXE87-169",
    "outputId": "57fd58a4-b3ce-414b-a79a-979c69bee4c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human> what do you work ?\n",
      "Bot: i ain t a dream .\n",
      "Human> what is it for ?\n",
      "Bot: nothing . not bad .\n",
      "Human> quit\n"
     ]
    }
   ],
   "source": [
    "searcher = GreedySearchDecoder(encoder, decoder)\n",
    "evaluateInput(encoder, decoder, searcher, voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eSOFd0pjIIuv"
   },
   "outputs": [],
   "source": [
    "def testing(\n",
    "    encoder,\n",
    "    decoder,\n",
    "    encoder_optimizer,\n",
    "    decoder_optimizer,\n",
    "    pairs,\n",
    "    starting_point,\n",
    "    n_batches=1000,\n",
    "    batch_size=10,\n",
    "):\n",
    "    test_data = data_generation(pairs, batch_size, n_batches, starting_point)\n",
    "\n",
    "    loss = 0\n",
    "    f1_score = 0\n",
    "    ppl = 0\n",
    "\n",
    "    for i in range(n_batches):\n",
    "        current_batch = test_data[i]\n",
    "        input_variable, lengths, target_variable, mask, max_target_len = current_batch\n",
    "\n",
    "        curr_loss, F1 = train(\n",
    "            input_variable,\n",
    "            lengths,\n",
    "            target_variable,\n",
    "            mask,\n",
    "            max_target_len,\n",
    "            encoder,\n",
    "            decoder,\n",
    "            encoder_optimizer,\n",
    "            decoder_optimizer,\n",
    "            batch_size,\n",
    "            clip,\n",
    "            10,\n",
    "            True,\n",
    "        )\n",
    "\n",
    "        loss += curr_loss\n",
    "        f1_score += F1\n",
    "        ppl += math.exp(curr_loss)\n",
    "\n",
    "    loss = loss / n_batches\n",
    "    f1_score = f1_score / n_batches\n",
    "    ppl = ppl / n_batches\n",
    "\n",
    "    print(\n",
    "        \"Loss Value: \"\n",
    "        + str(loss)\n",
    "        + \" F1_Score: \"\n",
    "        + str(f1_score)\n",
    "        + \" Current PPL: \"\n",
    "        + str(ppl)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "acdHoto0INkW"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyO8CJFPVWjyECMjrlBpDsSL",
   "collapsed_sections": [],
   "name": "seq2seq_withattn.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
