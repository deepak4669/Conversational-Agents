{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/deepak4669/Conversational-Agents/blob/master/Colab-Notebooks/transformer_cornell.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1045,
     "status": "ok",
     "timestamp": 1588205405978,
     "user": {
      "displayName": "Deepak Goyal",
      "photoUrl": "",
      "userId": "12155914074048018284"
     },
     "user_tz": -330
    },
    "id": "8i8or0mWcKPL",
    "outputId": "d2cfa759-c71a-45f6-9a5a-63b4470cb488"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6400,
     "status": "ok",
     "timestamp": 1588205415990,
     "user": {
      "displayName": "Deepak Goyal",
      "photoUrl": "",
      "userId": "12155914074048018284"
     },
     "user_tz": -330
    },
    "id": "U-K4mgwHE8oj",
    "outputId": "3450da75-6dec-4a58-d7e2-8ac56d596e0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Apr 30 00:10:12 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 440.64.00    Driver Version: 418.67       CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P4            Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   62C    P8     9W /  75W |      0MiB /  7611MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# !nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R-KwziUccUv9"
   },
   "outputs": [],
   "source": [
    "# Pre-Processing\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "# Model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# For visualising metrics\n",
    "# from visdom import Visdom\n",
    "\n",
    "# For visualising gradients plot\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "import copy\n",
    "import math\n",
    "import time\n",
    "\n",
    "from queue import PriorityQueue\n",
    "import operator\n",
    "\n",
    "from collections import namedtuple, Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6601,
     "status": "ok",
     "timestamp": 1588205416640,
     "user": {
      "displayName": "Deepak Goyal",
      "photoUrl": "",
      "userId": "12155914074048018284"
     },
     "user_tz": -330
    },
    "id": "bzOQagtOdqyn",
    "outputId": "c57d9bc1-c1a3-4962-c127-13175cf69007"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The device found: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device=torch.device(\"cpu\")\n",
    "print(\"The device found: \" + str(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "19BVL5GNdtbi"
   },
   "outputs": [],
   "source": [
    "def plot_grad_flow(named_parameters):\n",
    "    \"\"\"\n",
    "        Plotting gradient flow across various layers\n",
    "        Thanks to: https://discuss.pytorch.org/t/check-gradient-flow-in-network/15063/2\n",
    "    \"\"\"\n",
    "    ave_grads = []\n",
    "    layers = []\n",
    "    for n, p in named_parameters:\n",
    "        if (p.requires_grad) and (\"bias\" not in n):\n",
    "            layers.append(n)\n",
    "            ave_grads.append(p.grad.abs().mean())\n",
    "    plt.plot(ave_grads, alpha=0.3, color=\"b\")\n",
    "    plt.hlines(0, 0, len(ave_grads) + 1, linewidth=1, color=\"k\")\n",
    "    plt.xticks(range(0, len(ave_grads), 1), layers, rotation=\"vertical\")\n",
    "    plt.xlim(xmin=0, xmax=len(ave_grads))\n",
    "    plt.xlabel(\"Layers\")\n",
    "    plt.ylabel(\"average gradient\")\n",
    "    plt.title(\"Gradient flow\")\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XyVbmpGw26_v"
   },
   "source": [
    "# Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6129,
     "status": "ok",
     "timestamp": 1588205416642,
     "user": {
      "displayName": "Deepak Goyal",
      "photoUrl": "",
      "userId": "12155914074048018284"
     },
     "user_tz": -330
    },
    "id": "qz7CbxS4dwh3",
    "outputId": "4b633e5b-79c4-4393-edbb-3f78b363f207"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The final data corpus folder: /content/drive/My Drive/Data/cornell movie-dialogs corpus\n"
     ]
    }
   ],
   "source": [
    "path = \"/content/drive/My Drive/Data\"\n",
    "dataset = \"cornell movie-dialogs corpus\"\n",
    "\n",
    "data_folder = os.path.join(path, dataset)\n",
    "\n",
    "print(\"The final data corpus folder: \" + str(data_folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dDKqcO2de7RM"
   },
   "outputs": [],
   "source": [
    "def get_lines_conversations():\n",
    "    \"\"\"\n",
    "    Loads movie lines and conversations from the dataset.\n",
    "    \n",
    "    data_folder: Destination where conversations and lines are stored.\n",
    "    \n",
    "    movie_lines: Consist of movie lines as given by the dataset.\n",
    "    movie_conversations: Consist of movie conversations as given by the dataset.\n",
    "    \n",
    "    \"\"\"\n",
    "    movie_lines = []\n",
    "    movie_conversations = []\n",
    "\n",
    "    with open(\n",
    "        os.path.join(data_folder, \"movie_lines.txt\"), \"r\", encoding=\"iso-8859-1\"\n",
    "    ) as f:\n",
    "        for line in f:\n",
    "            movie_lines.append(line)\n",
    "\n",
    "    with open(\n",
    "        os.path.join(data_folder, \"movie_conversations.txt\"), \"r\", encoding=\"iso-8859-1\"\n",
    "    ) as f:\n",
    "        for line in f:\n",
    "            movie_conversations.append(line)\n",
    "\n",
    "    return movie_lines, movie_conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5749,
     "status": "ok",
     "timestamp": 1588205416643,
     "user": {
      "displayName": "Deepak Goyal",
      "photoUrl": "",
      "userId": "12155914074048018284"
     },
     "user_tz": -330
    },
    "id": "-txSMOjQfEnx",
    "outputId": "209f4f1a-78d1-4148-dd37-16fadeb4e26e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting movie lines and movie conversations...\n",
      "Number of distinct lines: 304713\n",
      "Number of conversations: 83097\n",
      "Average Number of lines per conversations: 3.6669554857576085\n",
      "L1045 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ They do not!\n",
      "\n",
      "u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L194', 'L195', 'L196', 'L197']\n",
      "\n",
      "Extracting took place in: 0.14994263648986816\n"
     ]
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "print(\"Extracting movie lines and movie conversations...\")\n",
    "movie_lines, movie_conversations = get_lines_conversations()\n",
    "\n",
    "print(\"Number of distinct lines: \" + str(len(movie_lines)))\n",
    "print(\"Number of conversations: \" + str(len(movie_conversations)))\n",
    "print(\n",
    "    \"Average Number of lines per conversations: \"\n",
    "    + str(len(movie_lines) / len(movie_conversations))\n",
    ")\n",
    "\n",
    "print(movie_lines[0])\n",
    "print(movie_conversations[0])\n",
    "\n",
    "print(\"Extracting took place in: \" + str(time.time() - t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3LXCFv_qfMsN"
   },
   "outputs": [],
   "source": [
    "exceptions = []\n",
    "\n",
    "\n",
    "def loadLines(movie_lines, fields):\n",
    "    lines = {}\n",
    "    for lineid in range(len(movie_lines)):\n",
    "\n",
    "        line = movie_lines[lineid]\n",
    "        values = line.split(\" +++$+++ \")\n",
    "\n",
    "        lineVals = {}\n",
    "\n",
    "        # print(\"values\"+str(len(values)))\n",
    "        # print(\"fields\"+str(len(fields)))\n",
    "\n",
    "        for i, field in enumerate(fields):\n",
    "            try:\n",
    "                lineVals[field] = values[i]\n",
    "            except:\n",
    "                print(\"Exception: \" + str(len(values)))\n",
    "                exceptions.append(lineid)\n",
    "\n",
    "        lines[lineVals[\"lineID\"]] = lineVals\n",
    "\n",
    "    return lines\n",
    "\n",
    "\n",
    "def loadConversations(movie_conversations, lines, fields):\n",
    "    conversations = []\n",
    "\n",
    "    for convo in movie_conversations:\n",
    "        values = convo.split(\" +++$+++ \")\n",
    "        conVals = {}\n",
    "\n",
    "        for i, field in enumerate(fields):\n",
    "            conVals[field] = values[i]\n",
    "\n",
    "        lineIDs = eval(conVals[\"utteranceIDs\"])\n",
    "\n",
    "        conVals[\"lines\"] = []\n",
    "\n",
    "        for lineID in lineIDs:\n",
    "            conVals[\"lines\"].append(lines[lineID])\n",
    "        conversations.append(conVals)\n",
    "\n",
    "    return conversations\n",
    "\n",
    "\n",
    "def sentencePairs(conversations):\n",
    "    qr_pairs = []\n",
    "\n",
    "    for conversation in conversations:\n",
    "        for i in range(len(conversation[\"lines\"]) - 1):\n",
    "            query = conversation[\"lines\"][i][\"text\"].strip()\n",
    "            response = conversation[\"lines\"][i + 1][\"text\"].strip()\n",
    "\n",
    "            if query and response:\n",
    "                qr_pairs.append([query, response])\n",
    "\n",
    "    return qr_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7814,
     "status": "ok",
     "timestamp": 1588205418975,
     "user": {
      "displayName": "Deepak Goyal",
      "photoUrl": "",
      "userId": "12155914074048018284"
     },
     "user_tz": -330
    },
    "id": "J2dngu2Xyvzt",
    "outputId": "27c8dbf1-7757-47f4-d252-a66813225ec2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Separating meaningfull information for our model...\n",
      "The number of query-response pairs are: 221282\n",
      "Separation took place in: 1.9910593032836914\n"
     ]
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "print(\"Separating meaningfull information for our model...\")\n",
    "\n",
    "lines = {}\n",
    "conversations = []\n",
    "qr_pairs = []\n",
    "\n",
    "movie_lines_fields = [\"lineID\", \"characterID\", \"movieID\", \"character\", \"text\"]\n",
    "movie_convo_fields = [\"charcaterID\", \"character2ID\", \"movieID\", \"utteranceIDs\"]\n",
    "\n",
    "lines = loadLines(movie_lines, movie_lines_fields)\n",
    "conversations = loadConversations(movie_conversations, lines, movie_convo_fields)\n",
    "qr_pairs = sentencePairs(conversations)\n",
    "\n",
    "print(\"The number of query-response pairs are: \" + str(len(qr_pairs)))\n",
    "print(\"Separation took place in: \" + str(time.time() - t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NFMCnpuO2jpr"
   },
   "outputs": [],
   "source": [
    "PAD_Token = 0\n",
    "START_Token = 1\n",
    "END_Token = 2\n",
    "\n",
    "\n",
    "class Vocabulary:\n",
    "    def __init__(self):\n",
    "        self.trimmed = False\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_Token: \"PAD\", START_Token: \"SOS\", END_Token: \"EOS\"}\n",
    "        self.word2index = {\"PAD\": PAD_Token, \"SOS\": START_Token, \"EOS\": END_Token}\n",
    "        self.num_words = 3\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(\" \"):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.num_words\n",
    "            self.index2word[self.num_words] = word\n",
    "            self.word2count[word] = 1\n",
    "            self.num_words = self.num_words + 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "    def trim(self, min_count):\n",
    "\n",
    "        if self.trimmed:\n",
    "            return\n",
    "        self.trimmed = True\n",
    "\n",
    "        keep_words = []\n",
    "\n",
    "        for word, freq in self.word2count.items():\n",
    "            if freq >= min_count:\n",
    "                keep_words.append(word)\n",
    "\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_Token: \"PAD\", START_Token: \"SOS\", END_Token: \"EOS\"}\n",
    "        self.word2index = {\"PAD\": PAD_Token, \"SOS\": START_Token, \"EOS\": END_Token}\n",
    "        self.num_words = 3\n",
    "\n",
    "        for word in keep_words:\n",
    "            self.addWord(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 13286,
     "status": "ok",
     "timestamp": 1588205424788,
     "user": {
      "displayName": "Deepak Goyal",
      "photoUrl": "",
      "userId": "12155914074048018284"
     },
     "user_tz": -330
    },
    "id": "GeScbB7iy0AC",
    "outputId": "67cb357a-9697-495f-e73e-344248d3b670"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataset and corresponding vocabulary...\n",
      "Preparation took place in: 5.789026260375977\n"
     ]
    }
   ],
   "source": [
    "Max_Length = 10\n",
    "\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = s.lower().strip()\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
    "    return s\n",
    "\n",
    "\n",
    "def readVocs(qr_pairs):\n",
    "\n",
    "    for qr_pair in qr_pairs:\n",
    "        qr_pair[0] = normalizeString(qr_pair[0])\n",
    "        qr_pair[1] = normalizeString(qr_pair[1])\n",
    "\n",
    "    voc = Vocabulary()\n",
    "    return voc, qr_pairs\n",
    "\n",
    "\n",
    "def filterPair(pair):\n",
    "    return len(pair[0].split(\" \")) < Max_Length and len(pair[1].split(\" \")) < Max_Length\n",
    "\n",
    "\n",
    "def filterPairs(qr_pairs):\n",
    "    return [pair for pair in qr_pairs if filterPair(pair)]\n",
    "\n",
    "\n",
    "def prepareDataset(qr_pairs):\n",
    "    voc, qr_pairs = readVocs(qr_pairs)\n",
    "    qr_pairs = filterPairs(qr_pairs)\n",
    "\n",
    "    for pair in qr_pairs:\n",
    "        voc.addSentence(pair[0])\n",
    "        voc.addSentence(pair[1])\n",
    "    #     print(\"Number\"+str(voc.num_words))\n",
    "    return voc, qr_pairs\n",
    "\n",
    "\n",
    "t1 = time.time()\n",
    "print(\"Preparing dataset and corresponding vocabulary...\")\n",
    "voc, pairs = prepareDataset(qr_pairs)\n",
    "print(\"Preparation took place in: \" + str(time.time() - t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 13007,
     "status": "ok",
     "timestamp": 1588205424789,
     "user": {
      "displayName": "Deepak Goyal",
      "photoUrl": "",
      "userId": "12155914074048018284"
     },
     "user_tz": -330
    },
    "id": "UUBpnFjQ2SCS",
    "outputId": "55a083c6-5d6f-46da-feeb-d04971d259f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trimming rare words from vocabulary and dataset..\n",
      "Trimming took place in: 0.12421536445617676\n"
     ]
    }
   ],
   "source": [
    "Min_Count = 3\n",
    "\n",
    "\n",
    "def trimRareWords(voc, qr_pairs):\n",
    "\n",
    "    voc.trim(Min_Count)\n",
    "    keep_pairs = []\n",
    "\n",
    "    for pair in qr_pairs:\n",
    "        input_sentence = pair[0]\n",
    "        output_sentence = pair[1]\n",
    "\n",
    "        keep_input = True\n",
    "        keep_output = True\n",
    "\n",
    "        for word in input_sentence.split(\" \"):\n",
    "            if word not in voc.word2index:\n",
    "                keep_input = False\n",
    "                break\n",
    "\n",
    "        for word in output_sentence.split(\" \"):\n",
    "            if word not in voc.word2index:\n",
    "                keep_output = False\n",
    "                break\n",
    "\n",
    "        if keep_input and keep_output:\n",
    "            keep_pairs.append(pair)\n",
    "\n",
    "    return keep_pairs\n",
    "\n",
    "\n",
    "t1 = time.time()\n",
    "print(\"Trimming rare words from vocabulary and dataset..\")\n",
    "\n",
    "pairs = trimRareWords(voc, pairs)\n",
    "\n",
    "print(\"Trimming took place in: \" + str(time.time() - t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VjOEe2nx2sbZ"
   },
   "outputs": [],
   "source": [
    "def indexesFromSentence(voc, sentence):\n",
    "    tokenised_sentence = []\n",
    "    tokenised_sentence.append(START_Token)\n",
    "\n",
    "    for word in sentence.split(\" \"):\n",
    "        tokenised_sentence.append(voc.word2index[word])\n",
    "\n",
    "    tokenised_sentence.append(END_Token)\n",
    "\n",
    "    assert len(tokenised_sentence) <= Max_Length + 2\n",
    "    for _ in range(Max_Length + 2 - len(tokenised_sentence)):\n",
    "        tokenised_sentence.append(PAD_Token)\n",
    "\n",
    "    return tokenised_sentence\n",
    "\n",
    "\n",
    "def binaryMatrix(l, value=PAD_Token):\n",
    "    m = []\n",
    "    for i, seq in enumerate(l):\n",
    "        m.append([])\n",
    "        for token in seq:\n",
    "            if token == value:\n",
    "                m[i].append(0)\n",
    "            else:\n",
    "                m[i].append(1)\n",
    "\n",
    "    return m\n",
    "\n",
    "\n",
    "def inputVar(voc, l):\n",
    "\n",
    "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
    "    input_lengths = torch.tensor([len(index) for index in indexes_batch])\n",
    "    padVar = torch.LongTensor(indexes_batch)\n",
    "    return input_lengths, padVar\n",
    "\n",
    "\n",
    "def outputVar(voc, l):\n",
    "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
    "    max_target_len = torch.tensor([len(index) for index in indexes_batch])\n",
    "    mask = binaryMatrix(indexes_batch)\n",
    "    mask = torch.ByteTensor(mask)\n",
    "    padVar = torch.LongTensor(indexes_batch)\n",
    "    return max_target_len, mask, padVar\n",
    "\n",
    "\n",
    "def batch2TrainData(voc, pair_batch):\n",
    "    # sort function see\n",
    "    input_batch = []\n",
    "    output_batch = []\n",
    "\n",
    "    for pair in pair_batch:\n",
    "        input_batch.append(pair[0])\n",
    "        output_batch.append(pair[1])\n",
    "\n",
    "    input_lengths, tokenised_input = inputVar(voc, input_batch)\n",
    "    max_out_length, mask, tokenised_output = outputVar(voc, output_batch)\n",
    "    return input_lengths, tokenised_input, max_out_length, mask, tokenised_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 416
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11972,
     "status": "ok",
     "timestamp": 1588205424790,
     "user": {
      "displayName": "Deepak Goyal",
      "photoUrl": "",
      "userId": "12155914074048018284"
     },
     "user_tz": -330
    },
    "id": "f-wd2hB-2y4b",
    "outputId": "fdf48fa7-1b2f-4e60-b271-ae1ea452cda9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query-response pairs after all the preprocessing: 53113\n",
      "Input length: tensor([12, 12, 12, 12, 12]) Size: torch.Size([5])\n",
      "--------------------------------------------------------------------------------\n",
      "Tokenised Input: tensor([[   1,  345,    4,    2,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [   1,    5,    6,    2,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [   1,   25,  105, 3347,    4,    2,    0,    0,    0,    0,    0,    0],\n",
      "        [   1,  303,   37,   44, 1664,  995,    7,  553,   83,    4,    2,    0],\n",
      "        [   1,   50,   53,  654,   92,    7,  206,   75,    6,    2,    0,    0]]) Size: torch.Size([5, 12])\n",
      "--------------------------------------------------------------------------------\n",
      "Max out length: tensor([12, 12, 12, 12, 12]) Size: torch.Size([5])\n",
      "--------------------------------------------------------------------------------\n",
      "Mask: tensor([[1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]], dtype=torch.uint8) Size: torch.Size([5, 12])\n",
      "--------------------------------------------------------------------------------\n",
      "Tokenised Output: tensor([[   1,  345,    4,    2,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [   1,  935,    3,  234,   53, 1258,    4,    2,    0,    0,    0,    0],\n",
      "        [   1,  115, 2390,    4,    4,    4,   96,  828,    6,    2,    0,    0],\n",
      "        [   1,   36,   37,   67,   38,   56,   76,    4,    2,    0,    0,    0],\n",
      "        [   1,   50,   37,   70,  706, 1734,    6,    2,    0,    0,    0,    0]]) Size: torch.Size([5, 12])\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of query-response pairs after all the preprocessing: \" + str(len(pairs)))\n",
    "\n",
    "# Sample batch\n",
    "batch = [random.choice(pairs) for _ in range(5)]\n",
    "(\n",
    "    input_lengths,\n",
    "    tokenised_input,\n",
    "    max_out_length,\n",
    "    mask,\n",
    "    tokenised_output,\n",
    ") = batch2TrainData(voc, batch)\n",
    "\n",
    "print(\"Input length: \" + str(input_lengths) + \" Size: \" + str(input_lengths.shape))\n",
    "print(\"-\" * 80)\n",
    "print(\n",
    "    \"Tokenised Input: \" + str(tokenised_input) + \" Size: \" + str(tokenised_input.shape)\n",
    ")\n",
    "print(\"-\" * 80)\n",
    "print(\"Max out length: \" + str(max_out_length) + \" Size: \" + str(max_out_length.shape))\n",
    "print(\"-\" * 80)\n",
    "print(\"Mask: \" + str(mask) + \" Size: \" + str(mask.shape))\n",
    "print(\"-\" * 80)\n",
    "print(\n",
    "    \"Tokenised Output: \"\n",
    "    + str(tokenised_output)\n",
    "    + \" Size: \"\n",
    "    + str(tokenised_output.shape)\n",
    ")\n",
    "print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DpR-1oMt3HA4"
   },
   "source": [
    "# Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RJnnpKgt3Nyg"
   },
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A standard Encoder-Decoder architecture. Base for this and many \n",
    "    other models.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.generator = generator\n",
    "\n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        \"Take in and process masked src and target sequences.\"\n",
    "        return self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask)\n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        return self.encoder(self.src_embed(src), src_mask)\n",
    "\n",
    "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
    "        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cRE6Ls4k_lvE"
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"Define standard linear + softmax generation step.\"\n",
    "\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Generator, self).__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.proj(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aPiLhBPP_oQL"
   },
   "outputs": [],
   "source": [
    "def clones(module, N):\n",
    "    \"Produce N identical layers.\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3ApjC3XD_quP"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"Core encoder is a stack of N layers\"\n",
    "\n",
    "    def __init__(self, layer, N):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"Pass the input (and mask) through each layer in turn.\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5wUahzPb_s88"
   },
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"Construct a layernorm module (See citation for details).\"\n",
    "\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zowO7FHp_v5s"
   },
   "outputs": [],
   "source": [
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual connection followed by a layer norm.\n",
    "    Note for code simplicity the norm is first as opposed to last.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"Apply residual connection to any sublayer with the same size.\"\n",
    "        return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_nkoHVj4_yiG"
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \"Encoder is made up of self-attn and feed forward (defined below)\"\n",
    "\n",
    "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"Follow Figure 1 (left) for connections.\"\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
    "        return self.sublayer[1](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BIJVFhXm_1FF"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"Generic N layer decoder with masking.\"\n",
    "\n",
    "    def __init__(self, layer, N):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "\n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, memory, src_mask, tgt_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZKn03mRp_3V2"
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \"Decoder is made of self-attn, src-attn, and feed forward (defined below)\"\n",
    "\n",
    "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.size = size\n",
    "        self.self_attn = self_attn\n",
    "        self.src_attn = src_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n",
    "\n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        \"Follow Figure 1 (right) for connections.\"\n",
    "        m = memory\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
    "        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n",
    "        return self.sublayer[2](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KddgfVJx_6DC"
   },
   "outputs": [],
   "source": [
    "def subsequent_mask(size):\n",
    "    \"Mask out subsequent positions.\"\n",
    "    attn_shape = (1, size, size)\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype(\"uint8\")\n",
    "    return torch.from_numpy(subsequent_mask) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BYWVMJoh_8-k"
   },
   "outputs": [],
   "source": [
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    \"Compute 'Scaled Dot Product Attention'\"\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    p_attn = F.softmax(scores, dim=-1)\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    return torch.matmul(p_attn, value), p_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "veKkKGgb__q9"
   },
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        \"Take in model size and number of heads.\"\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert d_model % h == 0\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"Implements Figure 2\"\n",
    "        if mask is not None:\n",
    "            # Same mask applied to all h heads.\n",
    "            mask = mask.unsqueeze(1)\n",
    "        nbatches = query.size(0)\n",
    "\n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k\n",
    "        query, key, value = [\n",
    "            l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "            for l, x in zip(self.linears, (query, key, value))\n",
    "        ]\n",
    "\n",
    "        # 2) Apply attention on all the projected vectors in batch.\n",
    "        x, self.attn = attention(query, key, value, mask=mask, dropout=self.dropout)\n",
    "\n",
    "        # 3) \"Concat\" using a view and apply a final linear.\n",
    "        x = x.transpose(1, 2).contiguous().view(nbatches, -1, self.h * self.d_k)\n",
    "        return self.linears[-1](x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aSqvuoJVAE_i"
   },
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"Implements FFN equation.\"\n",
    "\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(F.relu(self.w_1(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ft2ByV62AHlf"
   },
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.lut = nn.Embedding(vocab, d_model)\n",
    "        # self.lut.weight.requires_grad=False\n",
    "\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lut(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "flsGFWNWAKso"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"Implement the PE function.\"\n",
    "\n",
    "    def __init__(self, d_model, dropout, max_len=12):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + Variable(self.pe[:, : x.size(1)], requires_grad=False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "veyYVrUlANvh"
   },
   "outputs": [],
   "source": [
    "def make_model(src_vocab, tgt_vocab, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1):\n",
    "    \"Helper: Construct a model from hyperparameters.\"\n",
    "    c = copy.deepcopy\n",
    "    attn = MultiHeadedAttention(h, d_model)\n",
    "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "    position = PositionalEncoding(d_model, dropout)\n",
    "    model = EncoderDecoder(\n",
    "        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
    "        Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N),\n",
    "        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),\n",
    "        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),\n",
    "        Generator(d_model, tgt_vocab),\n",
    "    )\n",
    "\n",
    "    # This was important from their code.\n",
    "    # Initialize parameters with Glorot / fan_avg.\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v5xV-42LAX6d"
   },
   "source": [
    "# Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uHIbkFgbAhpa"
   },
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder, source_embed, target_embed, generator):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "        self.source_embed = source_embed\n",
    "        self.target_embed = target_embed\n",
    "\n",
    "        self.generator = generator  # Linear + Log_softmax\n",
    "\n",
    "    def forward(self, source, target, source_mask, target_mask):\n",
    "        return self.decode(\n",
    "            self.encode(source, source_mask), source_mask, target, target_mask\n",
    "        )\n",
    "\n",
    "    def encode(self, source, source_mask):\n",
    "        return self.encoder(self.source_embed(source), source_mask)\n",
    "\n",
    "    def decode(self, memory, source_mask, target, target_mask):\n",
    "        return self.decoder(self.target_embed(target), memory, source_mask, target_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ewcrG0AeCHQc"
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, d_model, vocab_size):\n",
    "        super().__init__()\n",
    "        self.projection = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, decoder_output):\n",
    "        return F.log_softmax(self.projection(decoder_output), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jNIgAbiYCKg0"
   },
   "outputs": [],
   "source": [
    "def clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qgHHuuzYCNL8"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, layer, N):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dQRX2bAMCPlO"
   },
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-sIDH01lCSsv"
   },
   "outputs": [],
   "source": [
    "class SublayerConnection(nn.Module):\n",
    "    def __init__(self, size, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm = LayerNorm(size)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1zQ_LsoCCVpO"
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "\n",
    "        x = self.sublayer[0](x, lambda x: self.attn(x, x, x, mask))\n",
    "        return self.sublayer[1](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cZjOa0vwCY5H"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, layer, N):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "\n",
    "    def forward(self, x, memory, curr_mask, tgt_mask):\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, memory, curr_mask, tgt_mask)\n",
    "\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FDHFdRvRCbgt"
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.size = size\n",
    "        self.self_attn = self_attn\n",
    "        self.src_attn = src_attn\n",
    "        self.feed_forward = feed_forward\n",
    "\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n",
    "\n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "\n",
    "        m = memory\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
    "        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n",
    "        return self.sublayer[2](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OWbh8piBCeC_"
   },
   "outputs": [],
   "source": [
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "\n",
    "    d_k = query.size(-1)\n",
    "\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "    p_attn = F.softmax(scores, dim=-1)\n",
    "\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "\n",
    "    return torch.matmul(p_attn, value), p_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZtHuOzFTCgie"
   },
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        assert d_model % h == 0\n",
    "\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, query, key, values, mask=None):\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "\n",
    "        nbatches = query.size(0)\n",
    "\n",
    "        query, key, values = [\n",
    "            l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "            for l, x in zip(self.linears, (query, key, values))\n",
    "        ]\n",
    "\n",
    "        x, self.attn = attention(query, key, values, mask=mask, dropout=self.dropout)\n",
    "\n",
    "        x = x.transpose(1, 2).contiguous().view(nbatches, -1, self.h * self.d_k)\n",
    "\n",
    "        return self.linears[-1](x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GD9vxJrECkSp"
   },
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(F.relu(self.w_1(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8Qna2tDeCmue"
   },
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed = nn.Embedding(vocab, d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        #         print(x.device)\n",
    "        return self.embed(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nCEQ9n0XCo7b"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        pe = torch.zeros(max_len, d_model, dtype=torch.float)\n",
    "        position = torch.arange(0.0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0.0, d_model, 2) * -(math.log(10000.0) / d_model)\n",
    "        )\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = x + Variable(self.pe[:, : x.size(1)], requires_grad=False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CkLif2nLC5U6"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "triu function generates a copy of matrix with elemens below kth diagonal zeroed.\n",
    "The main diagonal is zeroeth diagonal above is first(k=1) and so on.\n",
    "\n",
    "Eg:\n",
    "A=[[1,2,3],[4,5,6],[7,8,9]]\n",
    "for above matrix:\n",
    "triu(A,k=1)\n",
    "will give [[0,2,3],[0,0,6],[0,0,0]]\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def subsequent_mask(size):\n",
    "    attn_shape = (1, size, size)\n",
    "    mask = np.triu(np.ones(attn_shape), k=1).astype(\"uint8\")\n",
    "\n",
    "    return torch.from_numpy(mask) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WPOL8rdMCruh"
   },
   "outputs": [],
   "source": [
    "def make_model2(src_vocab, tgt_vocab, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1):\n",
    "\n",
    "    c = copy.deepcopy\n",
    "    attn = MultiHeadedAttention(h, d_model)\n",
    "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "    position = PositionalEncoding(d_model, dropout)\n",
    "    model = EncoderDecoder(\n",
    "        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
    "        Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N),\n",
    "        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),\n",
    "        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),\n",
    "        Generator(d_model, tgt_vocab),\n",
    "    )\n",
    "\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1033,
     "status": "ok",
     "timestamp": 1587774351341,
     "user": {
      "displayName": "Deepak Goyal",
      "photoUrl": "",
      "userId": "12155914074048018284"
     },
     "user_tz": -330
    },
    "id": "K6TsrSL5CuNH",
    "outputId": "a7350dfa-8ad4-436d-f043-b75566e2f82d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderDecoder(\n",
       "  (encoder): Encoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): EncoderLayer(\n",
       "        (attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm): LayerNorm()\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm): LayerNorm()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm()\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): DecoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (src_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm): LayerNorm()\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm): LayerNorm()\n",
       "          )\n",
       "          (2): SublayerConnection(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm): LayerNorm()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm()\n",
       "  )\n",
       "  (source_embed): Sequential(\n",
       "    (0): Embeddings(\n",
       "      (embed): Embedding(7816, 512)\n",
       "    )\n",
       "    (1): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (target_embed): Sequential(\n",
       "    (0): Embeddings(\n",
       "      (embed): Embedding(7816, 512)\n",
       "    )\n",
       "    (1): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (generator): Generator(\n",
       "    (projection): Linear(in_features=512, out_features=7816, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 207,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_model = make_model2(voc.num_words, voc.num_words, 1, 512, 2048, 8, 0.1)\n",
    "sample_model.to(device)\n",
    "# print(sample_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1026,
     "status": "ok",
     "timestamp": 1586270375254,
     "user": {
      "displayName": "deepak goyal",
      "photoUrl": "https://lh5.googleusercontent.com/-3gfEZruD9Sk/AAAAAAAAAAI/AAAAAAAACvg/54H4lMsdOgA/s64/photo.jpg",
      "userId": "05164064759516400423"
     },
     "user_tz": -330
    },
    "id": "TZ0e9Y5cCwzw",
    "outputId": "92dc2f78-b725-4b00-f1fe-c5d77264e95e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Output size: torch.Size([5, 12, 512])\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Sample Run\n",
    "source = torch.ones(5, 12, dtype=torch.long, device=device)\n",
    "target = torch.ones(5, 12, dtype=torch.long, device=device)\n",
    "source_mask = torch.ones(5, 12, 12, dtype=torch.long, device=device)\n",
    "target_mask = torch.ones(5, 12, 12, dtype=torch.long, device=device)\n",
    "out = sample_model(source, target, source_mask, target_mask)\n",
    "print(\"-\" * 80)\n",
    "print(\"Output size: \" + str(out.shape))\n",
    "print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hb-TFfrCDn_q"
   },
   "source": [
    "# Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EBjAhtyS16_t"
   },
   "outputs": [],
   "source": [
    "def F1_score(predictions, targets, average=True):\n",
    "\n",
    "    predictions = predictions.tolist()\n",
    "    targets = targets.tolist()\n",
    "\n",
    "    def f1_score_items(pred_items, gold_items):\n",
    "\n",
    "        common = Counter(gold_items) & Counter(pred_items)\n",
    "\n",
    "        num_same = sum(common.values())\n",
    "\n",
    "        if num_same == 0:\n",
    "            return 0\n",
    "\n",
    "        precision = num_same / len(pred_items)\n",
    "        recall = num_same / len(gold_items)\n",
    "        f1 = (2 * precision * recall) / (precision + recall)\n",
    "\n",
    "        return f1\n",
    "\n",
    "    scores = [f1_score_items(p, t) for p, t in zip(predictions, targets)]\n",
    "\n",
    "    if average:\n",
    "        return sum(scores) / len(scores)\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KBz2k8ywDqZ4"
   },
   "outputs": [],
   "source": [
    "def data_generation(pairs, batch_size, n_batches, start=0):\n",
    "\n",
    "    # sample_batches=[batch2TrainData(voc,[random.choice(pairs) for _ in range(batch_size)]) for _ in range(n_batches)]\n",
    "    sample_batches = []\n",
    "    batches = []\n",
    "    for i in range(n_batches):\n",
    "        curr_batch = []\n",
    "        for j in range(batch_size):\n",
    "            curr_id = i * batch_size + j + start\n",
    "            curr_batch.append(pairs[curr_id])\n",
    "\n",
    "        sample_batches.append(batch2TrainData(voc, curr_batch))\n",
    "\n",
    "    for i in range(n_batches):\n",
    "        batches.append(Batch(sample_batches[i][1], sample_batches[i][-1]))\n",
    "\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "upPkHvWXECA2"
   },
   "outputs": [],
   "source": [
    "class Batch:\n",
    "    \"Object for holding a batch of data with mask during training.\"\n",
    "\n",
    "    def __init__(self, src, trg=None, pad=0):\n",
    "        src = src.to(torch.int64)\n",
    "        trg = trg.to(torch.int64)\n",
    "        self.src = src\n",
    "        self.src_mask = (src != pad).unsqueeze(-2)\n",
    "        if trg is not None:\n",
    "            self.trg = trg[:, :-1]\n",
    "            self.trg_y = trg[:, 1:]\n",
    "            self.trg_mask = self.make_std_mask(self.trg, pad)\n",
    "            self.ntokens = (self.trg_y != pad).data.sum()\n",
    "        self.src.to(device)\n",
    "        self.trg.to(device)\n",
    "        self.src_mask.to(device)\n",
    "        self.trg_mask.to(device)\n",
    "\n",
    "    @staticmethod\n",
    "    def make_std_mask(tgt, pad):\n",
    "        \"Create a mask to hide padding and future words.\"\n",
    "        tgt_mask = (tgt != pad).unsqueeze(-2)\n",
    "        tgt_mask = tgt_mask & Variable(\n",
    "            subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data)\n",
    "        )\n",
    "        return tgt_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YRV3rCOGEMFn"
   },
   "outputs": [],
   "source": [
    "def run_single_batch(data, model, loss_compute):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    total_tokens = 0\n",
    "    total_loss = 0\n",
    "    tokens = 0\n",
    "\n",
    "    source = data.src\n",
    "    source_mask = data.src_mask\n",
    "    target = data.trg\n",
    "    target_mask = data.trg_mask\n",
    "    target_y = data.trg_y\n",
    "\n",
    "    source = source.to(device)\n",
    "    target = target.to(device)\n",
    "    source_mask = source_mask.to(device)\n",
    "    target_mask = target_mask.to(device)\n",
    "    target_y = target_y.to(device)\n",
    "\n",
    "    out = model(source, target, source_mask, target_mask)\n",
    "\n",
    "    loss, f1_score, ppl = loss_compute(out, target_y, data.ntokens)\n",
    "\n",
    "    return loss.item(), f1_score, data.ntokens.item(), ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a5Kfbj2TEXyp"
   },
   "outputs": [],
   "source": [
    "class SimpleLossCompute:\n",
    "    \"A simple loss compute and train function.\"\n",
    "\n",
    "    def __init__(self, generator, opt=None):\n",
    "        self.generator = generator\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.opt = opt\n",
    "\n",
    "    def __call__(self, x, y, norm):\n",
    "        x = self.generator(x)\n",
    "        sentence_length = x.size()[1]\n",
    "        bs = x.size()[0]\n",
    "\n",
    "        f1_score = F1_score(x.argmax(dim=-1), y)\n",
    "        # f1_score=0\n",
    "\n",
    "        loss = self.criterion(\n",
    "            x.contiguous().view(-1, x.size(-1)), y.contiguous().view(-1)\n",
    "        )\n",
    "        loss.backward()\n",
    "        _ = nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        if self.opt is not None:\n",
    "            self.opt.step()\n",
    "            self.opt.zero_grad()\n",
    "\n",
    "        return loss, f1_score, math.exp(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FixxgorW2fkQ"
   },
   "outputs": [],
   "source": [
    "def training(batches, model, n_epochs, n_batches, model_opt, loadFile, save_every):\n",
    "\n",
    "    start_epoch = 0\n",
    "    total_time = 0\n",
    "    if loadFile:\n",
    "        start_epoch = torch.load(loadFile)[\"epoch\"]\n",
    "        total_time = torch.load(loadFile)[\"time\"]\n",
    "        start_epoch = start_epoch + 1\n",
    "\n",
    "    for epoch in range(start_epoch, n_epochs):\n",
    "        t1 = time.time()\n",
    "        loss = 0\n",
    "        f1_score = 0\n",
    "        ppl = 0\n",
    "        n_tokens = 0\n",
    "        for i in range(n_batches):\n",
    "            current_batch = batches[i]\n",
    "            loss_val, current_f1_score, current_tokens, curr_ppl = run_single_batch(\n",
    "                current_batch, model, SimpleLossCompute(model.generator, model_opt)\n",
    "            )\n",
    "            loss += loss_val\n",
    "            f1_score += current_f1_score\n",
    "            n_tokens += current_tokens\n",
    "            ppl += curr_ppl\n",
    "\n",
    "        loss = loss / (n_batches)\n",
    "        ppl = ppl / n_batches\n",
    "        f1_score = f1_score / n_batches\n",
    "\n",
    "        if epoch % save_every == 0:\n",
    "            directory = os.path.join(save_dir, \"transformer\", \"cornell-movie-40K\")\n",
    "            if not os.path.exists(directory):\n",
    "                os.makedirs(directory)\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"epoch\": epoch,\n",
    "                    \"model\": model.state_dict(),\n",
    "                    \"opt\": model_opt.state_dict(),\n",
    "                    \"loss\": loss,\n",
    "                    \"ppl\": ppl,\n",
    "                    \"f1\": f1_score,\n",
    "                    \"time\": total_time,\n",
    "                },\n",
    "                os.path.join(directory, \"{}_{}.tar\".format(epoch, \"checkpoint\")),\n",
    "            )\n",
    "\n",
    "        print(\"=\" * 100)\n",
    "        print(\n",
    "            \"| End of Epoch : \"\n",
    "            + str(epoch)\n",
    "            + \"| Loss Value: \"\n",
    "            + str(loss)\n",
    "            + \"| F1 Score: \"\n",
    "            + str(f1_score / n_batches)\n",
    "            + \"| PPL: \"\n",
    "            + str(ppl)\n",
    "            + \"| Time Took: \"\n",
    "            + str(time.time() - t1)\n",
    "            + \" |\"\n",
    "        )\n",
    "        print(\"=\" * 100)\n",
    "        total_time += time.time() - t1\n",
    "\n",
    "    print(\"| Training Finished | Total Training Time: \" + str(total_time) + \" |\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kwbJnrp1V-R8"
   },
   "outputs": [],
   "source": [
    "def get_parameter_count(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8vivb-Oon46j"
   },
   "outputs": [],
   "source": [
    "### Hyperparameters\n",
    "\n",
    "N = 2\n",
    "d_model = 256\n",
    "d_ff = 512\n",
    "num_head = 8\n",
    "dropout = 0.1\n",
    "\n",
    "batch_size = 10\n",
    "n_batches = 4000\n",
    "n_epochs = 201\n",
    "save_every = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5833,
     "status": "ok",
     "timestamp": 1588205431974,
     "user": {
      "displayName": "Deepak Goyal",
      "photoUrl": "",
      "userId": "12155914074048018284"
     },
     "user_tz": -330
    },
    "id": "Ez4Swx52EfUT",
    "outputId": "ba9d1bef-a77c-4854-ef54-6d7f5452a82e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000\n"
     ]
    }
   ],
   "source": [
    "batches = data_generation(pairs, batch_size, n_batches)\n",
    "print(len(batches))\n",
    "# for i in range(5):\n",
    "#     print(str(batches[i].src)+\" \"+str(batches[i].trg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4852,
     "status": "ok",
     "timestamp": 1588205431975,
     "user": {
      "displayName": "Deepak Goyal",
      "photoUrl": "",
      "userId": "12155914074048018284"
     },
     "user_tz": -330
    },
    "id": "b67TIlqdEj-O",
    "outputId": "f0e6275e-5e40-4688-ab2d-36c7dad85cc3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialising and creating models....\n",
      "Number of learnable parameters for this model: 8647304\n",
      "====================================================================================================\n",
      "Creating Models took: 0.14099717140197754\n",
      "| Training Finished | Total Training Time: 26408.956478834152 |\n"
     ]
    }
   ],
   "source": [
    "print(\"Initialising and creating models....\")\n",
    "V = voc.num_words\n",
    "t1 = time.time()\n",
    "\n",
    "\n",
    "model = make_model(V, V, N, d_model, d_ff, num_head, dropout)\n",
    "model.to(device)\n",
    "\n",
    "learnable_parameter_count = get_parameter_count(model)\n",
    "\n",
    "print(\n",
    "    \"Number of learnable parameters for this model: \" + str(learnable_parameter_count)\n",
    ")\n",
    "\n",
    "model_opt = torch.optim.Adam(\n",
    "    model.parameters(), lr=0.00001, betas=(0.9, 0.98), eps=1e-9\n",
    ")\n",
    "print(\"=\" * 100)\n",
    "print(\"Creating Models took: \" + str(time.time() - t1))\n",
    "\n",
    "save_dir = \"/content/drive/My Drive/Model Data\"\n",
    "# loadFile=os.path.join(save_dir,'transformer','cornell-movie','2_checkpoint')\n",
    "# loadFile=\"C:\\\\Users\\\\deepa\\\\Conversational Agents\\\\transformer\\\\cornell-movie\\\\1_checkpoint.tar\"\n",
    "loadFile = \"/content/drive/My Drive/Model Data/transformer/195_checkpoint.tar\"\n",
    "loadFile = \"/content/drive/My Drive/Model Data/transformer/cornell-movie-40K/200_checkpoint.tar\"\n",
    "# loadFile=None\n",
    "\n",
    "if loadFile:\n",
    "    checkpoint = torch.load(loadFile)\n",
    "    model.load_state_dict(checkpoint[\"model\"])\n",
    "    model_opt.load_state_dict(checkpoint[\"opt\"])\n",
    "\n",
    "\n",
    "model.train()\n",
    "training(batches, model, n_epochs, n_batches, model_opt, loadFile, save_every)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eSNOkTzVEoLO"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "\n",
    "def greedy_decode(model, src, src_mask, max_len, start_symbol, trg=None):\n",
    "    memory = model.encode(src, src_mask)\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type_as(src.data)\n",
    "    loss_val = 0\n",
    "\n",
    "    for i in range(max_len - 1):\n",
    "        out = model.decode(\n",
    "            memory,\n",
    "            src_mask,\n",
    "            Variable(ys),\n",
    "            Variable(subsequent_mask(ys.size(1)).type_as(src.data)),\n",
    "        )\n",
    "\n",
    "        prob = model.generator(out[:, -1])\n",
    "        if trg != None:\n",
    "            loss_val += F.cross_entropy(prob, trg[0][i].view(-1)).item()\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.data[0]\n",
    "        ys = torch.cat([ys, torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1)\n",
    "    return ys, loss_val / (max_len - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XvCmMgMlxlNN"
   },
   "outputs": [],
   "source": [
    "def testing(model, pairs, starting_point):\n",
    "\n",
    "    loss = 0\n",
    "    F1 = 0\n",
    "    num = 0\n",
    "    for i in range(starting_point, len(pairs)):\n",
    "        test_data = data_generation(pairs, 1, 1, i)\n",
    "\n",
    "        source = test_data[0].src.view(-1, 12).to(device)\n",
    "        source_mask = test_data[0].src_mask.view(1, -1, 12).to(device)\n",
    "        target = test_data[0].trg.view(-1, 11).to(device)\n",
    "        # print(target[0][0].view(1,-1))\n",
    "        output, curr_loss = greedy_decode(model, source, source_mask, 10, 1, target)\n",
    "        curr_f1 = F1_score(output, target)\n",
    "        loss += curr_loss\n",
    "        F1 += curr_f1\n",
    "        num = num + 1\n",
    "        print(num)\n",
    "    return loss / num, F1 / num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HNZIiXrEaW76"
   },
   "outputs": [],
   "source": [
    "class BeamNode:\n",
    "    def __init__(self, previous_node, word_id, length, score):\n",
    "\n",
    "        self.previous_node = previous_node\n",
    "        self.word_id = word_id\n",
    "        self.length = length\n",
    "        self.score = score\n",
    "\n",
    "    def eval(self):\n",
    "        return self.score / self.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PxoTcWqNYwsy"
   },
   "outputs": [],
   "source": [
    "def beam_search(\n",
    "    model,\n",
    "    source,\n",
    "    source_mask,\n",
    "    max_len,\n",
    "    start_symbol,\n",
    "    beam_size,\n",
    "    max_queue_size,\n",
    "    num_sentences,\n",
    "):\n",
    "\n",
    "    encoder_outputs = model.encode(source, source_mask)\n",
    "\n",
    "    decoder_input = torch.ones(1, 1).fill_(start_symbol).type_as(source.data)\n",
    "\n",
    "    node = BeamNode(None, decoder_input, 1, 0)\n",
    "\n",
    "    nodes = PriorityQueue()\n",
    "\n",
    "    nodes.put((-node.eval(), node))\n",
    "    queue_size = 1\n",
    "\n",
    "    sentence_nodes = []\n",
    "\n",
    "    while True:\n",
    "\n",
    "        if queue_size > max_queue_size:\n",
    "            break\n",
    "\n",
    "        curr_score, curr_node = nodes.get()\n",
    "\n",
    "        # decoder_input=curr_node.word_id\n",
    "\n",
    "        if queue_size != 1:\n",
    "            decoder_input = torch.cat(\n",
    "                [\n",
    "                    decoder_input,\n",
    "                    torch.ones(1, 1)\n",
    "                    .type_as(source.data)\n",
    "                    .fill_(curr_node.word_id.item()),\n",
    "                ],\n",
    "                dim=1,\n",
    "            )\n",
    "        else:\n",
    "            decoder_input = curr_node.word_id\n",
    "\n",
    "        if curr_node.word_id.item() == 0 and curr_node.previous_node != None:\n",
    "            sentence_nodes.append((curr_score, curr_node))\n",
    "            break\n",
    "\n",
    "        decoder_output = model.decode(\n",
    "            encoder_outputs,\n",
    "            source_mask,\n",
    "            Variable(decoder_input),\n",
    "            Variable(subsequent_mask(decoder_input.size(1)).type_as(source.data)),\n",
    "        )\n",
    "\n",
    "        prob = model.generator(decoder_output[:, -1])\n",
    "\n",
    "        log_prob, indexes = torch.topk(prob, beam_size)\n",
    "        nextnodes = []\n",
    "\n",
    "        for i in range(beam_size):\n",
    "            decoded_t = indexes[0][i]\n",
    "            log_p = log_prob[0][i].item()\n",
    "\n",
    "            node = BeamNode(\n",
    "                curr_node, decoded_t, curr_node.length + 1, curr_node.score + log_p\n",
    "            )\n",
    "            score = -node.eval()\n",
    "            nextnodes.append((score, node))\n",
    "\n",
    "        for i in range(len(nextnodes)):\n",
    "            score, nn = nextnodes[i]\n",
    "            nodes.put((score, nn))\n",
    "\n",
    "        queue_size += len(nextnodes) - 1\n",
    "\n",
    "    if len(sentence_nodes) == 0:\n",
    "        sentence_nodes = [nodes.get() for _ in range(numb_sentences)]\n",
    "\n",
    "    utterances = []\n",
    "\n",
    "    for score, n in sorted(sentence_nodes, key=operator.itemgetter(0)):\n",
    "        curr_utterance = []\n",
    "        curr_utterance.append(n.word_id)\n",
    "\n",
    "        while n.previous_node != None:\n",
    "            n = n.previous_node\n",
    "            curr_utterance.append(n.word_id)\n",
    "        curr_utterance = curr_utterance[::-1]\n",
    "        utterances.append(curr_utterance)\n",
    "\n",
    "    return utterances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dAj4kZlOF96b"
   },
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    batch = batches[i]\n",
    "    for i in range(10):\n",
    "        source = batch.src[i].view(-1, 12).to(device)\n",
    "        source_mask = batch.src_mask[i].view(1, -1, 12).to(device)\n",
    "        # output=beam_search(model,source,source_mask,10,1,2,200,1)\n",
    "        output = greedy_decode(model, source, source_mask, 10, 1)\n",
    "        src = batch.src[i].view(-1)\n",
    "        trg = batch.trg[i].view(-1)\n",
    "        # pred=torch.LongTensor(output[0]).view(-1)\n",
    "        pred = output.view(-1)\n",
    "        # print(src.size())\n",
    "        # for id in src:\n",
    "        #     print(id)\n",
    "        src_sentence = [voc.index2word[id.item()] for id in src]\n",
    "        trg_sentence = [voc.index2word[id.item()] for id in trg]\n",
    "        pred_sentence = [voc.index2word[id.item()] for id in pred]\n",
    "        print(src_sentence)\n",
    "        print(trg_sentence)\n",
    "        print(pred_sentence)\n",
    "        print(\"-\" * 80)\n",
    "#         print(\"-\"*80)\n",
    "#         print(str(output)+\" \"+str(batch.src[i])+\" \"+str(batch.trg[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R2lay0ahGArk"
   },
   "outputs": [],
   "source": [
    "def evaluate(model,sentence,voc,max_length):\n",
    "\n",
    "    tokenised_sentence=[indexesFromSentence(voc,sentence)]\n",
    "    input_sentence=torch.LongTensor(tokenised_sentence)\n",
    "    input_sentence_mask=(input_sentence!=0)\n",
    "\n",
    "    input_sentence=input_sentence.view(-1,max_length+2)\n",
    "    input_sentence_mask=input_sentence_mask.view(1,-1,max_length+2)\n",
    "\n",
    "    # output_sentence,_=greedy_decode(model,input_sentence.to(device),input_sentence_mask.to(device),max_length,1)\n",
    "    # output_sentence=torch.LongTensor(beam_search(model,input_sentence.to(device),input_sentence_mask.to(device),10,1,20,2000,1)[0])\n",
    "    output_sentence=top_p_top_k(model,input_sentence.to(device),input_sentence_mask.to(device),max_length,1,1.0,0,0.5)\n",
    "\n",
    "    output_sentence=output_sentence.view(-1)\n",
    "\n",
    "    decoded_words=[voc.index2word[id.item()] for id in output_sentence]\n",
    "\n",
    "    return decoded_words\n",
    "\n",
    "def evaluateInput(model,voc,max_length):\n",
    "    input_sentence=''\n",
    "\n",
    "    while(1):\n",
    "        try:\n",
    "            input_sentence=input(\"Human: \")\n",
    "            if input_sentence=='q' or input_sentence=='quit':\n",
    "                break\n",
    "            input_sentence=normalizeString(input_sentence)\n",
    "\n",
    "            output_words=evaluate(model,input_sentence,voc,max_length)\n",
    "            output_words[:]=[x for x in output_words if not(x==\"SOS\" or x==\"EOS\" or x==\"PAD\")]\n",
    "\n",
    "            print(\"Machine:\",\" \".join(output_words))\n",
    "\n",
    "        except KeyError:\n",
    "            print(\"Unkown Word..!!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 347
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 114974,
     "status": "ok",
     "timestamp": 1588205892408,
     "user": {
      "displayName": "Deepak Goyal",
      "photoUrl": "",
      "userId": "12155914074048018284"
     },
     "user_tz": -330
    },
    "id": "njCaog7vq0YH",
    "outputId": "5409d1dd-7950-4a0d-ca9b-6a297d597da5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: hello\n",
      "Machine: hi .\n",
      "Human: what are you doing ?\n",
      "Machine: i m sorry .\n",
      "Human: which is you favourite song\n",
      "Machine: it s why you been there .\n",
      "Human: okay\n",
      "Machine: you ll get it .\n",
      "Human: whom do you live with ?\n",
      "Machine: the store .\n",
      "Human: where do you live ?\n",
      "Machine: right here .\n",
      "Human: who do you work for >\n",
      "Machine: for seven will do you ?\n",
      "Human: who do you work for ?\n",
      "Machine: for a stupid friend .\n",
      "Human: oh\n",
      "Machine: what ?\n",
      "Human: quit\n"
     ]
    }
   ],
   "source": [
    "evaluateInput(model,voc,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rPEztuLi7aLA"
   },
   "outputs": [],
   "source": [
    "def top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):\n",
    "    \"\"\" Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n",
    "        Args:\n",
    "            logits: logits distribution shape (vocabulary size)\n",
    "            top_k >0: keep only top k tokens with highest probability (top-k filtering).\n",
    "            top_p >0.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n",
    "                Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)\n",
    "    \"\"\"\n",
    "    assert logits.dim() == 1  # batch size 1 for now - could be updated for more but the code would be less clear\n",
    "    top_k = min(top_k, logits.size(-1))  # Safety check\n",
    "    if top_k > 0:\n",
    "        # Remove all tokens with a probability less than the last token of the top-k\n",
    "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "\n",
    "    if top_p > 0.0:\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "        # Remove tokens with cumulative probability above the threshold\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        # Shift the indices to the right to keep also the first token above the threshold\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WDrGxYBJ5GIv"
   },
   "outputs": [],
   "source": [
    "def top_p_top_k(model, src, src_mask,max_len, start_symbol,temperature,top_k,top_p):\n",
    "    memory = model.encode(src, src_mask)\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type_as(src.data)\n",
    "    loss_val=0\n",
    "\n",
    "    for i in range(max_len-1):\n",
    "        out = model.decode(memory, src_mask, \n",
    "                           Variable(ys), \n",
    "                           Variable(subsequent_mask(ys.size(1))\n",
    "                                    .type_as(src.data)))\n",
    "        \n",
    "        logits = model.generator(out[:, -1])\n",
    "        logits=logits/temperature\n",
    "\n",
    "        filtered_logits = top_k_top_p_filtering(logits.view(-1), top_k=top_k, top_p=top_p)\n",
    "        probabilities = F.softmax(filtered_logits, dim=-1)\n",
    "        next_word = torch.multinomial(probabilities, 1)\n",
    "        \n",
    "        ys = torch.cat([ys, \n",
    "                        torch.ones(1, 1).type_as(src.data).fill_(next_word.item())], dim=1)\n",
    "    return ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vzl39hjo5Gk0"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "v5xV-42LAX6d"
   ],
   "name": "transformer_cornell.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
