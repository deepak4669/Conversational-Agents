{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"seq2seq_withattn.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPCOUCKED/6WIW93c4jT6ML"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"3JqhjAnVSaCY","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":124},"executionInfo":{"status":"ok","timestamp":1598128532618,"user_tz":-330,"elapsed":34111,"user":{"displayName":"deepak goyal","photoUrl":"https://lh5.googleusercontent.com/-3gfEZruD9Sk/AAAAAAAAAAI/AAAAAAAACvg/54H4lMsdOgA/s64/photo.jpg","userId":"05164064759516400423"}},"outputId":"87a208c0-cf59-4287-c428-ff0af3ff7cfe"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8LceekfUUWxL","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":572},"executionInfo":{"status":"ok","timestamp":1598128572216,"user_tz":-330,"elapsed":24175,"user":{"displayName":"deepak goyal","photoUrl":"https://lh5.googleusercontent.com/-3gfEZruD9Sk/AAAAAAAAAAI/AAAAAAAACvg/54H4lMsdOgA/s64/photo.jpg","userId":"05164064759516400423"}},"outputId":"f2589e10-cd0b-4ca9-f597-222802b80d3b"},"source":["!nvidia-smi \n","!pip install wandb -q"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Sat Aug 22 20:35:55 2020       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 450.57       Driver Version: 418.67       CUDA Version: 10.1     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   38C    P8    10W /  70W |      0MiB / 15079MiB |      0%      Default |\n","|                               |                      |                 ERR! |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n","\u001b[K     |████████████████████████████████| 1.4MB 4.9MB/s \n","\u001b[K     |████████████████████████████████| 102kB 12.0MB/s \n","\u001b[K     |████████████████████████████████| 163kB 39.1MB/s \n","\u001b[K     |████████████████████████████████| 122kB 23.4MB/s \n","\u001b[K     |████████████████████████████████| 102kB 9.4MB/s \n","\u001b[K     |████████████████████████████████| 71kB 8.4MB/s \n","\u001b[K     |████████████████████████████████| 71kB 9.5MB/s \n","\u001b[?25h  Building wheel for gql (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for watchdog (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for graphql-core (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"CRbeP6r1VmTD","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598128575799,"user_tz":-330,"elapsed":27339,"user":{"displayName":"deepak goyal","photoUrl":"https://lh5.googleusercontent.com/-3gfEZruD9Sk/AAAAAAAAAAI/AAAAAAAACvg/54H4lMsdOgA/s64/photo.jpg","userId":"05164064759516400423"}}},"source":["import os\n","import re\n","import copy\n","import math\n","import time\n","import random\n","import pickle\n","import itertools\n","import numpy as np\n","from collections import namedtuple, Counter\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","\n","import wandb\n","import logging\n","logging.propagate=False\n","logging.getLogger().setLevel(logging.ERROR)\n"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"5i5yWtmBH7LU","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1598128575803,"user_tz":-330,"elapsed":26717,"user":{"displayName":"deepak goyal","photoUrl":"https://lh5.googleusercontent.com/-3gfEZruD9Sk/AAAAAAAAAAI/AAAAAAAACvg/54H4lMsdOgA/s64/photo.jpg","userId":"05164064759516400423"}},"outputId":"04f273c6-74a9-45f3-a78f-f6d73affa52e"},"source":["device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"The device found: \"+str(device))"],"execution_count":4,"outputs":[{"output_type":"stream","text":["The device found: cuda\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Z565czS1IFdU","colab_type":"text"},"source":["# Preprocessing\n"]},{"cell_type":"code","metadata":{"id":"c9AkyMgfWetG","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":69},"executionInfo":{"status":"ok","timestamp":1598128575804,"user_tz":-330,"elapsed":25880,"user":{"displayName":"deepak goyal","photoUrl":"https://lh5.googleusercontent.com/-3gfEZruD9Sk/AAAAAAAAAAI/AAAAAAAACvg/54H4lMsdOgA/s64/photo.jpg","userId":"05164064759516400423"}},"outputId":"f45c7833-8735-4046-8ece-fe0a0c203daa"},"source":["path='/content/drive/My Drive/Data'\n","dataset='cornell movie-dialogs corpus'\n","\n","data_folder=os.path.join(path,dataset)\n","\n","train_path=os.path.join(data_folder,'train')\n","test_path=os.path.join(data_folder,'test')\n","\n","print(\"The final data corpus folder: \"+str(data_folder))\n","print(\"The training data folder: \"+str(train_path))\n","print(\"The testing data folder: \"+str(test_path))"],"execution_count":5,"outputs":[{"output_type":"stream","text":["The final data corpus folder: /content/drive/My Drive/Data/cornell movie-dialogs corpus\n","The training data folder: /content/drive/My Drive/Data/cornell movie-dialogs corpus/train\n","The testing data folder: /content/drive/My Drive/Data/cornell movie-dialogs corpus/test\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"sNFBnhpYWhB4","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598128575805,"user_tz":-330,"elapsed":25627,"user":{"displayName":"deepak goyal","photoUrl":"https://lh5.googleusercontent.com/-3gfEZruD9Sk/AAAAAAAAAAI/AAAAAAAACvg/54H4lMsdOgA/s64/photo.jpg","userId":"05164064759516400423"}}},"source":["def get_lines_conversations():\n","    \"\"\"\n","    Loads movie lines and conversations from the dataset.\n","    \n","    @returns list[lines],list[conversations]: The lines and conversations in the cornell movie dataset\n","    \"\"\"\n","    movie_lines=[]\n","    movie_conversations=[]\n","\n","    with open(os.path.join(data_folder,'movie_lines.txt'),'r',encoding='iso-8859-1') as f:\n","        for line in f:\n","            movie_lines.append(line)\n","    \n","    with open(os.path.join(data_folder,'movie_conversations.txt'),'r', encoding='iso-8859-1') as f:\n","        for line in f:\n","            movie_conversations.append(line)\n","                                       \n","\n","    return movie_lines,movie_conversations"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"_j92fLQrW1pq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":173},"executionInfo":{"status":"ok","timestamp":1598128577799,"user_tz":-330,"elapsed":27499,"user":{"displayName":"deepak goyal","photoUrl":"https://lh5.googleusercontent.com/-3gfEZruD9Sk/AAAAAAAAAAI/AAAAAAAACvg/54H4lMsdOgA/s64/photo.jpg","userId":"05164064759516400423"}},"outputId":"d58eb82e-f455-4a0f-c743-cdcac5f8bdf0"},"source":["t1=time.time()\n","print(\"Extracting movie lines and movie conversations...\")\n","movie_lines,movie_conversations=get_lines_conversations()\n","\n","print(\"Number of distinct lines: \"+str(len(movie_lines)))\n","print(\"Number of conversations: \"+str(len(movie_conversations)))\n","print(\"Average Number of lines per conversations: \"+str(len(movie_lines)/len(movie_conversations)))\n","\n","print(movie_lines[0])\n","print(movie_conversations[0])\n","\n","print(\"Extracting took place in: \"+str(time.time()-t1)+\" seconds\")"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Extracting movie lines and movie conversations...\n","Number of distinct lines: 304713\n","Number of conversations: 83097\n","Average Number of lines per conversations: 3.6669554857576085\n","L1045 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ They do not!\n","\n","u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L194', 'L195', 'L196', 'L197']\n","\n","Extracting took place in: 1.772658109664917 seconds\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"eG0A5czeXz9H","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598128577802,"user_tz":-330,"elapsed":27131,"user":{"displayName":"deepak goyal","photoUrl":"https://lh5.googleusercontent.com/-3gfEZruD9Sk/AAAAAAAAAAI/AAAAAAAACvg/54H4lMsdOgA/s64/photo.jpg","userId":"05164064759516400423"}}},"source":["def loadLines(movie_lines,fields):\n","    \"\"\" The function to load based on id from the movie_lines from above\n","\n","    @param movie_lines(List of Lines): The lines extracted from above.\n","    @param fields(List of Strings): Each line in movie_lines is a dictionary with keys described by this.\n","\n","    @returns dictionary[lineID:line]: Line corresponding to each line_id.\n","\n","    \"\"\"\n","    lines={}\n","    for lineid in range(len(movie_lines)):\n","        \n","        line=movie_lines[lineid]\n","        values=line.split(\" +++$+++ \")\n","        lineVals={}\n","           \n","        for i,field in enumerate(fields):\n","            lineVals[field]=values[i]\n","            \n","        lines[lineVals['lineID']]=lineVals\n","    \n","    return lines\n","\n","def loadConversations(movie_conversations,lines,fields):\n","    \"\"\" The function to load lines of the conversation\n","\n","    @param movie_conversations List[String]: Movie Conversation extracted from the original dataset\n","    @param lines Dict[lineID:line]: Extracted from loadLines function\n","    @param fields List[String]: Fields we have for each string from conversation element from original conversation data\n","\n","    @returns List[Dict]: Conversations extracted from original data from raw Strings\n","    \"\"\"\n","\n","    conversations=[]\n","    \n","    for convo in movie_conversations:\n","        values=convo.split(\" +++$+++ \")\n","        conVals={}\n","       \n","        for i,field in enumerate(fields):\n","            conVals[field]=values[i]\n","        \n","        lineIDs=eval(conVals[\"utteranceIDs\"])\n","        \n","        conVals[\"lines\"]=[]\n","        \n","        for lineID in lineIDs:\n","            conVals[\"lines\"].append(lines[lineID])\n","        conversations.append(conVals)\n","        \n","    return conversations\n","\n","def sentencePairs(conversations):\n","    \"\"\"\n","        The function to give query based pairs from each conversation.\n","\n","    @param conversations List[Dict]: The conversations retrieved from previous function\n","\n","    @returns List[List]: The query response pairs for each conversational exchange\n","\n","    \"\"\"\n","    qr_pairs=[]\n","    \n","    for conversation in conversations:\n","        for i in range(len(conversation[\"lines\"])-1):\n","            query=conversation[\"lines\"][i][\"text\"].strip()\n","            response=conversation[\"lines\"][i+1][\"text\"].strip()\n","            \n","            if query and response:\n","                qr_pairs.append([query,response])\n","        \n","    return qr_pairs"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"Bh37EXhxYQAJ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":69},"executionInfo":{"status":"ok","timestamp":1598128579597,"user_tz":-330,"elapsed":28803,"user":{"displayName":"deepak goyal","photoUrl":"https://lh5.googleusercontent.com/-3gfEZruD9Sk/AAAAAAAAAAI/AAAAAAAACvg/54H4lMsdOgA/s64/photo.jpg","userId":"05164064759516400423"}},"outputId":"e407343f-0919-48ca-c41e-89180547a43a"},"source":["t1=time.time()\n","print(\"Separating meaningfull information for our model...\")\n","\n","lines={}\n","conversations=[]\n","qr_pairs=[]\n","\n","movie_lines_fields=[\"lineID\",\"characterID\",\"movieID\",\"character\",\"text\"]\n","movie_convo_fields=[\"charcaterID\",\"character2ID\",\"movieID\",\"utteranceIDs\"]\n","\n","lines=loadLines(movie_lines,movie_lines_fields)\n","conversations=loadConversations(movie_conversations,lines,movie_convo_fields)\n","qr_pairs=sentencePairs(conversations)\n","\n","print(\"The number of query-response pairs are: \"+str(len(qr_pairs)))\n","print(\"Separation took place in: \"+str(time.time()-t1))"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Separating meaningfull information for our model...\n","The number of query-response pairs are: 221282\n","Separation took place in: 1.88006591796875\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"t1TuGphxCEVj","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598128579600,"user_tz":-330,"elapsed":28563,"user":{"displayName":"deepak goyal","photoUrl":"https://lh5.googleusercontent.com/-3gfEZruD9Sk/AAAAAAAAAAI/AAAAAAAACvg/54H4lMsdOgA/s64/photo.jpg","userId":"05164064759516400423"}}},"source":["PAD_Token=0\n","START_Token=1\n","END_Token=2\n","\n","Min_Count=3\n","Max_Length=10"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"p5vYOofuzsaP","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598128579602,"user_tz":-330,"elapsed":28452,"user":{"displayName":"deepak goyal","photoUrl":"https://lh5.googleusercontent.com/-3gfEZruD9Sk/AAAAAAAAAAI/AAAAAAAACvg/54H4lMsdOgA/s64/photo.jpg","userId":"05164064759516400423"}}},"source":["class Vocabulary:\n","    \"\"\"\n","        Vocabulary class for the words in the data\n","    \"\"\"\n","\n","    def __init__(self):\n","\n","        self.trimmed=False\n","        self.word2count={}\n","        self.index2word={PAD_Token:\"PAD\",START_Token:\"SOS\",END_Token:\"EOS\"}\n","        self.word2index={\"PAD\":PAD_Token,\"SOS\":START_Token,\"EOS\":END_Token}\n","        self.num_words=3\n","        \n","    def addSentence(self,sentence):\n","        \"\"\" Take into the account all the words occuring in the input sentence\n","\n","        @param String [sentence]: Input sentence\n","\n","        \"\"\"\n","        for word in sentence.split(\" \"):\n","            self.addWord(word)\n","\n","    def addWord(self,word):\n","        \"\"\" Takes into consideration of the word appeared in the corpus\n","\n","        @param word: A single word\n","\n","        \"\"\"\n","        if word not in self.word2index:\n","            self.word2index[word]=self.num_words\n","            self.index2word[self.num_words]=word\n","            self.word2count[word]=1\n","            self.num_words=self.num_words+1\n","        else:\n","            self.word2count[word]+=1\n","            \n","    def trim(self,min_count):\n","        \"\"\" Reconstructs the entire vocabulary by removing words with frequency less than min_count\n","\n","        @param min_count(int): The count threshold you want to keep for the words\n","\n","        \"\"\"\n","        \n","        if self.trimmed:\n","            return\n","        self.trimmed=True\n","        \n","        keep_words=[]\n","        \n","        for word,freq in self.word2count.items():\n","            if freq>=min_count:\n","                keep_words.append(word)\n","        \n","        self.word2count={}\n","        self.index2word={PAD_Token:\"PAD\",START_Token:\"SOS\",END_Token:\"EOS\"}\n","        self.word2index={\"PAD\":PAD_Token,\"SOS\":START_Token,\"EOS\":END_Token}\n","        self.num_words=3\n","        \n","        for word in keep_words:\n","            self.addWord(word)"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"MEPrFn1h4sFp","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1598128585363,"user_tz":-330,"elapsed":33943,"user":{"displayName":"deepak goyal","photoUrl":"https://lh5.googleusercontent.com/-3gfEZruD9Sk/AAAAAAAAAAI/AAAAAAAACvg/54H4lMsdOgA/s64/photo.jpg","userId":"05164064759516400423"}},"outputId":"487748c6-9fdb-42f8-d0bf-a025eb91f461"},"source":["\n","def normalizeString(s):\n","    \"\"\" Preprocess sentence given. Space between punctuations, lowercase all the letters.\n","\n","    @param String : The sentence to undergo preprocessing\n","\n","    @returns String: The preprocessed string\n","    \"\"\"\n","    s=s.lower().strip()\n","    s=re.sub(r\"([.!?])\", r\" \\1\", s)\n","    s=re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n","    s=re.sub(r\"\\s+\", r\" \", s).strip()\n","    return s\n","\n","def readVocs(qr_pairs):\n","    \"\"\" Normalise each of the sentence in the query-reponse pair and create vocab\n","\n","    @param qr_pair List[List[]]: The list query response pairs\n","    \n","    @returns Voc(Vocabulary Object), qr_pairs(List[List]): The vocabulary and the processed q-r pairs list\n","\n","    \"\"\"\n","    \n","    for qr_pair in qr_pairs:\n","        qr_pair[0]=normalizeString(qr_pair[0])\n","        qr_pair[1]=normalizeString(qr_pair[1])\n","    \n","    voc=Vocabulary()\n","    return voc,qr_pairs\n","\n","def filterPair(pair):\n","    \"\"\" Checks whether a pair(both sentences) have words <Max_length(Globally defined)\n","\n","    @param pair list[q,r]: The particular pair\n","\n","    @returns boolean: Whether pair has both sentences having length less than the Max_length\n","    \"\"\"\n","    return len(pair[0].split(\" \"))<Max_Length and len(pair[1].split(\" \"))<Max_Length\n","\n","def filterPairs(qr_pairs):\n","    \"\"\" Filter pairs which have any sentence having length greater than Max_length\n","\n","    @param qr_pair List[List[q,r]]: The list of query-response pairs\n","\n","    @returns List[List[q,r]]: The filtered out list of query response pairs\n","    \"\"\"\n","    return [pair for pair in qr_pairs if filterPair(pair)]\n","\n","def prepareDataset(qr_pairs):\n","    \"\"\" Prepares vocabulary and preprocesses sentences from the dataset\n","\n","    @param qr_pairs List[List[q,r]]: The list of query-response pairs\n","\n","    @returns Vocabulary Object, List[List[q,r]]: The prepared vocabulary and the processed query-response pairs\n","\n","    \"\"\"\n","\n","    voc, qr_pairs=readVocs(qr_pairs)\n","    qr_pairs=filterPairs(qr_pairs)\n","       \n","    for pair in qr_pairs:\n","        voc.addSentence(pair[0])\n","        voc.addSentence(pair[1])\n","\n","    return voc,qr_pairs\n","\n","t1=time.time()\n","print(\"Preparing dataset and corresponding vocabulary...\")\n","voc, pairs=prepareDataset(qr_pairs)\n","print(\"Preparation took place in: \"+str(time.time()-t1))"],"execution_count":12,"outputs":[{"output_type":"stream","text":["Preparing dataset and corresponding vocabulary...\n","Preparation took place in: 5.679647207260132\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wv1ruAcB4wxK","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1598128585365,"user_tz":-330,"elapsed":33561,"user":{"displayName":"deepak goyal","photoUrl":"https://lh5.googleusercontent.com/-3gfEZruD9Sk/AAAAAAAAAAI/AAAAAAAACvg/54H4lMsdOgA/s64/photo.jpg","userId":"05164064759516400423"}},"outputId":"bd695552-78c9-4a26-e807-c565790b1f9a"},"source":["def trimRareWords(voc,qr_pairs,min_count):\n","    \"\"\" Trims the rare words from the vocabulary with min_count threshold. This also\n","        removes those pairs which contains these words.\n","\n","    @param voc Vocabulary Object: The vocabulary so far created\n","    @param qr_pairs List[List[]]: The list of query-response pairs\n","    @param min_count Integer: The threshold below which frequency words are removed\n","\n","    @returns List[List[]]: The query-response with pairs removed pairs which contains less freq\n","                           words\n","\n","    \"\"\"\n","    \n","    voc.trim(min_count)\n","    keep_pairs=[]\n","    \n","    for pair in qr_pairs:\n","        input_sentence=pair[0]\n","        output_sentence=pair[1]\n","        \n","        keep_input=True\n","        keep_output=True\n","        \n","        for word in input_sentence.split(\" \"):\n","            if word not in voc.word2index:\n","                keep_input=False\n","                break\n","        \n","        for word in output_sentence.split(\" \"):\n","            if word not in voc.word2index:\n","                keep_output=False\n","                break\n","                \n","        if keep_input and keep_output:\n","            keep_pairs.append(pair)\n","            \n","    return keep_pairs\n","\n","t1=time.time()\n","print(\"Trimming rare words from vocabulary and dataset..\")\n","\n","pairs=trimRareWords(voc,pairs,Min_Count)\n","\n","print(\"Trimming took place in: \"+str(time.time()-t1))"],"execution_count":13,"outputs":[{"output_type":"stream","text":["Trimming rare words from vocabulary and dataset..\n","Trimming took place in: 0.1347339153289795\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"JAgPL9At_wrR","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598128585366,"user_tz":-330,"elapsed":33424,"user":{"displayName":"deepak goyal","photoUrl":"https://lh5.googleusercontent.com/-3gfEZruD9Sk/AAAAAAAAAAI/AAAAAAAACvg/54H4lMsdOgA/s64/photo.jpg","userId":"05164064759516400423"}}},"source":["random.shuffle(pairs)\n","training_pairs=pairs[0:40000]\n","testing_pairs=pairs[40000:len(pairs)]\n","\n","if not os.path.exists(train_path):\n","    os.makedirs(train_path)\n","    os.makedirs(test_path)\n","    with open(os.path.join(train_path,\"training_data\"),'wb') as fp:\n","        pickle.dump(training_pairs,fp)\n","\n","    with open(os.path.join(test_path,\"testing_pairs\"),\"wb\") as fp:\n","        pickle.dump(testing_pairs,fp)\n","        "],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"v2ts59Zhp3zl","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598128586418,"user_tz":-330,"elapsed":34243,"user":{"displayName":"deepak goyal","photoUrl":"https://lh5.googleusercontent.com/-3gfEZruD9Sk/AAAAAAAAAAI/AAAAAAAACvg/54H4lMsdOgA/s64/photo.jpg","userId":"05164064759516400423"}}},"source":["with open(os.path.join(train_path,\"training_data\"),\"rb\") as fp:\n","    training_data=pickle.load(fp)\n","\n","with open(os.path.join(test_path,\"testing_pairs\"),\"rb\") as fp:\n","    testing_data=pickle.load(fp)\n"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"YVoU5avw4zay","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598128586421,"user_tz":-330,"elapsed":33952,"user":{"displayName":"deepak goyal","photoUrl":"https://lh5.googleusercontent.com/-3gfEZruD9Sk/AAAAAAAAAAI/AAAAAAAACvg/54H4lMsdOgA/s64/photo.jpg","userId":"05164064759516400423"}}},"source":["def indexesFromSentence(voc, sentence):\n","    \"\"\" Replace words in the sentence with the index from the vocabulary\n","    \n","    @param voc: The vocabulary we have created\n","    @param sentence: The sentence to be indexed\n","\n","    @returns List[]: The list of tokens/index corresponding to each word in the sentence\n","\n","    \"\"\"\n","\n","    return [voc.word2index[word] for word in sentence.split(' ')] + [END_Token]\n","\n","\n","def zeroPadding(l, fillvalue=PAD_Token):\n","    \"\"\" The function takes each tokenised sentences and take out each token from their respective position in a single list.\n","\n","    [[1,2,3],[1,2]]==>[[1,1],[2,2],[3,0]]\n","\n","    @l list[list]: The batch of tokens for each sentence (bs,random sizes)\n","    @fillvalue Integer: The padding value to be used\n","\n","    @returns list[list]: Of size (max_length_sentence,bs)\n","\n","    \"\"\"\n","    return list(itertools.zip_longest(*l, fillvalue=fillvalue))\n","\n","def binaryMatrix(l, value=PAD_Token):\n","    \"\"\" Create a binary mask. 0 where pad value appears otherwise its 1.\n","\n","    @param l list[list]: The batch of tokens in the sentences (max_length_sentence,bs)\n","    @param value Integer: The value to which to be masked\n","\n","    @returns m list[list]: The mask corresponding to l (max_length_sentence,bs)\n","    \"\"\"\n","    m = []\n","    for i, seq in enumerate(l):\n","        m.append([])\n","        for token in seq:\n","            if token == PAD_Token:\n","                m[i].append(0)\n","            else:\n","                m[i].append(1)\n","    return m\n","\n","def inputVar(l, voc):\n","    \"\"\" The function to preprocess input sentences. Returns padded input sentences and lengths of each sentences\n","\n","    @param l list[String]: The list of sentences\n","    @param voc Vocabulary: The vocabulary of our corpus\n","\n","    @returns tensor(max_length_sentence,bs),tensor(bs): \n","    \"\"\"\n","    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n","    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n","    padList = zeroPadding(indexes_batch)\n","    padVar = torch.LongTensor(padList)\n","    return padVar, lengths\n","\n","\n","def outputVar(l, voc):\n","    \"\"\" Returns padded output variable, boolean mask and the maximum target length\n","\n","    @param l list[Str]: The list of sentences\n","    @voc voc Vocabulary: The voc of our corpus\n","\n","    @returns tensor(max_length_sentence,bs), tensor(max_length_sentence,bs), integer\n","\n","    \"\"\"\n","\n","    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n","    max_target_len = max([len(indexes) for indexes in indexes_batch])\n","    padList = zeroPadding(indexes_batch)\n","    mask = binaryMatrix(padList)\n","    mask = torch.BoolTensor(mask)\n","    padVar = torch.LongTensor(padList)\n","    return padVar, mask, max_target_len\n","\n","def batch2TrainData(voc, pair_batch):\n","    \"\"\" Returns padded, indexed input and out put vectors,lengths of input sentences,mask, maximum target sentence length\n","\n","    @param voc Vocabulary: The vocabulary of our corpus\n","    @param pair_batch list[list[q,r]]: The query-response pairs list\n","\n","    @returns tensor(max_sentence_length,bs),tensort(bs),output(max_target_length,bs),mask(max_sentence_length,bs), integer\n","    \"\"\"\n","    pair_batch.sort(key=lambda x: len(x[0].split(\" \")), reverse=True)\n","    input_batch, output_batch = [], []\n","    for pair in pair_batch:\n","        input_batch.append(pair[0])\n","        output_batch.append(pair[1])\n","    \n","    inp, lengths = inputVar(input_batch, voc)\n","    output, mask, max_target_len = outputVar(output_batch, voc)\n","    return inp, lengths, output, mask, max_target_len\n"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"d8-18tBuMZMx","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":485},"executionInfo":{"status":"ok","timestamp":1598128586422,"user_tz":-330,"elapsed":33603,"user":{"displayName":"deepak goyal","photoUrl":"https://lh5.googleusercontent.com/-3gfEZruD9Sk/AAAAAAAAAAI/AAAAAAAACvg/54H4lMsdOgA/s64/photo.jpg","userId":"05164064759516400423"}},"outputId":"db0bc130-9dad-4406-d8f8-4108a4217f8e"},"source":["print(\"Number of query-response pairs after all the preprocessing: \"+str(len(pairs)))\n","print(\"Number of Unique Words in our vocabulary: \"+str(voc.num_words))\n","\n","#Sample batch\n","batch=[random.choice(pairs) for _ in range(5)]\n","tokenised_input,input_lengths,tokenised_output,mask,max_out_length=batch2TrainData(voc,batch)\n","\n","print(\"Input length: \"+str(input_lengths)+\" Size: \"+str(input_lengths.shape))\n","print(\"-\"*80)\n","print(\"Tokenised Input: \"+str(tokenised_input)+\" Size: \"+str(tokenised_input.shape))\n","print(\"-\"*80)\n","print(\"Max out length: \"+str(max_out_length)+\" Size: \")\n","print(\"-\"*80)\n","print(\"Mask: \"+str(mask)+\" Size: \"+str(mask.shape))\n","print(\"-\"*80)\n","print(\"Tokenised Output: \"+str(tokenised_output)+\" Size: \"+str(tokenised_output.shape))\n","print(\"-\"*80)"],"execution_count":17,"outputs":[{"output_type":"stream","text":["Number of query-response pairs after all the preprocessing: 53113\n","Number of Unique Words in our vocabulary: 7816\n","Input length: tensor([8, 5, 5, 5, 5]) Size: torch.Size([5])\n","--------------------------------------------------------------------------------\n","Tokenised Input: tensor([[7353,   25,  383,   98,    7],\n","        [   6,  200,    7,   12,  974],\n","        [  50,  483, 2651,   99,  774],\n","        [ 115,    4,    4,    4,    4],\n","        [  36,    2,    2,    2,    2],\n","        [ 530,    0,    0,    0,    0],\n","        [   6,    0,    0,    0,    0],\n","        [   2,    0,    0,    0,    0]]) Size: torch.Size([8, 5])\n","--------------------------------------------------------------------------------\n","Max out length: 5 Size: \n","--------------------------------------------------------------------------------\n","Mask: tensor([[ True,  True,  True,  True,  True],\n","        [ True,  True,  True,  True,  True],\n","        [ True,  True,  True,  True,  True],\n","        [False,  True,  True,  True, False],\n","        [False,  True, False, False, False]]) Size: torch.Size([5, 5])\n","--------------------------------------------------------------------------------\n","Tokenised Output: tensor([[318,  38,  98,  12,  23],\n","        [  4, 266,  50,  99,   6],\n","        [  2, 680,   6,   4,   2],\n","        [  0,   4,   2,   2,   0],\n","        [  0,   2,   0,   0,   0]]) Size: torch.Size([5, 5])\n","--------------------------------------------------------------------------------\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Oppo7HKtKMxW","colab_type":"text"},"source":["# Model"]},{"cell_type":"code","metadata":{"id":"ra7_TfQv42Ra","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598128586423,"user_tz":-330,"elapsed":30698,"user":{"displayName":"deepak goyal","photoUrl":"https://lh5.googleusercontent.com/-3gfEZruD9Sk/AAAAAAAAAAI/AAAAAAAACvg/54H4lMsdOgA/s64/photo.jpg","userId":"05164064759516400423"}}},"source":["class EncoderRNN(nn.Module):\n","    \n","    def __init__(self,hidden_size,embedding,n_layers=1,dropout=0):\n","        \"\"\"\n","        Encoder module for seq2seq architechture.\n","        \"\"\"\n","    \n","        super().__init__()\n","        \n","        self.n_layers=n_layers\n","        self.hidden_size=hidden_size\n","        \n","        self.embedding=embedding\n","        self.gru=nn.GRU(hidden_size,hidden_size,n_layers,dropout=(0 if n_layers==1 else dropout),bidirectional=True)\n","        \n","    def forward(self,input_seq,input_lengths,hidden=None):\n","        \n","        embedded_input=self.embedding(input_seq)\n","        packed=nn.utils.rnn.pack_padded_sequence(embedded_input,input_lengths)\n","        outputs,hidden=self.gru(packed,hidden)\n","        \n","        outputs,_=nn.utils.rnn.pad_packed_sequence(outputs)\n","        \n","        outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:]\n","        \n","        return outputs,hidden\n","\n","    def hidden_init(self,batch_size):\n","        return torch.zeros(self.n_layers*2,batch_size,self.hidden_size,device=device)\n"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"2ZsVL85g49Er","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598128757846,"user_tz":-330,"elapsed":997,"user":{"displayName":"deepak goyal","photoUrl":"https://lh5.googleusercontent.com/-3gfEZruD9Sk/AAAAAAAAAAI/AAAAAAAACvg/54H4lMsdOgA/s64/photo.jpg","userId":"05164064759516400423"}}},"source":["# Luong attention layer\n","class Attn(torch.nn.Module):\n","    def __init__(self, method, hidden_size):\n","        super(Attn, self).__init__()\n","        self.method = method\n","        if self.method not in ['dot', 'general', 'concat']:\n","            raise ValueError(self.method, \"is not an appropriate attention method.\")\n","        self.hidden_size = hidden_size\n","        if self.method == 'general':\n","            self.attn = torch.nn.Linear(self.hidden_size, hidden_size)\n","        elif self.method == 'concat':\n","            self.attn = torch.nn.Linear(self.hidden_size * 2, hidden_size)\n","            self.v = torch.nn.Parameter(torch.FloatTensor(hidden_size))\n","\n","    def dot_score(self, hidden, encoder_output):\n","        return torch.sum(hidden * encoder_output, dim=2)\n","\n","    def general_score(self, hidden, encoder_output):\n","        energy = self.attn(encoder_output)\n","        return torch.sum(hidden * energy, dim=2)\n","\n","    def concat_score(self, hidden, encoder_output):\n","        energy = self.attn(torch.cat((hidden.expand(encoder_output.size(0), -1, -1), encoder_output), 2)).tanh()\n","        return torch.sum(self.v * energy, dim=2)\n","\n","    def forward(self, hidden, encoder_outputs):\n","        # Calculate the attention weights (energies) based on the given method\n","        if self.method == 'general':\n","            attn_energies = self.general_score(hidden, encoder_outputs)\n","        elif self.method == 'concat':\n","            attn_energies = self.concat_score(hidden, encoder_outputs)\n","        elif self.method == 'dot':\n","            attn_energies = self.dot_score(hidden, encoder_outputs)\n","\n","        # Transpose max_length and batch_size dimensions\n","        attn_energies = attn_energies.t()\n","\n","        # Return the softmax normalized probability scores (with added dimension)\n","        return F.softmax(attn_energies, dim=1).unsqueeze(1)"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"id":"S6Vv95PR5TBu","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598128758950,"user_tz":-330,"elapsed":620,"user":{"displayName":"deepak goyal","photoUrl":"https://lh5.googleusercontent.com/-3gfEZruD9Sk/AAAAAAAAAAI/AAAAAAAACvg/54H4lMsdOgA/s64/photo.jpg","userId":"05164064759516400423"}}},"source":["class LuongAttnDecoderRNN(nn.Module):\n","    def __init__(self, attn_model, embedding, hidden_size, output_size, n_layers=1, dropout=0.1):\n","        super(LuongAttnDecoderRNN, self).__init__()\n","\n","        # Keep for reference\n","        self.attn_model = attn_model\n","        self.hidden_size = hidden_size\n","        self.output_size = output_size\n","        self.n_layers = n_layers\n","        self.dropout = dropout\n","\n","        # Define layers\n","        self.embedding = embedding\n","        self.embedding_dropout = nn.Dropout(dropout)\n","        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers == 1 else dropout))\n","        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n","        self.out = nn.Linear(hidden_size, output_size)\n","\n","        self.attn = Attn(attn_model, hidden_size)\n","\n","    def forward(self, input_step, last_hidden, encoder_outputs):\n","        # Note: we run this one step (word) at a time\n","        # Get embedding of current input word\n","        embedded = self.embedding(input_step)\n","        embedded = self.embedding_dropout(embedded)\n","        # Forward through unidirectional GRU\n","        \n","        rnn_output, hidden = self.gru(embedded, last_hidden)\n","        \n","        # Calculate attention weights from the current GRU output\n","        attn_weights = self.attn(rnn_output, encoder_outputs)\n","        # Multiply attention weights to encoder outputs to get new \"weighted sum\" context vector\n","        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))\n","        # Concatenate weighted context vector and GRU output using Luong eq. 5\n","        rnn_output = rnn_output.squeeze(0)\n","        context = context.squeeze(1)\n","        concat_input = torch.cat((rnn_output, context), 1)\n","        concat_output = torch.tanh(self.concat(concat_input))\n","        # Predict next word using Luong eq. 6\n","        output = self.out(concat_output)\n","        # output = F.softmax(output, dim=1)\n","        # Return output and final hidden state\n","        return output, hidden"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"id":"PybGzLUrPRGD","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598128761115,"user_tz":-330,"elapsed":916,"user":{"displayName":"deepak goyal","photoUrl":"https://lh5.googleusercontent.com/-3gfEZruD9Sk/AAAAAAAAAAI/AAAAAAAACvg/54H4lMsdOgA/s64/photo.jpg","userId":"05164064759516400423"}}},"source":["def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"id":"Bqc_v6v09aN3","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598128761462,"user_tz":-330,"elapsed":956,"user":{"displayName":"deepak goyal","photoUrl":"https://lh5.googleusercontent.com/-3gfEZruD9Sk/AAAAAAAAAAI/AAAAAAAACvg/54H4lMsdOgA/s64/photo.jpg","userId":"05164064759516400423"}}},"source":["def make_model(vocabulary_size,d_model=500,num_encoders=1,num_decoders=1,dropout_encoder=0.1,dropout_decoder=0.1,attn_model='general'):\n","\n","    embedding=nn.Embedding(vocabulary_size,d_model)\n","    embedding.weight.requires_grad=False\n","\n","    encoder=EncoderRNN(d_model,embedding,num_encoders,dropout_encoder)\n","    decoder=LuongAttnDecoderRNN(attn_model,embedding,d_model,vocabulary_size,num_decoders,dropout_decoder)\n","\n","    for p in encoder.parameters():\n","        if p.dim()>1:\n","            nn.init.xavier_uniform_(p)\n","    \n","    for p in decoder.parameters():\n","        if p.dim()>1:\n","            nn.init.xavier_uniform_(p)\n","    \n","    num_parameters=0\n","\n","    num_parameters+=count_parameters(encoder)\n","    num_parameters+=count_parameters(decoder)\n","\n","    return encoder,decoder,num_parameters\n","\n"],"execution_count":22,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nxlGp-uOKbLD","colab_type":"text"},"source":["# Training"]},{"cell_type":"code","metadata":{"id":"zUu-jqKopRYD","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":87},"executionInfo":{"status":"ok","timestamp":1598128782909,"user_tz":-330,"elapsed":17863,"user":{"displayName":"deepak goyal","photoUrl":"https://lh5.googleusercontent.com/-3gfEZruD9Sk/AAAAAAAAAAI/AAAAAAAACvg/54H4lMsdOgA/s64/photo.jpg","userId":"05164064759516400423"}},"outputId":"fac08709-3ddb-445b-f8ce-cda470d3e80e"},"source":["!wandb login"],"execution_count":23,"outputs":[{"output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://app.wandb.ai/authorize\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter: 849679b0d800cf9a8ecb95e52bc8d8840d316a1e\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","\u001b[32mSuccessfully logged in to Weights & Biases!\u001b[0m\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"G5RqXDqbpp5g","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598129020069,"user_tz":-330,"elapsed":974,"user":{"displayName":"deepak goyal","photoUrl":"https://lh5.googleusercontent.com/-3gfEZruD9Sk/AAAAAAAAAAI/AAAAAAAACvg/54H4lMsdOgA/s64/photo.jpg","userId":"05164064759516400423"}}},"source":["def f1_score(predictions, targets, average=True):\n","    predictions=predictions.tolist()\n","    targets=targets.tolist()\n","\n","    def f1_score_items(pred_items, gold_items):\n","        common = Counter(gold_items) & Counter(pred_items)\n","        num_same = sum(common.values())\n","\n","        if num_same == 0:\n","            return 0\n","\n","        precision = num_same / len(pred_items)\n","        recall = num_same / len(gold_items)\n","        f1 = (2 * precision * recall) / (precision + recall)\n","\n","        return f1\n","    \n","    scores = [f1_score_items(p, t) for p, t in zip(predictions, targets)]\n","\n","    if average:\n","        return sum(scores) / len(scores)    \n","\n","    return scores"],"execution_count":24,"outputs":[]},{"cell_type":"code","metadata":{"id":"Px-a9O5l8wVo","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598129909211,"user_tz":-330,"elapsed":958,"user":{"displayName":"deepak goyal","photoUrl":"https://lh5.googleusercontent.com/-3gfEZruD9Sk/AAAAAAAAAAI/AAAAAAAACvg/54H4lMsdOgA/s64/photo.jpg","userId":"05164064759516400423"}}},"source":["def test(input_variable,lengths,target_variable,mask,max_target_len,encoder,decoder,batch_size,max_length=Max_Length):\n","\n","\n","    encoder.eval()\n","    decoder.eval()\n","\n","    loss_func=nn.CrossEntropyLoss()\n","\n","    input_variable=torch.tensor(input_variable).to(device)\n","    lengths=lengths.to(device)\n","    target_variable=torch.tensor(target_variable).to(device)\n","    mask=mask.to(device)\n","\n","    loss=0\n","    print_losses=[]\n","    n_totals=0\n","\n","    encoder_hidden=encoder.hidden_init(input_variable.size()[1])\n","    encoder_outputs, encoder_hidden=encoder(input_variable,lengths,encoder_hidden)\n","\n","    decoder_input=torch.LongTensor([[START_Token for _ in range(batch_size)]])\n","    decoder_input=decoder_input.to(device)\n","    decoder_hidden=encoder_hidden[:decoder.n_layers]\n","\n","    predicted=torch.ones(max_target_len,input_variable.size()[1])\n","\n","    for t in range(max_target_len):\n","        decoder_output,decoder_hidden=decoder(decoder_input,decoder_hidden,encoder_outputs)\n","    \n","        _,topi=decoder_output.topk(1)\n","        decoder_input=torch.LongTensor([[topi[i][0] for i in range(batch_size)]])\n","        decoder_input=decoder_input.to(device)\n","        \n","        mask_loss=loss_func(decoder_output,target_variable[t])\n","        loss+=mask_loss\n","        nTotal=(mask.sum()).item()\n","        print_losses.append(mask_loss.item()*nTotal)\n","        n_totals+=nTotal\n","        predicted[t]=torch.argmax(decoder_output,dim=-1)\n","    \n","    F1=f1_score(predicted,target_variable.transpose(0,1))\n","\n","    return sum(print_losses)/n_totals,F1\n"],"execution_count":75,"outputs":[]},{"cell_type":"code","metadata":{"id":"1xk1Eu8y5cYn","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598129993379,"user_tz":-330,"elapsed":962,"user":{"displayName":"deepak goyal","photoUrl":"https://lh5.googleusercontent.com/-3gfEZruD9Sk/AAAAAAAAAAI/AAAAAAAACvg/54H4lMsdOgA/s64/photo.jpg","userId":"05164064759516400423"}}},"source":["def train(input_variable,lengths,target_variable,mask,max_target_len,encoder,decoder,\n","          encoder_optimizer,decoder_optimizer,batch_size,clip,max_length=Max_Length,teacher_forcing=True):\n","    \n","    \n","    encoder.train()\n","    decoder.train()\n","    encoder_optimizer.zero_grad()\n","    decoder_optimizer.zero_grad()\n","\n","    loss_fn=nn.CrossEntropyLoss()\n","    \n","    input_variable=torch.tensor(input_variable).to(device)\n","    lengths=lengths.to(device)\n","\n","    target_variable=torch.tensor(target_variable).to(device)\n","    mask=mask.to(device)\n","    \n","    loss=0\n","    print_losses=[]\n","    n_totals=0\n","    encoder_hidden=encoder.hidden_init(input_variable.size()[1])\n","    encoder_outputs, encoder_hidden=encoder(input_variable,lengths,encoder_hidden)\n","    \n","    decoder_input=torch.LongTensor([[START_Token for _ in range(batch_size)]])\n","    decoder_input=decoder_input.to(device)\n","    use_teacher_forcing=teacher_forcing #if random.random()<teacher_forcing_ratio else False\n","    \n","    decoder_hidden=encoder_hidden[:decoder.n_layers]\n","\n","    predicted=torch.ones(max_target_len,input_variable.size()[1])\n","    \n","    if use_teacher_forcing:\n","        \n","        for t in range(max_target_len):\n","            decoder_output,decoder_hidden=decoder(decoder_input,decoder_hidden,encoder_outputs)\n","            \n","            decoder_input=target_variable[t].view(1,-1)\n","            \n","            \n","            # mask_loss=F.cross_entropy(decoder_output,target_variable[t])\n","            # mask_loss,nTotal=maskNLLLoss(decoder_output,target_variable[t],mask)\n","            mask_loss=loss_fn(decoder_output,target_variable[t])\n","            loss+=mask_loss\n","            nTotal=(mask.sum()).item()\n","            print_losses.append(mask_loss.item()*nTotal)\n","            n_totals+=nTotal\n","            predicted[t]=torch.argmax(decoder_output,dim=-1)\n","            \n","\n","            \n","    else:\n","        \n","        for t in range(max_target_len):\n","            decoder_output,decoder_hidden=decoder(decoder_input,decoder_hidden,encoder_outputs)\n","            \n","            _,topi=decoder_output.topk(1)\n","            decoder_input=torch.LongTensor([[topi[i][0] for i in range(batch_size)]])\n","            decoder_input=decoder_input.to(device)\n","            \n","            mask_loss=loss_fn(decoder_output,target_variable[t])\n","            loss+=mask_loss\n","            nTotal=(mask.sum()).item()\n","            print_losses.append(mask_loss.item()*nTotal)\n","            n_totals+=nTotal\n","\n","\n","            \n","    F1=f1_score(predicted,target_variable.transpose(0,1))\n","    if teacher_forcing:\n","        loss.backward()\n","        \n","        _=nn.utils.clip_grad_norm_(encoder.parameters(),clip)\n","        _=nn.utils.clip_grad_norm_(decoder.parameters(),clip)\n","        \n","        \n","        # plot_grad_flow(encoder.named_parameters())\n","        # plot_grad_flow(decoder.named_parameters())\n","        \n","        encoder_optimizer.step()\n","        decoder_optimizer.step()\n","        \n","    return sum(print_losses)/n_totals,F1\n","    \n"],"execution_count":79,"outputs":[]},{"cell_type":"code","metadata":{"id":"6-GQ9XmH5fLl","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598130582070,"user_tz":-330,"elapsed":980,"user":{"displayName":"deepak goyal","photoUrl":"https://lh5.googleusercontent.com/-3gfEZruD9Sk/AAAAAAAAAAI/AAAAAAAACvg/54H4lMsdOgA/s64/photo.jpg","userId":"05164064759516400423"}}},"source":["def trainIters(model_name,voc,pairs,encoder,decoder,encoder_optimizer,decoder_optimizer,\n","               encoder_n_layers,decoder_n_layers,save_dir,n_batches,batch_size,\n","               save_every,clip,corpus_name,loadFileName,n_epochs,training_batches,teacher_forcing,save_want,\n","               testing_batches,batch_size_test,n_batch_test):\n","    \n","    \n","    \n","    start_epoch=0\n","    loss=0\n","    perplexity=0\n","    time_taken=0\n","    f1_score=0\n","    \n","    if loadFileName:\n","        start_epoch=checkpoint['epoch']+1\n","        time_taken=checkpoint['time']\n","        \n","        \n","    for epoch in range(start_epoch,n_epochs):\n","\n","        t1=time.time()\n","        loss=0\n","        perplexity=0\n","        f1_score=0\n","\n","        test_loss=0\n","        test_perplexity=0\n","        test_f1_score=0\n","\n","        for i in range(n_batches):\n","            \n","            training_batch=training_batches[i]\n","            \n","            input_variable,lengths,target_variable,mask,max_target_len=training_batch\n","\n","            curr_loss,F1=train(input_variable,lengths,target_variable,mask,max_target_len,encoder,decoder,\n","                    encoder_optimizer,decoder_optimizer,batch_size,clip,10,teacher_forcing)\n","            \n","            loss+=curr_loss\n","            perplexity+=math.exp(curr_loss)\n","            f1_score+=F1\n","            \n","        \n","        \n","        loss=loss/n_batches\n","        perplexity=perplexity/n_batches\n","        f1_score=f1_score/n_batches\n","\n","        for i in range(n_batch_test):\n","\n","            testing_batch=testing_batches[i]\n","            input_variable,lengths,target_variable,mask,max_target_len=testing_batch\n","\n","            curr_loss,F1=test(input_variable,lengths,target_variable,mask,max_target_len,encoder,decoder,\n","                              batch_size_test)\n","            \n","            test_loss+=curr_loss\n","            test_perplexity+=math.exp(curr_loss)\n","            test_f1_score+=F1\n","\n","        test_loss=test_loss/n_batch_test\n","        test_perplexity=test_perplexity/n_batch_test\n","        test_f1_score=test_f1_score/n_batch_test\n","         \n","        \n","        if epoch%save_every==0 and save_want:\n","\n","            directory=os.path.join(save_dir,model_name,corpus_name)\n","            if not os.path.exists(directory):\n","                os.makedirs(directory)\n","            \n","            torch.save({\n","                \"epoch\":epoch,\n","                \"encoder\":encoder.state_dict(),\n","                \"decoder\":decoder.state_dict(),\n","                \"loss\":loss,\n","                \"encoder_opt\":encoder_optimizer.state_dict(),\n","                \"decoder_opt\":decoder_optimizer.state_dict(),\n","                \"ppl\":perplexity,\n","                \"time\":time_taken,\n","                \"F1\":f1_score\n","\n","            },os.path.join(directory,'{}_{}.tar'.format(epoch,\"checkpoint\")))\n","        \n","        print(\"=\"*100)\n","        print(\"| End of epoch : \"+str(epoch)+\"| Loss Value: \"+str(loss)+\"| PPL: \"+str(perplexity)+\"| F1: \"+str(f1_score)+\"| Time Took: \"+\n","            str(time.time()-t1)+\" |\")\n","        print(\"=\"*100)\n","        time_taken+=(time.time()-t1)\n","\n","        \n","        wandb.log({\n","            \"Training loss\": loss,\n","            \"Training PPL(Perplexity)\": perplexity,\n","            \"Training F1 Score\": f1_score,\n","            \"Testing loss\": test_loss,\n","            \"Testing PPL(Perplexity)\":test_perplexity,\n","            \"Testing F1 Score\": test_f1_score\n","\n","        })\n","\n","    print(\"| Training Finished | Took:\"+str(time_taken))          \n","        \n","\n","        \n","\n","        "],"execution_count":81,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z3yEoRzsIsPN","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598130584461,"user_tz":-330,"elapsed":975,"user":{"displayName":"deepak goyal","photoUrl":"https://lh5.googleusercontent.com/-3gfEZruD9Sk/AAAAAAAAAAI/AAAAAAAACvg/54H4lMsdOgA/s64/photo.jpg","userId":"05164064759516400423"}}},"source":["\n","def data_generation(pairs,batch_size,n_batches,start=0):\n","    \n","    # sample_batches=[batch2TrainData(voc,[random.choice(pairs) for _ in range(batch_size)]) for _ in range(n_batches)]\n","    sample_batches=[]\n","    \n","    for i in range(n_batches):\n","        curr_batch=[]\n","        for j in range(batch_size):\n","            curr_id=i*batch_size+j+start\n","            curr_batch.append(pairs[curr_id])\n","\n","        sample_batches.append(batch2TrainData(voc,curr_batch))\n","\n","    return sample_batches"],"execution_count":82,"outputs":[]},{"cell_type":"code","metadata":{"id":"rY4G76OMs0PW","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":69},"executionInfo":{"status":"ok","timestamp":1598130588661,"user_tz":-330,"elapsed":3473,"user":{"displayName":"deepak goyal","photoUrl":"https://lh5.googleusercontent.com/-3gfEZruD9Sk/AAAAAAAAAAI/AAAAAAAACvg/54H4lMsdOgA/s64/photo.jpg","userId":"05164064759516400423"}},"outputId":"89bdfc16-5c61-4a8b-9e19-532396f3f242"},"source":["wandb.init(project=\"seq2seq_withattn\")\n","wandb.watch_called=False\n","config=wandb.config\n"],"execution_count":83,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n","                Project page: <a href=\"https://app.wandb.ai/deepak4669/seq2seq_withattn\" target=\"_blank\">https://app.wandb.ai/deepak4669/seq2seq_withattn</a><br/>\n","                Run page: <a href=\"https://app.wandb.ai/deepak4669/seq2seq_withattn/runs/15ho92an\" target=\"_blank\">https://app.wandb.ai/deepak4669/seq2seq_withattn/runs/15ho92an</a><br/>\n","            "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"9D6B5hoG5mfA","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598130612311,"user_tz":-330,"elapsed":3516,"user":{"displayName":"deepak goyal","photoUrl":"https://lh5.googleusercontent.com/-3gfEZruD9Sk/AAAAAAAAAAI/AAAAAAAACvg/54H4lMsdOgA/s64/photo.jpg","userId":"05164064759516400423"}}},"source":["config.model_name='seq2seq_attn'\n","config.corpus_name='cornell-movie'\n","\n","config.attn_model='dot'\n","config.num_encoder=2\n","config.num_decoder=2\n","config.d_model=500\n","\n","config.dropout_encoder=0.1\n","config.dropout_decoder=0.1\n","\n","config.batch_size=10\n","config.n_batches=4000\n","\n","config.batch_size_test=10\n","config.n_batches_test=1000\n","\n","config.clip=50.0\n","config.teacher_forcing_ratio=1.0\n","config.learning_rate=0.0001\n","config.decoder_learning_ratio=5.0\n","\n","\n","config.save_every=50\n","config.n_epochs=500\n","\n","config.teacher_forcing=True\n","config.save_want=True\n","\n","loadFile=\"/content/drive/My Drive/Model Data/seq2seq_attn/cornell-movie/200_checkpoint.tar\"\n","loadFile=None\n","\n","save_dir=\"/content/drive/My Drive/Model Data\""],"execution_count":84,"outputs":[]},{"cell_type":"code","metadata":{"id":"LkB9JkW15omn","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"89831be8-2a3b-44e4-e67c-5b796da25a1a"},"source":["training_batches=data_generation(training_data,config.batch_size,config.n_batches,0)\n","testing_batches=data_generation(testing_data,config.batch_size_test,config.n_batches_test,0)\n","\n","print(\"Making models....\")\n","encoder,decoder,num_parameters=make_model(voc.num_words,config.d_model,config.num_encoder,config.num_decoder,config.dropout_encoder,config.dropout_decoder,config.attn_model)\n","print(\"Model building finished, the model has: \"+str(num_parameters)+\" parameters..\")\n","\n","encoder_optimizer=torch.optim.Adam(encoder.parameters(),lr=config.learning_rate)\n","decoder_optimizer=torch.optim.Adam(decoder.parameters(),lr=config.learning_rate*config.decoder_learning_ratio)\n","\n","if loadFile:\n","    checkpoint=torch.load(loadFile)\n","\n","    encoder.load_state_dict(checkpoint['encoder'])\n","    decoder.load_state_dict(checkpoint['decoder'])\n","    encoder_optimizer.load_state_dict(checkpoint['encoder_opt'])\n","    decoder_optimizer.load_state_dict(checkpoint['decoder_opt'])\n","\n","\n","\n","encoder.to(device)\n","decoder.to(device)\n","\n","encoder.train()\n","decoder.train()\n","\n","for state in encoder_optimizer.state.values():\n","    for k, v in state.items():\n","        if isinstance(v, torch.Tensor):\n","            state[k] = v.cuda()\n","\n","for state in decoder_optimizer.state.values():\n","    for k, v in state.items():\n","        if isinstance(v, torch.Tensor):\n","            state[k] = v.cuda()\n","\n","wandb.watch((encoder,decoder),log=\"all\")\n","\n","trainIters(config.model_name, voc, training_data, encoder, decoder, encoder_optimizer, decoder_optimizer,\n","        config.num_encoder, config.num_decoder, save_dir, config.n_batches, config.batch_size, config.save_every, \n","        config.clip, config.corpus_name, loadFile,config.n_epochs,training_batches,config.teacher_forcing,config.save_want,\n","        testing_batches,config.batch_size_test,config.n_batches_test)\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Making models....\n","Model building finished, the model has: 14934316 parameters..\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  if sys.path[0] == '':\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  from ipykernel import kernelapp as app\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  if __name__ == '__main__':\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  # This is added back by InteractiveShellApp.init_path()\n"],"name":"stderr"},{"output_type":"stream","text":["====================================================================================================\n","| End of epoch : 0| Loss Value: 2.7284285665109267| PPL: 28.565142200150433| F1: 0.3382457854194264| Time Took: 164.19071793556213 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 1| Loss Value: 2.379492056145009| PPL: 11.613224135172375| F1: 0.3441194499422537| Time Took: 163.4325864315033 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 2| Loss Value: 2.250132328340516| PPL: 10.126186555647134| F1: 0.3437790417526609| Time Took: 162.47869491577148 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 3| Loss Value: 2.142036951653526| PPL: 9.024630292537646| F1: 0.3437299841515508| Time Took: 163.82451605796814 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 4| Loss Value: 2.035143141217644| PPL: 8.053009597373732| F1: 0.3436980512555854| Time Took: 164.0412516593933 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 5| Loss Value: 1.9272947216304022| PPL: 7.181435472786187| F1: 0.3436582065150581| Time Took: 164.14976525306702 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 6| Loss Value: 1.8183058969509305| PPL: 6.400973403511774| F1: 0.34404383845643094| Time Took: 160.4093358516693 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 7| Loss Value: 1.7137640784832784| PPL: 5.735988582482881| F1: 0.3443211503022216| Time Took: 159.37511944770813 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 8| Loss Value: 1.6139934833578915| PPL: 5.168703526980789| F1: 0.3442372637475994| Time Took: 159.9401605129242 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 9| Loss Value: 1.5199847519336909| PPL: 4.686487129440049| F1: 0.3446479310715471| Time Took: 158.9706952571869 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 10| Loss Value: 1.4333377554699729| PPL: 4.28423370195517| F1: 0.34470329985502535| Time Took: 158.4226734638214 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 11| Loss Value: 1.352647993818259| PPL: 3.942282048668356| F1: 0.34504028143274473| Time Took: 159.4922857284546 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 12| Loss Value: 1.278129484580491| PPL: 3.6511007110823996| F1: 0.34548056863973203| Time Took: 158.7536609172821 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 13| Loss Value: 1.2095761297954353| PPL: 3.403626242977079| F1: 0.34577759042212947| Time Took: 160.06118655204773 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 14| Loss Value: 1.1448491567623338| PPL: 3.1853078496915646| F1: 0.34614852366823573| Time Took: 159.1711413860321 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 15| Loss Value: 1.0853953273360752| PPL: 2.9985398903666978| F1: 0.3462354996252847| Time Took: 159.7530734539032 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 16| Loss Value: 1.0279959744092748| PPL: 2.8282996675904477| F1: 0.3466942929013661| Time Took: 158.28176474571228 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 17| Loss Value: 0.9745043416280585| PPL: 2.677956937594079| F1: 0.3472334321403958| Time Took: 159.47017693519592 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 18| Loss Value: 0.9256161924132317| PPL: 2.5485516241216404| F1: 0.3480207653324446| Time Took: 160.15040063858032 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 19| Loss Value: 0.8789229024709614| PPL: 2.4304598705547606| F1: 0.34842350683694057| Time Took: 159.74812150001526 |\n","====================================================================================================\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OA5TjKYt5hhA","colab_type":"code","colab":{}},"source":["class GreedySearchDecoder(nn.Module):\n","    \n","    def __init__(self,encoder,decoder):\n","        super().__init__()\n","        \n","        self.encoder=encoder\n","        self.decoder=decoder\n","        \n","    def forward(self,input_seq,input_length,max_length,target=None):\n","        \n","        encoder_outputs,encoder_hidden=self.encoder(input_seq,input_length) \n","        decoder_hidden=encoder_hidden[:decoder.n_layers]\n","        decoder_input=torch.ones(1,1,device=device,dtype=torch.long)*START_Token\n","    \n","        all_tokens=torch.zeros([0],device=device,dtype=torch.long)\n","        all_scores=torch.zeros([0],device=device)\n","        loss=0\n","        \n","        for i in range(max_length):\n","            \n","            decoder_output,decoder_hidden=self.decoder(decoder_input,decoder_hidden,encoder_outputs)\n","            if target!=None:\n","                loss+=F.cross_entropy(decoder_output,target[i]).item()\n","            decoder_scores,decoder_input=torch.max(decoder_output,dim=1)\n","            all_scores=torch.cat((all_scores,decoder_scores),dim=0)\n","            all_tokens=torch.cat((all_tokens,decoder_input),dim=0)\n","            \n","            decoder_input=torch.unsqueeze(decoder_input,0)\n","            \n","        return all_tokens, all_scores,loss/max_length\n","        \n","        "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"66bZDfiW0U6W","colab_type":"code","colab":{}},"source":["def testing(encoder,decoder,searcher,pairs,starting_point):\n","\n","    loss=0\n","    F1=0\n","    num=0\n","\n","    for i in range(starting_point,len(pairs)):\n","        test_data=data_generation(pairs,1,1,i)\n","        input_variable,lengths,target_variable,mask,max_target_len=test_data[0]\n","        input_variable=input_variable.to(device)\n","        lengths=lengths.to(device)\n","        target_variable=target_variable.to(device)\n","\n","        tokens,scores,curr_loss=searcher(input_variable,lengths,max_target_len,target_variable)\n","        loss+=curr_loss\n","        F1+=f1_score(tokens.view(-1,max_target_len),target_variable)\n","        num=num+1\n","\n","    return loss/num,F1/num\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5EMWVrmC3uud","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1588011958332,"user_tz":-330,"elapsed":393710,"user":{"displayName":"deepak goyal","photoUrl":"https://lh5.googleusercontent.com/-3gfEZruD9Sk/AAAAAAAAAAI/AAAAAAAACvg/54H4lMsdOgA/s64/photo.jpg","userId":"05164064759516400423"}},"outputId":"edff01c8-0844-4093-9a33-584b99fabdfa"},"source":["encoder.eval()\n","decoder.eval()\n","searcher=GreedySearchDecoder(encoder,decoder)\n","testing(encoder,decoder,searcher,pairs,0)\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(17.45475177370771, 0.0887326835004401)"]},"metadata":{"tags":[]},"execution_count":33}]},{"cell_type":"code","metadata":{"id":"EhpBX3QT5kCf","colab_type":"code","colab":{}},"source":["def evaluate(encoder, decoder, searcher,voc,sentence,max_length=MAX_LENGTH):\n","    \n","    index_batch=[indexesFromSentence(voc,sentence)]\n","    lengths=torch.tensor([len(index) for index in index_batch])\n","    input_batch=torch.LongTensor(index_batch).transpose(0,1)\n","    \n","    input_batch=input_batch.to(device)\n","    lengths=lengths.to(device)\n","    \n","    tokens, scores,loss=searcher(input_batch,lengths,max_length)\n","    decoded_words=[voc.index2word[token.item()] for token in tokens]\n","    return decoded_words\n","\n","def evaluateInput(encoder,decoder,searcher,voc):\n","    input_sentence=''\n","    while True:\n","        try:\n","            input_sentence=input('Human> ')\n","            \n","            if input_sentence=='q' or input_sentence=='quit':\n","                break\n","            input_sentence=normalizeString(input_sentence)\n","            output_words=evaluate(encoder,decoder,searcher,voc,input_sentence)\n","            \n","            output_words[:]=[x for x in output_words if not(x==\"PAD\" or x==\"EOS\")]\n","            print(\"Bot:\",\" \".join(output_words))\n","            \n","        except KeyError:\n","            print(\"Unknown Word\")\n","            \n","            \n","    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Bk193gyI54Iv","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":156},"executionInfo":{"status":"ok","timestamp":1590910256537,"user_tz":-330,"elapsed":1195,"user":{"displayName":"deepak goyal","photoUrl":"https://lh5.googleusercontent.com/-3gfEZruD9Sk/AAAAAAAAAAI/AAAAAAAACvg/54H4lMsdOgA/s64/photo.jpg","userId":"05164064759516400423"}},"outputId":"5b5de870-25b9-4759-d621-61a454f7d550"},"source":["encoder.eval()\n","decoder.eval()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["LuongAttnDecoderRNN(\n","  (embedding): Embedding(7816, 500)\n","  (embedding_dropout): Dropout(p=0.1, inplace=False)\n","  (gru): GRU(500, 500, num_layers=2, dropout=0.1)\n","  (concat): Linear(in_features=1000, out_features=500, bias=True)\n","  (out): Linear(in_features=500, out_features=7816, bias=True)\n","  (attn): Attn()\n",")"]},"metadata":{"tags":[]},"execution_count":32}]},{"cell_type":"code","metadata":{"id":"VoxgXE87-169","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":104},"executionInfo":{"status":"ok","timestamp":1590910673435,"user_tz":-330,"elapsed":65609,"user":{"displayName":"deepak goyal","photoUrl":"https://lh5.googleusercontent.com/-3gfEZruD9Sk/AAAAAAAAAAI/AAAAAAAACvg/54H4lMsdOgA/s64/photo.jpg","userId":"05164064759516400423"}},"outputId":"57fd58a4-b3ce-414b-a79a-979c69bee4c5"},"source":["searcher=GreedySearchDecoder(encoder,decoder)\n","evaluateInput(encoder,decoder,searcher,voc)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Human> what do you work ?\n","Bot: i ain t a dream .\n","Human> what is it for ?\n","Bot: nothing . not bad .\n","Human> quit\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"eSOFd0pjIIuv","colab_type":"code","colab":{}},"source":["def testing(encoder,decoder,encoder_optimizer,decoder_optimizer,pairs,starting_point,n_batches=1000,batch_size=10):\n","    test_data=data_generation(pairs,batch_size,n_batches,starting_point)\n","\n","    loss=0\n","    f1_score=0\n","    ppl=0\n","\n","    for i in range(n_batches):\n","        current_batch=test_data[i]\n","        input_variable,lengths,target_variable,mask,max_target_len=current_batch\n","\n","        curr_loss,F1=train(input_variable,lengths,target_variable,mask,max_target_len,encoder,decoder,\n","                    encoder_optimizer,decoder_optimizer,batch_size,clip,10,True)\n","        \n","        loss+=curr_loss\n","        f1_score+=F1\n","        ppl+=math.exp(curr_loss)\n","    \n","    loss=loss/n_batches\n","    f1_score=f1_score/n_batches\n","    ppl=ppl/n_batches\n","\n","    print(\"Loss Value: \"+str(loss)+\" F1_Score: \"+str(f1_score)+\" Current PPL: \"+str(ppl))\n","        \n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"acdHoto0INkW","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}