{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"transformer_cornell.ipynb","provenance":[],"collapsed_sections":["v5xV-42LAX6d"],"authorship_tag":"ABX9TyONnjBVnHbAFVKNQzuye/Bm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"8i8or0mWcKPL","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"79acd9a0-d5d9-461d-f5e3-09b257967dab","executionInfo":{"status":"ok","timestamp":1586271763445,"user_tz":-330,"elapsed":2150,"user":{"displayName":"deepak goyal","photoUrl":"https://lh5.googleusercontent.com/-3gfEZruD9Sk/AAAAAAAAAAI/AAAAAAAACvg/54H4lMsdOgA/s64/photo.jpg","userId":"05164064759516400423"}}},"source":["from google.colab import drive\n","drive.mount(\"/content/drive\")"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"U-K4mgwHE8oj","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":312},"outputId":"a8e222e8-9514-4516-df1f-fcfa8055ed05","executionInfo":{"status":"ok","timestamp":1586271770627,"user_tz":-330,"elapsed":6604,"user":{"displayName":"deepak goyal","photoUrl":"https://lh5.googleusercontent.com/-3gfEZruD9Sk/AAAAAAAAAAI/AAAAAAAACvg/54H4lMsdOgA/s64/photo.jpg","userId":"05164064759516400423"}}},"source":["!nvidia-smi"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Tue Apr  7 15:02:49 2020       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 440.64.00    Driver Version: 418.67       CUDA Version: 10.1     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   39C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                       GPU Memory |\n","|  GPU       PID   Type   Process name                             Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"R-KwziUccUv9","colab_type":"code","colab":{}},"source":["#Pre-Processing\n","import os\n","import re\n","import torch\n","import random\n","import itertools\n","\n","#Model\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","\n","import numpy as np\n","\n","# For visualising metrics\n","# from visdom import Visdom\n","\n","# For visualising gradients plot\n","import matplotlib.pyplot as plt\n","from matplotlib.lines import Line2D\n","\n","import copy\n","import math\n","import time"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bzOQagtOdqyn","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"1adb8163-7c65-42ab-ffa8-0f0277eed803","executionInfo":{"status":"ok","timestamp":1586271772565,"user_tz":-330,"elapsed":1748,"user":{"displayName":"deepak goyal","photoUrl":"https://lh5.googleusercontent.com/-3gfEZruD9Sk/AAAAAAAAAAI/AAAAAAAACvg/54H4lMsdOgA/s64/photo.jpg","userId":"05164064759516400423"}}},"source":["device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","# device=torch.device(\"cpu\")\n","print(\"The device found: \"+str(device))"],"execution_count":4,"outputs":[{"output_type":"stream","text":["The device found: cuda\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"19BVL5GNdtbi","colab_type":"code","colab":{}},"source":["def plot_grad_flow(named_parameters):\n","    \"\"\"\n","        Plotting gradient flow across various layers\n","        Thanks to: https://discuss.pytorch.org/t/check-gradient-flow-in-network/15063/2\n","    \"\"\"   \n","    ave_grads = []\n","    layers = []\n","    for n, p in named_parameters:\n","        if(p.requires_grad) and (\"bias\" not in n):\n","            layers.append(n)\n","            ave_grads.append(p.grad.abs().mean())\n","    plt.plot(ave_grads, alpha=0.3, color=\"b\")\n","    plt.hlines(0, 0, len(ave_grads)+1, linewidth=1, color=\"k\" )\n","    plt.xticks(range(0,len(ave_grads), 1), layers, rotation=\"vertical\")\n","    plt.xlim(xmin=0, xmax=len(ave_grads))\n","    plt.xlabel(\"Layers\")\n","    plt.ylabel(\"average gradient\")\n","    plt.title(\"Gradient flow\")\n","    plt.grid(True)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XyVbmpGw26_v","colab_type":"text"},"source":["# Preprocessing\n"]},{"cell_type":"code","metadata":{"id":"qz7CbxS4dwh3","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"f2d77f15-e097-4b17-e108-29ae55bcc119","executionInfo":{"status":"ok","timestamp":1586271773321,"user_tz":-330,"elapsed":859,"user":{"displayName":"deepak goyal","photoUrl":"https://lh5.googleusercontent.com/-3gfEZruD9Sk/AAAAAAAAAAI/AAAAAAAACvg/54H4lMsdOgA/s64/photo.jpg","userId":"05164064759516400423"}}},"source":["path='/content/drive/My Drive/Data'\n","dataset='cornell movie-dialogs corpus'\n","\n","data_folder=os.path.join(path,dataset)\n","\n","print(\"The final data corpus folder: \"+str(data_folder))"],"execution_count":6,"outputs":[{"output_type":"stream","text":["The final data corpus folder: /content/drive/My Drive/Data/cornell movie-dialogs corpus\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"dDKqcO2de7RM","colab_type":"code","colab":{}},"source":["def get_lines_conversations():\n","    \"\"\"\n","    Loads movie lines and conversations from the dataset.\n","    \n","    data_folder: Destination where conversations and lines are stored.\n","    \n","    movie_lines: Consist of movie lines as given by the dataset.\n","    movie_conversations: Consist of movie conversations as given by the dataset.\n","    \n","    \"\"\"\n","    movie_lines=[]\n","    movie_conversations=[]\n","\n","    with open(os.path.join(data_folder,'movie_lines.txt'),'r',encoding='iso-8859-1') as f:\n","        for line in f:\n","            movie_lines.append(line)\n","    \n","    with open(os.path.join(data_folder,'movie_conversations.txt'),'r', encoding='iso-8859-1') as f:\n","        for line in f:\n","            movie_conversations.append(line)\n","                                       \n","\n","    return movie_lines,movie_conversations"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-txSMOjQfEnx","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":173},"outputId":"6f83edf9-22ae-4aaf-94a5-26b10684b69d","executionInfo":{"status":"ok","timestamp":1586271774395,"user_tz":-330,"elapsed":1199,"user":{"displayName":"deepak goyal","photoUrl":"https://lh5.googleusercontent.com/-3gfEZruD9Sk/AAAAAAAAAAI/AAAAAAAACvg/54H4lMsdOgA/s64/photo.jpg","userId":"05164064759516400423"}}},"source":["t1=time.time()\n","print(\"Extracting movie lines and movie conversations...\")\n","movie_lines,movie_conversations=get_lines_conversations()\n","\n","print(\"Number of distinct lines: \"+str(len(movie_lines)))\n","print(\"Number of conversations: \"+str(len(movie_conversations)))\n","print(\"Average Number of lines per conversations: \"+str(len(movie_lines)/len(movie_conversations)))\n","\n","print(movie_lines[0])\n","print(movie_conversations[0])\n","\n","print(\"Extracting took place in: \"+str(time.time()-t1))"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Extracting movie lines and movie conversations...\n","Number of distinct lines: 304713\n","Number of conversations: 83097\n","Average Number of lines per conversations: 3.6669554857576085\n","L1045 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ They do not!\n","\n","u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L194', 'L195', 'L196', 'L197']\n","\n","Extracting took place in: 0.13982319831848145\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3LXCFv_qfMsN","colab_type":"code","colab":{}},"source":["exceptions=[]\n","def loadLines(movie_lines,fields):\n","    lines={}\n","    for lineid in range(len(movie_lines)):\n","        \n","        line=movie_lines[lineid]\n","        values=line.split(\" +++$+++ \")\n","        \n","        \n","        lineVals={}\n","        \n","        # print(\"values\"+str(len(values)))\n","        # print(\"fields\"+str(len(fields)))\n","              \n","        for i,field in enumerate(fields):\n","            try:\n","                lineVals[field]=values[i]\n","            except:\n","                print(\"Exception: \"+str(len(values)))\n","                exceptions.append(lineid)\n","        \n","        lines[lineVals['lineID']]=lineVals\n","    \n","    return lines\n","\n","def loadConversations(movie_conversations,lines,fields):\n","    conversations=[]\n","    \n","    for convo in movie_conversations:\n","        values=convo.split(\" +++$+++ \")\n","        conVals={}\n","       \n","        for i,field in enumerate(fields):\n","            conVals[field]=values[i]\n","        \n","        lineIDs=eval(conVals[\"utteranceIDs\"])\n","        \n","        conVals[\"lines\"]=[]\n","        \n","        for lineID in lineIDs:\n","            conVals[\"lines\"].append(lines[lineID])\n","        conversations.append(conVals)\n","        \n","    return conversations\n","\n","def sentencePairs(conversations):\n","    qr_pairs=[]\n","    \n","    for conversation in conversations:\n","        for i in range(len(conversation[\"lines\"])-1):\n","            query=conversation[\"lines\"][i][\"text\"].strip()\n","            response=conversation[\"lines\"][i+1][\"text\"].strip()\n","            \n","            if query and response:\n","                qr_pairs.append([query,response])\n","        \n","    return qr_pairs"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"J2dngu2Xyvzt","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":69},"outputId":"d121f28e-3fb1-4079-81b7-e9550dd8ba95","executionInfo":{"status":"ok","timestamp":1586271777745,"user_tz":-330,"elapsed":3587,"user":{"displayName":"deepak goyal","photoUrl":"https://lh5.googleusercontent.com/-3gfEZruD9Sk/AAAAAAAAAAI/AAAAAAAACvg/54H4lMsdOgA/s64/photo.jpg","userId":"05164064759516400423"}}},"source":["t1=time.time()\n","print(\"Separating meaningfull information for our model...\")\n","\n","lines={}\n","conversations=[]\n","qr_pairs=[]\n","\n","movie_lines_fields=[\"lineID\",\"characterID\",\"movieID\",\"character\",\"text\"]\n","movie_convo_fields=[\"charcaterID\",\"character2ID\",\"movieID\",\"utteranceIDs\"]\n","\n","lines=loadLines(movie_lines,movie_lines_fields)\n","conversations=loadConversations(movie_conversations,lines,movie_convo_fields)\n","qr_pairs=sentencePairs(conversations)\n","\n","print(\"The number of query-response pairs are: \"+str(len(qr_pairs)))\n","print(\"Separation took place in: \"+str(time.time()-t1))"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Separating meaningfull information for our model...\n","The number of query-response pairs are: 221282\n","Separation took place in: 1.9323523044586182\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"NFMCnpuO2jpr","colab_type":"code","colab":{}},"source":["PAD_Token=0\n","START_Token=1\n","END_Token=2\n","\n","class Vocabulary:\n","    def __init__(self):\n","        self.trimmed=False\n","        self.word2count={}\n","        self.index2word={PAD_Token:\"PAD\",START_Token:\"SOS\",END_Token:\"EOS\"}\n","        self.word2index={\"PAD\":PAD_Token,\"SOS\":START_Token,\"EOS\":END_Token}\n","        self.num_words=3\n","        \n","    def addSentence(self,sentence):\n","        for word in sentence.split(\" \"):\n","            self.addWord(word)\n","    def addWord(self,word):\n","        if word not in self.word2index:\n","            self.word2index[word]=self.num_words\n","            self.index2word[self.num_words]=word\n","            self.word2count[word]=1\n","            self.num_words=self.num_words+1\n","        else:\n","            self.word2count[word]+=1\n","            \n","    def trim(self,min_count):\n","        \n","        if self.trimmed:\n","            return\n","        self.trimmed=True\n","        \n","        keep_words=[]\n","        \n","        for word,freq in self.word2count.items():\n","            if freq>=min_count:\n","                keep_words.append(word)\n","        \n","        self.word2count={}\n","        self.index2word={PAD_Token:\"PAD\",START_Token:\"SOS\",END_Token:\"EOS\"}\n","        self.word2index={\"PAD\":PAD_Token,\"SOS\":START_Token,\"EOS\":END_Token}\n","        self.num_words=3\n","        \n","        for word in keep_words:\n","            self.addWord(word)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GeScbB7iy0AC","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"outputId":"de2941df-37d2-4fb3-c029-dac9f3761afc","executionInfo":{"status":"ok","timestamp":1586271783454,"user_tz":-330,"elapsed":8227,"user":{"displayName":"deepak goyal","photoUrl":"https://lh5.googleusercontent.com/-3gfEZruD9Sk/AAAAAAAAAAI/AAAAAAAACvg/54H4lMsdOgA/s64/photo.jpg","userId":"05164064759516400423"}}},"source":["Max_Length=10\n","\n","def normalizeString(s):\n","    s=s.lower().strip()\n","    s=re.sub(r\"([.!?])\", r\" \\1\", s)\n","    s=re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n","    s=re.sub(r\"\\s+\", r\" \", s).strip()\n","    return s\n","\n","def readVocs(qr_pairs):\n","    \n","    for qr_pair in qr_pairs:\n","        qr_pair[0]=normalizeString(qr_pair[0])\n","        qr_pair[1]=normalizeString(qr_pair[1])\n","    \n","    voc=Vocabulary()\n","    return voc,qr_pairs\n","\n","def filterPair(pair):\n","    return len(pair[0].split(\" \"))<Max_Length and len(pair[1].split(\" \"))<Max_Length\n","\n","def filterPairs(qr_pairs):\n","    return [pair for pair in qr_pairs if filterPair(pair)]\n","\n","def prepareDataset(qr_pairs):\n","    voc, qr_pairs=readVocs(qr_pairs)\n","    qr_pairs=filterPairs(qr_pairs)\n","       \n","    for pair in qr_pairs:\n","        voc.addSentence(pair[0])\n","        voc.addSentence(pair[1])\n","#     print(\"Number\"+str(voc.num_words))\n","    return voc,qr_pairs\n","\n","t1=time.time()\n","print(\"Preparing dataset and corresponding vocabulary...\")\n","voc, pairs=prepareDataset(qr_pairs)\n","print(\"Preparation took place in: \"+str(time.time()-t1))"],"execution_count":12,"outputs":[{"output_type":"stream","text":["Preparing dataset and corresponding vocabulary...\n","Preparation took place in: 5.706905364990234\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"UUBpnFjQ2SCS","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"outputId":"50465abe-47cb-441e-a638-8cf3fbcee6e5","executionInfo":{"status":"ok","timestamp":1586271783455,"user_tz":-330,"elapsed":7510,"user":{"displayName":"deepak goyal","photoUrl":"https://lh5.googleusercontent.com/-3gfEZruD9Sk/AAAAAAAAAAI/AAAAAAAACvg/54H4lMsdOgA/s64/photo.jpg","userId":"05164064759516400423"}}},"source":["Min_Count=3\n","\n","def trimRareWords(voc,qr_pairs):\n","    \n","    voc.trim(Min_Count)\n","    keep_pairs=[]\n","    \n","    for pair in qr_pairs:\n","        input_sentence=pair[0]\n","        output_sentence=pair[1]\n","        \n","        keep_input=True\n","        keep_output=True\n","        \n","        for word in input_sentence.split(\" \"):\n","            if word not in voc.word2index:\n","                keep_input=False\n","                break\n","        \n","        for word in output_sentence.split(\" \"):\n","            if word not in voc.word2index:\n","                keep_output=False\n","                break\n","                \n","        if keep_input and keep_output:\n","            keep_pairs.append(pair)\n","            \n","    return keep_pairs\n","\n","t1=time.time()\n","print(\"Trimming rare words from vocabulary and dataset..\")\n","\n","pairs=trimRareWords(voc,pairs)\n","\n","print(\"Trimming took place in: \"+str(time.time()-t1))"],"execution_count":13,"outputs":[{"output_type":"stream","text":["Trimming rare words from vocabulary and dataset..\n","Trimming took place in: 0.1210787296295166\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"VjOEe2nx2sbZ","colab_type":"code","colab":{}},"source":["def indexesFromSentence(voc,sentence):\n","    tokenised_sentence=[]\n","    tokenised_sentence.append(START_Token)\n","    \n","    for word in sentence.split(\" \"):\n","        tokenised_sentence.append(voc.word2index[word])\n","        \n","    tokenised_sentence.append(END_Token)\n","    \n","    assert len(tokenised_sentence)<=Max_Length+2\n","    for _ in range(Max_Length+2-len(tokenised_sentence)):\n","        tokenised_sentence.append(PAD_Token)\n","        \n","    return tokenised_sentence\n","\n","def binaryMatrix(l,value=PAD_Token):\n","    m=[]\n","    for i,seq in enumerate(l):\n","        m.append([])\n","        for token in seq:\n","            if token==value:\n","                m[i].append(0)\n","            else:\n","                m[i].append(1)\n","        \n","    return m\n","\n","def inputVar(voc,l):\n","    \n","    indexes_batch=[indexesFromSentence(voc,sentence) for sentence in l]\n","    input_lengths=torch.tensor([len(index) for index in indexes_batch])\n","    padVar=torch.LongTensor(indexes_batch)\n","    return input_lengths,padVar\n","\n","def outputVar(voc,l):\n","    indexes_batch=[indexesFromSentence(voc,sentence) for sentence in l]\n","    max_target_len=torch.tensor([len(index) for index in indexes_batch])\n","    mask=binaryMatrix(indexes_batch)\n","    mask=torch.ByteTensor(mask)\n","    padVar=torch.LongTensor(indexes_batch)\n","    return max_target_len, mask, padVar\n","\n","def batch2TrainData(voc,pair_batch):\n","    #sort function see \n","    input_batch=[]\n","    output_batch=[]\n","\n","    for pair in pair_batch:\n","        input_batch.append(pair[0])\n","        output_batch.append(pair[1])\n","                                  \n","    \n","    input_lengths,tokenised_input=inputVar(voc,input_batch)\n","    max_out_length,mask,tokenised_output=outputVar(voc,output_batch)\n","    return input_lengths,tokenised_input,max_out_length,mask,tokenised_output"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"f-wd2hB-2y4b","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":416},"outputId":"f16b6eb0-5b95-42f9-db67-f5e7bba38b8f","executionInfo":{"status":"ok","timestamp":1586271783456,"user_tz":-330,"elapsed":6553,"user":{"displayName":"deepak goyal","photoUrl":"https://lh5.googleusercontent.com/-3gfEZruD9Sk/AAAAAAAAAAI/AAAAAAAACvg/54H4lMsdOgA/s64/photo.jpg","userId":"05164064759516400423"}}},"source":["print(\"Number of query-response pairs after all the preprocessing: \"+str(len(pairs)))\n","\n","#Sample batch\n","batch=[random.choice(pairs) for _ in range(5)]\n","input_lengths,tokenised_input,max_out_length,mask,tokenised_output=batch2TrainData(voc,batch)\n","\n","print(\"Input length: \"+str(input_lengths)+\" Size: \"+str(input_lengths.shape))\n","print(\"-\"*80)\n","print(\"Tokenised Input: \"+str(tokenised_input)+\" Size: \"+str(tokenised_input.shape))\n","print(\"-\"*80)\n","print(\"Max out length: \"+str(max_out_length)+\" Size: \"+str(max_out_length.shape))\n","print(\"-\"*80)\n","print(\"Mask: \"+str(mask)+\" Size: \"+str(mask.shape))\n","print(\"-\"*80)\n","print(\"Tokenised Output: \"+str(tokenised_output)+\" Size: \"+str(tokenised_output.shape))\n","print(\"-\"*80)"],"execution_count":15,"outputs":[{"output_type":"stream","text":["Number of query-response pairs after all the preprocessing: 53113\n","Input length: tensor([12, 12, 12, 12, 12]) Size: torch.Size([5])\n","--------------------------------------------------------------------------------\n","Tokenised Input: tensor([[   1,    4,    4,    4,    7,   14,  660, 3140,    4,    2,    0,    0],\n","        [   1,  167,    4,    2,    0,    0,    0,    0,    0,    0,    0,    0],\n","        [   1,   25,    8,   12,  807, 1960,   56,  227,    4,    2,    0,    0],\n","        [   1,  101,  215,   76,  102,   12, 2007,    4,    2,    0,    0,    0],\n","        [   1,  662,    6,    2,    0,    0,    0,    0,    0,    0,    0,    0]]) Size: torch.Size([5, 12])\n","--------------------------------------------------------------------------------\n","Max out length: tensor([12, 12, 12, 12, 12]) Size: torch.Size([5])\n","--------------------------------------------------------------------------------\n","Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n","        [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]], dtype=torch.uint8) Size: torch.Size([5, 12])\n","--------------------------------------------------------------------------------\n","Tokenised Output: tensor([[   1,  147,  410,   36,  380, 1095,  120,    6,    2,    0,    0,    0],\n","        [   1,  147,  152,    6,    2,    0,    0,    0,    0,    0,    0,    0],\n","        [   1, 3536,    4, 3536,   37,  444,  445, 6357,    4,    2,    0,    0],\n","        [   1,  167,  266,    4,    2,    0,    0,    0,    0,    0,    0,    0],\n","        [   1,  318,    4,   95, 5343,    4,    4,    4,    4,    2,    0,    0]]) Size: torch.Size([5, 12])\n","--------------------------------------------------------------------------------\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"DpR-1oMt3HA4","colab_type":"text"},"source":["# Model 1"]},{"cell_type":"code","metadata":{"id":"RJnnpKgt3Nyg","colab_type":"code","colab":{}},"source":["class EncoderDecoder(nn.Module):\n","    \"\"\"\n","    A standard Encoder-Decoder architecture. Base for this and many \n","    other models.\n","    \"\"\"\n","    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n","        super(EncoderDecoder, self).__init__()\n","        self.encoder = encoder\n","        self.decoder = decoder\n","        self.src_embed = src_embed\n","        self.tgt_embed = tgt_embed\n","        self.generator = generator\n","        \n","    def forward(self, src, tgt, src_mask, tgt_mask):\n","        \"Take in and process masked src and target sequences.\"\n","        return self.decode(self.encode(src, src_mask), src_mask,\n","                            tgt, tgt_mask)\n","    \n","    def encode(self, src, src_mask):\n","        return self.encoder(self.src_embed(src), src_mask)\n","    \n","    def decode(self, memory, src_mask, tgt, tgt_mask):\n","        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cRE6Ls4k_lvE","colab_type":"code","colab":{}},"source":["class Generator(nn.Module):\n","    \"Define standard linear + softmax generation step.\"\n","    def __init__(self, d_model, vocab):\n","        super(Generator, self).__init__()\n","        self.proj = nn.Linear(d_model, vocab)\n","\n","    def forward(self, x):\n","        return F.log_softmax(self.proj(x), dim=-1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aPiLhBPP_oQL","colab_type":"code","colab":{}},"source":["def clones(module, N):\n","    \"Produce N identical layers.\"\n","    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3ApjC3XD_quP","colab_type":"code","colab":{}},"source":["class Encoder(nn.Module):\n","    \"Core encoder is a stack of N layers\"\n","    def __init__(self, layer, N):\n","        super(Encoder, self).__init__()\n","        self.layers = clones(layer, N)\n","        self.norm = LayerNorm(layer.size)\n","        \n","    def forward(self, x, mask):\n","        \"Pass the input (and mask) through each layer in turn.\"\n","        for layer in self.layers:\n","            x = layer(x, mask)\n","        return self.norm(x)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5wUahzPb_s88","colab_type":"code","colab":{}},"source":["class LayerNorm(nn.Module):\n","    \"Construct a layernorm module (See citation for details).\"\n","    def __init__(self, features, eps=1e-6):\n","        super(LayerNorm, self).__init__()\n","        self.a_2 = nn.Parameter(torch.ones(features))\n","        self.b_2 = nn.Parameter(torch.zeros(features))\n","        self.eps = eps\n","\n","    def forward(self, x):\n","        mean = x.mean(-1, keepdim=True)\n","        std = x.std(-1, keepdim=True)\n","        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zowO7FHp_v5s","colab_type":"code","colab":{}},"source":["class SublayerConnection(nn.Module):\n","    \"\"\"\n","    A residual connection followed by a layer norm.\n","    Note for code simplicity the norm is first as opposed to last.\n","    \"\"\"\n","    def __init__(self, size, dropout):\n","        super(SublayerConnection, self).__init__()\n","        self.norm = LayerNorm(size)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x, sublayer):\n","        \"Apply residual connection to any sublayer with the same size.\"\n","        return x + self.dropout(sublayer(self.norm(x)))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_nkoHVj4_yiG","colab_type":"code","colab":{}},"source":["class EncoderLayer(nn.Module):\n","    \"Encoder is made up of self-attn and feed forward (defined below)\"\n","    def __init__(self, size, self_attn, feed_forward, dropout):\n","        super(EncoderLayer, self).__init__()\n","        self.self_attn = self_attn\n","        self.feed_forward = feed_forward\n","        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n","        self.size = size\n","\n","    def forward(self, x, mask):\n","        \"Follow Figure 1 (left) for connections.\"\n","        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n","        return self.sublayer[1](x, self.feed_forward)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BIJVFhXm_1FF","colab_type":"code","colab":{}},"source":["class Decoder(nn.Module):\n","    \"Generic N layer decoder with masking.\"\n","    def __init__(self, layer, N):\n","        super(Decoder, self).__init__()\n","        self.layers = clones(layer, N)\n","        self.norm = LayerNorm(layer.size)\n","        \n","    def forward(self, x, memory, src_mask, tgt_mask):\n","        for layer in self.layers:\n","            x = layer(x, memory, src_mask, tgt_mask)\n","        return self.norm(x)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZKn03mRp_3V2","colab_type":"code","colab":{}},"source":["class DecoderLayer(nn.Module):\n","    \"Decoder is made of self-attn, src-attn, and feed forward (defined below)\"\n","    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n","        super(DecoderLayer, self).__init__()\n","        self.size = size\n","        self.self_attn = self_attn\n","        self.src_attn = src_attn\n","        self.feed_forward = feed_forward\n","        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n"," \n","    def forward(self, x, memory, src_mask, tgt_mask):\n","        \"Follow Figure 1 (right) for connections.\"\n","        m = memory\n","        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n","        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n","        return self.sublayer[2](x, self.feed_forward)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KddgfVJx_6DC","colab_type":"code","colab":{}},"source":["def subsequent_mask(size):\n","    \"Mask out subsequent positions.\"\n","    attn_shape = (1, size, size)\n","    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n","    return torch.from_numpy(subsequent_mask) == 0"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BYWVMJoh_8-k","colab_type":"code","colab":{}},"source":["def attention(query, key, value, mask=None, dropout=None):\n","    \"Compute 'Scaled Dot Product Attention'\"\n","    d_k = query.size(-1)\n","    scores = torch.matmul(query, key.transpose(-2, -1)) \\\n","             / math.sqrt(d_k)\n","    if mask is not None:\n","        scores = scores.masked_fill(mask == 0, -1e9)\n","    p_attn = F.softmax(scores, dim = -1)\n","    if dropout is not None:\n","        p_attn = dropout(p_attn)\n","    return torch.matmul(p_attn, value), p_attn"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"veKkKGgb__q9","colab_type":"code","colab":{}},"source":["class MultiHeadedAttention(nn.Module):\n","    def __init__(self, h, d_model, dropout=0.1):\n","        \"Take in model size and number of heads.\"\n","        super(MultiHeadedAttention, self).__init__()\n","        assert d_model % h == 0\n","        # We assume d_v always equals d_k\n","        self.d_k = d_model // h\n","        self.h = h\n","        self.linears = clones(nn.Linear(d_model, d_model), 4)\n","        self.attn = None\n","        self.dropout = nn.Dropout(p=dropout)\n","        \n","    def forward(self, query, key, value, mask=None):\n","        \"Implements Figure 2\"\n","        if mask is not None:\n","            # Same mask applied to all h heads.\n","            mask = mask.unsqueeze(1)\n","        nbatches = query.size(0)\n","        \n","        # 1) Do all the linear projections in batch from d_model => h x d_k \n","        query, key, value = \\\n","            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n","             for l, x in zip(self.linears, (query, key, value))]\n","        \n","        # 2) Apply attention on all the projected vectors in batch. \n","        x, self.attn = attention(query, key, value, mask=mask, \n","                                 dropout=self.dropout)\n","        \n","        # 3) \"Concat\" using a view and apply a final linear. \n","        x = x.transpose(1, 2).contiguous() \\\n","             .view(nbatches, -1, self.h * self.d_k)\n","        return self.linears[-1](x)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aSqvuoJVAE_i","colab_type":"code","colab":{}},"source":["class PositionwiseFeedForward(nn.Module):\n","    \"Implements FFN equation.\"\n","    def __init__(self, d_model, d_ff, dropout=0.1):\n","        super(PositionwiseFeedForward, self).__init__()\n","        self.w_1 = nn.Linear(d_model, d_ff)\n","        self.w_2 = nn.Linear(d_ff, d_model)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        return self.w_2(self.dropout(F.relu(self.w_1(x))))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ft2ByV62AHlf","colab_type":"code","colab":{}},"source":["class Embeddings(nn.Module):\n","    def __init__(self, d_model, vocab):\n","        super(Embeddings, self).__init__()\n","        self.lut = nn.Embedding(vocab, d_model)\n","        self.d_model = d_model\n","\n","    def forward(self, x):\n","        return self.lut(x) * math.sqrt(self.d_model)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"flsGFWNWAKso","colab_type":"code","colab":{}},"source":["class PositionalEncoding(nn.Module):\n","    \"Implement the PE function.\"\n","    def __init__(self, d_model, dropout, max_len=5000):\n","        super(PositionalEncoding, self).__init__()\n","        self.dropout = nn.Dropout(p=dropout)\n","        \n","        # Compute the positional encodings once in log space.\n","        pe = torch.zeros(max_len, d_model)\n","        position = torch.arange(0, max_len).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2) *\n","                             -(math.log(10000.0) / d_model))\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","        pe = pe.unsqueeze(0)\n","        self.register_buffer('pe', pe)\n","        \n","    def forward(self, x):\n","        x = x + Variable(self.pe[:, :x.size(1)], \n","                         requires_grad=False)\n","        return self.dropout(x)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"veyYVrUlANvh","colab_type":"code","colab":{}},"source":["def make_model(src_vocab, tgt_vocab, N=6, \n","               d_model=512, d_ff=2048, h=8, dropout=0.1):\n","    \"Helper: Construct a model from hyperparameters.\"\n","    c = copy.deepcopy\n","    attn = MultiHeadedAttention(h, d_model)\n","    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n","    position = PositionalEncoding(d_model, dropout)\n","    model = EncoderDecoder(\n","        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n","        Decoder(DecoderLayer(d_model, c(attn), c(attn), \n","                             c(ff), dropout), N),\n","        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),\n","        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),\n","        Generator(d_model, tgt_vocab))\n","    \n","    # This was important from their code. \n","    # Initialize parameters with Glorot / fan_avg.\n","    for p in model.parameters():\n","        if p.dim() > 1:\n","            nn.init.xavier_uniform(p)\n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v5xV-42LAX6d","colab_type":"text"},"source":["# Model 2"]},{"cell_type":"code","metadata":{"id":"uHIbkFgbAhpa","colab_type":"code","colab":{}},"source":["class EncoderDecoder(nn.Module):\n","    \n","    def __init__(self,encoder,decoder,source_embed,target_embed,generator):\n","        super().__init__()\n","        \n","        self.encoder=encoder\n","        self.decoder=decoder\n","        \n","        self.source_embed=source_embed\n","        self.target_embed=target_embed\n","        \n","        self.generator=generator # Linear + Log_softmax\n","        \n","    def forward(self,source,target,source_mask,target_mask):\n","        return self.decode(self.encode(source,source_mask),source_mask,target,target_mask)\n","    \n","    def encode(self,source,source_mask):\n","        return self.encoder(self.source_embed(source),source_mask)\n","    \n","    def decode(self,memory, source_mask,target,target_mask):\n","        return self.decoder(self.target_embed(target),memory,source_mask,target_mask)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ewcrG0AeCHQc","colab_type":"code","colab":{}},"source":["class Generator(nn.Module):\n","    \n","    def __init__(self,d_model,vocab_size):\n","        super().__init__()\n","        self.projection=nn.Linear(d_model,vocab_size)\n","        \n","    def forward(self,decoder_output):\n","        return F.log_softmax(self.projection(decoder_output),dim=-1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jNIgAbiYCKg0","colab_type":"code","colab":{}},"source":["def clones(module,N):\n","    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qgHHuuzYCNL8","colab_type":"code","colab":{}},"source":["class Encoder(nn.Module):\n","    \n","    def __init__(self,layer,N):\n","        super().__init__()\n","        \n","        self.layers=clones(layer,N)\n","        self.norm=LayerNorm(layer.size)\n","    \n","    def forward(self,x,mask):\n","        \n","        for layer in self.layers:\n","            x=layer(x,mask)\n","        \n","        return self.norm(x)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dQRX2bAMCPlO","colab_type":"code","colab":{}},"source":["class LayerNorm(nn.Module):\n","    \n","    def __init__(self,features,eps=1e-6):\n","        super().__init__()\n","        self.a_2=nn.Parameter(torch.ones(features))\n","        self.b_2=nn.Parameter(torch.zeros(features))\n","        self.eps=eps\n","        \n","    def forward(self,x):\n","        mean=x.mean(-1,keepdim=True)\n","        std=x.std(-1,keepdim=True)\n","        return self.a_2*(x-mean)/(std+self.eps)+self.b_2"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-sIDH01lCSsv","colab_type":"code","colab":{}},"source":["class SublayerConnection(nn.Module):\n","    \n","    def __init__(self,size,dropout):\n","        super().__init__()\n","        \n","        self.dropout=nn.Dropout(dropout)\n","        self.norm=LayerNorm(size)\n","        \n","    def forward(self,x,sublayer):\n","        return x+self.dropout(sublayer(self.norm(x)))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1zQ_LsoCCVpO","colab_type":"code","colab":{}},"source":["class EncoderLayer(nn.Module):\n","    def __init__(self,size,self_attn,feed_forward,dropout):\n","        super().__init__()\n","        \n","        self.attn=self_attn\n","        self.feed_forward=feed_forward\n","        self.sublayer=clones(SublayerConnection(size,dropout),2)\n","        self.size=size\n","        \n","    def forward(self,x,mask):\n","        \n","        x=self.sublayer[0](x,lambda x: self.attn(x,x,x,mask))\n","        return self.sublayer[1](x,self.feed_forward)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cZjOa0vwCY5H","colab_type":"code","colab":{}},"source":["class Decoder(nn.Module):\n","    \n","    def __init__(self,layer,N):\n","        super().__init__()\n","        \n","        self.layers=clones(layer,N)\n","        self.norm=LayerNorm(layer.size)\n","    \n","    def forward(self,x,memory,curr_mask,tgt_mask):\n","        \n","        for layer in self.layers:\n","            x=layer(x,memory,curr_mask,tgt_mask)\n","            \n","        return self.norm(x)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FDHFdRvRCbgt","colab_type":"code","colab":{}},"source":["class DecoderLayer(nn.Module):\n","    \n","    def __init__(self,size,self_attn,src_attn,feed_forward,dropout):\n","        super().__init__()\n","        \n","        self.size=size\n","        self.self_attn=self_attn\n","        self.src_attn=src_attn\n","        self.feed_forward=feed_forward\n","        \n","        self.sublayer=clones(SublayerConnection(size,dropout),3)\n","        \n","    def forward(self,x,memory,src_mask,tgt_mask):\n","        \n","        m=memory\n","        x=self.sublayer[0](x,lambda x:self.self_attn(x,x,x,tgt_mask))\n","        x=self.sublayer[1](x,lambda x: self.src_attn(x,m,m,src_mask))\n","        return self.sublayer[2](x,self.feed_forward)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OWbh8piBCeC_","colab_type":"code","colab":{}},"source":["def attention(query,key,value,mask=None,dropout=None):\n","    \n","    d_k=query.size(-1)\n","\n","    scores=torch.matmul(query,key.transpose(-2,-1))/math.sqrt(d_k)\n","    \n","    if mask is not None:\n","        scores=scores.masked_fill(mask==0,-1e9)\n","        \n","    p_attn=F.softmax(scores,dim=-1)\n","    \n","    if dropout is not None:\n","        p_attn=dropout(p_attn)\n","        \n","    return torch.matmul(p_attn,value),p_attn"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZtHuOzFTCgie","colab_type":"code","colab":{}},"source":["class MultiHeadedAttention(nn.Module):\n","    \n","    def __init__(self,h,d_model,dropout=0.1):\n","        super().__init__()\n","        \n","        assert d_model%h==0\n","        \n","        self.d_k=d_model//h\n","        self.h=h\n","        self.linears=clones(nn.Linear(d_model,d_model),4)\n","        self.attn=None\n","        self.dropout=nn.Dropout(dropout)\n","        \n","    def forward(self,query,key,values,mask=None):\n","        \n","        if mask is not None:\n","            mask=mask.unsqueeze(1)\n","            \n","        nbatches=query.size(0)\n","        \n","        query,key,values=[l(x).view(nbatches,-1,self.h,self.d_k).transpose(1,2) for l, x in zip(self.linears,(query,key,values))]\n","        \n","        x,self.attn=attention(query,key,values,mask=mask,dropout=self.dropout)\n","        \n","        x=x.transpose(1,2).contiguous().view(nbatches,-1,self.h*self.d_k)\n","        \n","        return self.linears[-1](x)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GD9vxJrECkSp","colab_type":"code","colab":{}},"source":["class PositionwiseFeedForward(nn.Module):\n","    \n","    def __init__(self,d_model,d_ff,dropout=0.1):\n","        super().__init__()\n","        \n","        self.w_1=nn.Linear(d_model,d_ff)\n","        self.w_2=nn.Linear(d_ff,d_model)\n","        self.dropout=nn.Dropout(dropout)\n","        \n","    def forward(self,x):\n","        return self.w_2(self.dropout(F.relu(self.w_1(x))))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8Qna2tDeCmue","colab_type":"code","colab":{}},"source":["class Embeddings(nn.Module):\n","    \n","    def __init__(self,d_model,vocab):\n","        super().__init__()\n","        \n","        self.embed=nn.Embedding(vocab,d_model)\n","        self.d_model=d_model\n","    \n","    def forward(self,x):\n","#         print(x.device)\n","        return self.embed(x)*math.sqrt(self.d_model)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nCEQ9n0XCo7b","colab_type":"code","colab":{}},"source":["class PositionalEncoding(nn.Module):\n","    \n","    def __init__(self,d_model,dropout,max_len=5000):\n","        super().__init__()\n","        \n","        self.dropout=nn.Dropout(dropout)\n","        pe=torch.zeros(max_len,d_model,dtype=torch.float)\n","        position=torch.arange(0.,max_len).unsqueeze(1)\n","        div_term=torch.exp(torch.arange(0.,d_model,2)*-(math.log(10000.0)/d_model))\n","        \n","        pe[:,0::2]=torch.sin(position*div_term)\n","        pe[:,1::2]=torch.cos(position*div_term)\n","        \n","        pe=pe.unsqueeze(0)\n","        self.register_buffer('pe',pe)\n","        \n","    def forward(self,x):\n","        \n","        x=x+Variable(self.pe[:,:x.size(1)],requires_grad=False)\n","        return self.dropout(x)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CkLif2nLC5U6","colab_type":"code","colab":{}},"source":["\"\"\"\n","triu function generates a copy of matrix with elemens below kth diagonal zeroed.\n","The main diagonal is zeroeth diagonal above is first(k=1) and so on.\n","\n","Eg:\n","A=[[1,2,3],[4,5,6],[7,8,9]]\n","for above matrix:\n","triu(A,k=1)\n","will give [[0,2,3],[0,0,6],[0,0,0]]\n","\"\"\"\n","\n","def subsequent_mask(size):\n","    attn_shape=(1,size,size)\n","    mask=np.triu(np.ones(attn_shape),k=1).astype('uint8')\n","    \n","    return torch.from_numpy(mask)==0"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WPOL8rdMCruh","colab_type":"code","colab":{}},"source":["def make_model2(src_vocab,tgt_vocab,N=6,d_model=512,d_ff=2048,h=8,dropout=0.1):\n","    \n","    c=copy.deepcopy\n","    attn=MultiHeadedAttention(h,d_model)\n","    ff=PositionwiseFeedForward(d_model,d_ff,dropout)\n","    position=PositionalEncoding(d_model,dropout)\n","    model=EncoderDecoder(Encoder(EncoderLayer(d_model,c(attn),c(ff),dropout),N),\n","                        Decoder(DecoderLayer(d_model,c(attn),c(attn),c(ff),dropout),N),\n","                        nn.Sequential(Embeddings(d_model,src_vocab),c(position)),\n","                        nn.Sequential(Embeddings(d_model,tgt_vocab),c(position)),\n","                        Generator(d_model,tgt_vocab))\n","    \n","    for p in model.parameters():\n","        if p.dim()>1:\n","            nn.init.xavier_uniform_(p)\n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"K6TsrSL5CuNH","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"34069e92-eab5-45ab-88c8-91d25c463e1b","executionInfo":{"status":"ok","timestamp":1586270368434,"user_tz":-330,"elapsed":10181,"user":{"displayName":"deepak goyal","photoUrl":"https://lh5.googleusercontent.com/-3gfEZruD9Sk/AAAAAAAAAAI/AAAAAAAACvg/54H4lMsdOgA/s64/photo.jpg","userId":"05164064759516400423"}}},"source":["sample_model=make_model2(voc.num_words,voc.num_words,1,512,2048,8,0.1)\n","sample_model.to(device)\n","# print(sample_model)"],"execution_count":32,"outputs":[{"output_type":"execute_result","data":{"text/plain":["EncoderDecoder(\n","  (encoder): Encoder(\n","    (layers): ModuleList(\n","      (0): EncoderLayer(\n","        (attn): MultiHeadedAttention(\n","          (linears): ModuleList(\n","            (0): Linear(in_features=512, out_features=512, bias=True)\n","            (1): Linear(in_features=512, out_features=512, bias=True)\n","            (2): Linear(in_features=512, out_features=512, bias=True)\n","            (3): Linear(in_features=512, out_features=512, bias=True)\n","          )\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (sublayer): ModuleList(\n","          (0): SublayerConnection(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (norm): LayerNorm()\n","          )\n","          (1): SublayerConnection(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (norm): LayerNorm()\n","          )\n","        )\n","      )\n","    )\n","    (norm): LayerNorm()\n","  )\n","  (decoder): Decoder(\n","    (layers): ModuleList(\n","      (0): DecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linears): ModuleList(\n","            (0): Linear(in_features=512, out_features=512, bias=True)\n","            (1): Linear(in_features=512, out_features=512, bias=True)\n","            (2): Linear(in_features=512, out_features=512, bias=True)\n","            (3): Linear(in_features=512, out_features=512, bias=True)\n","          )\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (src_attn): MultiHeadedAttention(\n","          (linears): ModuleList(\n","            (0): Linear(in_features=512, out_features=512, bias=True)\n","            (1): Linear(in_features=512, out_features=512, bias=True)\n","            (2): Linear(in_features=512, out_features=512, bias=True)\n","            (3): Linear(in_features=512, out_features=512, bias=True)\n","          )\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (sublayer): ModuleList(\n","          (0): SublayerConnection(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (norm): LayerNorm()\n","          )\n","          (1): SublayerConnection(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (norm): LayerNorm()\n","          )\n","          (2): SublayerConnection(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (norm): LayerNorm()\n","          )\n","        )\n","      )\n","    )\n","    (norm): LayerNorm()\n","  )\n","  (source_embed): Sequential(\n","    (0): Embeddings(\n","      (embed): Embedding(7816, 512)\n","    )\n","    (1): PositionalEncoding(\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","  )\n","  (target_embed): Sequential(\n","    (0): Embeddings(\n","      (embed): Embedding(7816, 512)\n","    )\n","    (1): PositionalEncoding(\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","  )\n","  (generator): Generator(\n","    (projection): Linear(in_features=512, out_features=7816, bias=True)\n","  )\n",")"]},"metadata":{"tags":[]},"execution_count":32}]},{"cell_type":"code","metadata":{"id":"TZ0e9Y5cCwzw","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":69},"outputId":"92dc2f78-b725-4b00-f1fe-c5d77264e95e","executionInfo":{"status":"ok","timestamp":1586270375254,"user_tz":-330,"elapsed":1026,"user":{"displayName":"deepak goyal","photoUrl":"https://lh5.googleusercontent.com/-3gfEZruD9Sk/AAAAAAAAAAI/AAAAAAAACvg/54H4lMsdOgA/s64/photo.jpg","userId":"05164064759516400423"}}},"source":["#Sample Run\n","source=torch.ones(5,12,dtype=torch.long,device=device)\n","target=torch.ones(5,12,dtype=torch.long,device=device)\n","source_mask=torch.ones(5,12,12,dtype=torch.long,device=device)\n","target_mask=torch.ones(5,12,12,dtype=torch.long,device=device)\n","out=sample_model(source,target,source_mask,target_mask)\n","print(\"-\"*80)\n","print(\"Output size: \"+str(out.shape))\n","print(\"-\"*80)"],"execution_count":33,"outputs":[{"output_type":"stream","text":["--------------------------------------------------------------------------------\n","Output size: torch.Size([5, 12, 512])\n","--------------------------------------------------------------------------------\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Hb-TFfrCDn_q","colab_type":"text"},"source":["# Training\n"]},{"cell_type":"code","metadata":{"id":"KBz2k8ywDqZ4","colab_type":"code","colab":{}},"source":["def data_generation(pairs,batch_size,n_batches):\n","    \n","    sample_batches=[batch2TrainData(voc,[random.choice(pairs) for _ in range(batch_size)]) for _ in range(n_batches)]\n","    batches=[]\n","    \n","    for i in range(n_batches):\n","        batches.append(Batch(sample_batches[i][1],sample_batches[i][-1]))\n","#     batches=[]\n","#     for i in range(n_batches):\n","#         data = torch.from_numpy(np.random.randint(1, 11, size=(batch_size, 10)))\n","#         data[:,0]=1\n","        \n","#         batches.append(Batch(data,data))\n","    \n","    return batches"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"upPkHvWXECA2","colab_type":"code","colab":{}},"source":["class Batch:\n","    \"Object for holding a batch of data with mask during training.\"\n","    def __init__(self, src, trg=None, pad=0):\n","        src=torch.tensor(src).to(torch.int64)\n","        trg=torch.tensor(trg).to(torch.int64)\n","        self.src = src\n","        self.src_mask = (src != pad).unsqueeze(-2)\n","        if trg is not None:\n","            self.trg = trg[:, :-1]\n","            self.trg_y = trg[:, 1:]\n","            self.trg_mask = \\\n","                self.make_std_mask(self.trg, pad)\n","            self.ntokens = (self.trg_y != pad).data.sum()\n","        self.src.to(device)\n","        self.trg.to(device)\n","        self.src_mask.to(device)\n","        self.trg_mask.to(device)\n","    \n","    @staticmethod\n","    def make_std_mask(tgt, pad):\n","        \"Create a mask to hide padding and future words.\"\n","        tgt_mask = (tgt != pad).unsqueeze(-2)\n","        tgt_mask = tgt_mask & Variable(\n","            subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data))\n","        return tgt_mask"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YRV3rCOGEMFn","colab_type":"code","colab":{}},"source":["def run_epoch(data,model,loss_compute):\n","    \n","    start_time=time.time()\n","    total_tokens=0\n","    total_loss=0\n","    tokens=0\n","    source=data.src\n","    source=source.to(device)\n","    target=data.trg\n","    target=target.to(device)\n","    source_mask=data.src_mask\n","    source_mask=source_mask.to(device)\n","    target_mask=data.trg_mask\n","    target_mask=target_mask.to(device)\n","    target_y=data.trg_y\n","    target_y=target_y.to(device)\n","#     print(source.device)\n","    out=model(source,target,source_mask,target_mask)\n","#     print(\"Model output: \"+str(out.size()))\n","    loss=loss_compute(out,target_y,data.ntokens)\n","    \n","    return loss.item()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"54vD6-G4EQKM","colab_type":"code","colab":{}},"source":["class LabelSmoothing(nn.Module):\n","    \"Implement label smoothing.\"\n","    def __init__(self, size, padding_idx, smoothing=0.0):\n","        super(LabelSmoothing, self).__init__()\n","        self.criterion = nn.KLDivLoss(size_average=False)\n","        self.padding_idx = padding_idx\n","        self.confidence = 1.0 - smoothing\n","        self.smoothing = smoothing\n","        self.size = size\n","        self.true_dist = None\n","        \n","    def forward(self, x, target):\n","#         print(\"Before assertion: \"+str(x.size())+str(self.size))\n","        assert x.size(1) == self.size\n","        true_dist = x.data.clone()\n","        true_dist.fill_(self.smoothing / (self.size - 2))\n","        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n","        true_dist[:, self.padding_idx] = 0\n","        mask = torch.nonzero(target.data == self.padding_idx)\n","        if mask.dim() > 0:\n","            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n","        self.true_dist = true_dist\n","        return self.criterion(x, Variable(true_dist, requires_grad=False))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"a5Kfbj2TEXyp","colab_type":"code","colab":{}},"source":["class SimpleLossCompute:\n","    \"A simple loss compute and train function.\"\n","    def __init__(self, generator, criterion, opt=None):\n","        self.generator = generator\n","        self.criterion = criterion\n","        self.opt = opt\n","        \n","    def __call__(self, x, y, norm):\n","        x = self.generator(x)\n","#         print(str(x.size())+\" \"+str(y.size()))\n","        loss = self.criterion(x.contiguous().view(-1, x.size(-1)), \n","                              y.contiguous().view(-1)) / norm\n","        loss.backward()\n","        _=nn.utils.clip_grad_norm_(model.parameters(),10)\n","        plot_grad_flow(model.named_parameters())\n","        if self.opt is not None:\n","            self.opt.step()\n","            self.opt.optimizer.zero_grad()\n","        return loss.item()* norm\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vskTSBniEasq","colab_type":"code","colab":{}},"source":["class NoamOpt:\n","    \"Optim wrapper that implements rate.\"\n","    def __init__(self, model_size, factor, warmup, optimizer):\n","        self.optimizer = optimizer\n","        self._step = 0\n","        self.warmup = warmup\n","        self.factor = factor\n","        self.model_size = model_size\n","        self._rate = 0\n","        \n","    def step(self):\n","        \"Update parameters and rate\"\n","        self._step += 1\n","        rate = self.rate()\n","        for p in self.optimizer.param_groups:\n","            p['lr'] = rate\n","        self._rate = rate\n","        self.optimizer.step()\n","        \n","    def rate(self, step = None):\n","        \"Implement `lrate` above\"\n","        if step is None:\n","            step = self._step\n","        return self.factor * \\\n","            (self.model_size ** (-0.5) *\n","            min(step ** (-0.5), step * self.warmup ** (-1.5)))\n","        \n","def get_std_opt(model):\n","    return NoamOpt(model.src_embed[0].d_model, 2, 4000,\n","            torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ez4Swx52EfUT","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":107},"outputId":"18a7d6e4-85c0-43bc-c6bf-c1947d46906e","executionInfo":{"status":"ok","timestamp":1586272426783,"user_tz":-330,"elapsed":1063,"user":{"displayName":"deepak goyal","photoUrl":"https://lh5.googleusercontent.com/-3gfEZruD9Sk/AAAAAAAAAAI/AAAAAAAACvg/54H4lMsdOgA/s64/photo.jpg","userId":"05164064759516400423"}}},"source":["batches=data_generation(pairs,10,100)"],"execution_count":50,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  after removing the cwd from sys.path.\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  \"\"\"\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"b67TIlqdEj-O","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"4887b8e1-6aa4-4731-ca8a-0cb81fd7da0d"},"source":["print(\"Initialising and creating models....\")\n","V=voc.num_words\n","t1=time.time()\n","# criterion=LabelSmoothing()\n","criterion = LabelSmoothing(size=V, padding_idx=0, smoothing=0.0)\n","\n","model=make_model(V,V)\n","model.cuda()\n","\n","# model_opt=torch.optim.Adam(model.parameters(),lr=0.0001,betas=(0.9,0.988),eps=1e-9)\n","model_opt = NoamOpt(model.src_embed[0].d_model, 1, 400,\n","        torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))\n","print(\"=\"*100)\n","print(\"Creating Models took: \"+str(time.time()-t1))\n","\n","\n","\n","model.train()\n","for epoch in range(1000):\n","    t1=time.time()\n","    loss=0\n","    for i in range(100):\n","        current_batch=batches[i]\n","        loss_val=run_epoch(current_batch,model,SimpleLossCompute(model.generator, criterion, model_opt))\n","        loss+=loss_val\n","    print(\"Epoch: \"+str(epoch)+\" Loss Value: \"+str(loss/100))\n","    print(\"Time taken: \"+str(time.time()-t1))\n","    print(\"-\"*80)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Initialising and creating models....\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n","  warnings.warn(warning.format(ret))\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n"],"name":"stderr"},{"output_type":"stream","text":["====================================================================================================\n","Creating Models took: 0.6480197906494141\n","Epoch: 0 Loss Value: 360.74354583740234\n","Time taken: 10.859010457992554\n","--------------------------------------------------------------------------------\n","Epoch: 1 Loss Value: 269.6616229248047\n","Time taken: 10.693137407302856\n","--------------------------------------------------------------------------------\n","Epoch: 2 Loss Value: 251.21804077148437\n","Time taken: 10.867587327957153\n","--------------------------------------------------------------------------------\n","Epoch: 3 Loss Value: 238.109871673584\n","Time taken: 10.907806873321533\n","--------------------------------------------------------------------------------\n","Epoch: 4 Loss Value: 224.3737356567383\n","Time taken: 10.925720691680908\n","--------------------------------------------------------------------------------\n","Epoch: 5 Loss Value: 215.37139877319336\n","Time taken: 10.790450811386108\n","--------------------------------------------------------------------------------\n","Epoch: 6 Loss Value: 209.2080061340332\n","Time taken: 10.808822631835938\n","--------------------------------------------------------------------------------\n","Epoch: 7 Loss Value: 205.99732971191406\n","Time taken: 10.849873542785645\n","--------------------------------------------------------------------------------\n","Epoch: 8 Loss Value: 198.752992477417\n","Time taken: 10.812121152877808\n","--------------------------------------------------------------------------------\n","Epoch: 9 Loss Value: 194.18675094604492\n","Time taken: 11.006321907043457\n","--------------------------------------------------------------------------------\n","Epoch: 10 Loss Value: 189.73580574035645\n","Time taken: 11.173975944519043\n","--------------------------------------------------------------------------------\n","Epoch: 11 Loss Value: 186.79862800598144\n","Time taken: 11.041038751602173\n","--------------------------------------------------------------------------------\n","Epoch: 12 Loss Value: 187.16823486328124\n","Time taken: 14.614574432373047\n","--------------------------------------------------------------------------------\n","Epoch: 13 Loss Value: 180.24308372497558\n","Time taken: 10.999351024627686\n","--------------------------------------------------------------------------------\n","Epoch: 14 Loss Value: 174.57910263061524\n","Time taken: 11.036198854446411\n","--------------------------------------------------------------------------------\n","Epoch: 15 Loss Value: 169.26246513366698\n","Time taken: 11.030765056610107\n","--------------------------------------------------------------------------------\n","Epoch: 16 Loss Value: 166.64700019836425\n","Time taken: 11.072497606277466\n","--------------------------------------------------------------------------------\n","Epoch: 17 Loss Value: 161.4532396697998\n","Time taken: 11.099773645401001\n","--------------------------------------------------------------------------------\n","Epoch: 18 Loss Value: 159.2569200897217\n","Time taken: 11.10700273513794\n","--------------------------------------------------------------------------------\n","Epoch: 19 Loss Value: 157.164175491333\n","Time taken: 11.186789274215698\n","--------------------------------------------------------------------------------\n","Epoch: 20 Loss Value: 155.74467239379882\n","Time taken: 11.13568639755249\n","--------------------------------------------------------------------------------\n","Epoch: 21 Loss Value: 154.50112739562988\n","Time taken: 11.100713729858398\n","--------------------------------------------------------------------------------\n","Epoch: 22 Loss Value: 153.38719665527344\n","Time taken: 11.240790128707886\n","--------------------------------------------------------------------------------\n","Epoch: 23 Loss Value: 153.1590705871582\n","Time taken: 11.191323280334473\n","--------------------------------------------------------------------------------\n","Epoch: 24 Loss Value: 149.18539436340333\n","Time taken: 11.225869178771973\n","--------------------------------------------------------------------------------\n","Epoch: 25 Loss Value: 148.7892446899414\n","Time taken: 11.37455415725708\n","--------------------------------------------------------------------------------\n","Epoch: 26 Loss Value: 145.10810470581055\n","Time taken: 11.269887208938599\n","--------------------------------------------------------------------------------\n","Epoch: 27 Loss Value: 148.1492970275879\n","Time taken: 11.32280707359314\n","--------------------------------------------------------------------------------\n","Epoch: 28 Loss Value: 146.65223739624022\n","Time taken: 11.268312692642212\n","--------------------------------------------------------------------------------\n","Epoch: 29 Loss Value: 143.2439479827881\n","Time taken: 11.383978605270386\n","--------------------------------------------------------------------------------\n","Epoch: 30 Loss Value: 141.60976722717285\n","Time taken: 11.47257661819458\n","--------------------------------------------------------------------------------\n","Epoch: 31 Loss Value: 141.4158822631836\n","Time taken: 11.426642417907715\n","--------------------------------------------------------------------------------\n","Epoch: 32 Loss Value: 138.71025398254395\n","Time taken: 11.581490516662598\n","--------------------------------------------------------------------------------\n","Epoch: 33 Loss Value: 137.66039985656738\n","Time taken: 11.800240993499756\n","--------------------------------------------------------------------------------\n","Epoch: 34 Loss Value: 137.82530784606934\n","Time taken: 11.618523836135864\n","--------------------------------------------------------------------------------\n","Epoch: 35 Loss Value: 136.2249777984619\n","Time taken: 11.514086484909058\n","--------------------------------------------------------------------------------\n","Epoch: 36 Loss Value: 132.9758851623535\n","Time taken: 11.695669889450073\n","--------------------------------------------------------------------------------\n","Epoch: 37 Loss Value: 133.75371932983398\n","Time taken: 11.914561033248901\n","--------------------------------------------------------------------------------\n","Epoch: 38 Loss Value: 132.82759010314942\n","Time taken: 11.791089057922363\n","--------------------------------------------------------------------------------\n","Epoch: 39 Loss Value: 131.61925262451172\n","Time taken: 11.778231143951416\n","--------------------------------------------------------------------------------\n","Epoch: 40 Loss Value: 131.99238151550293\n","Time taken: 11.702773571014404\n","--------------------------------------------------------------------------------\n","Epoch: 41 Loss Value: 134.9387281036377\n","Time taken: 11.71981692314148\n","--------------------------------------------------------------------------------\n","Epoch: 42 Loss Value: 135.7906388092041\n","Time taken: 11.699045658111572\n","--------------------------------------------------------------------------------\n","Epoch: 43 Loss Value: 133.0953267669678\n","Time taken: 11.680201530456543\n","--------------------------------------------------------------------------------\n","Epoch: 44 Loss Value: 130.59558685302736\n","Time taken: 11.796676635742188\n","--------------------------------------------------------------------------------\n","Epoch: 45 Loss Value: 129.722600479126\n","Time taken: 11.817068576812744\n","--------------------------------------------------------------------------------\n","Epoch: 46 Loss Value: 128.09071769714356\n","Time taken: 11.884864807128906\n","--------------------------------------------------------------------------------\n","Epoch: 47 Loss Value: 130.6101831817627\n","Time taken: 12.326869249343872\n","--------------------------------------------------------------------------------\n","Epoch: 48 Loss Value: 132.58103790283204\n","Time taken: 11.82738995552063\n","--------------------------------------------------------------------------------\n","Epoch: 49 Loss Value: 132.78848114013672\n","Time taken: 11.954917192459106\n","--------------------------------------------------------------------------------\n","Epoch: 50 Loss Value: 131.62730560302734\n","Time taken: 11.956029176712036\n","--------------------------------------------------------------------------------\n","Epoch: 51 Loss Value: 129.75407432556153\n","Time taken: 12.060813188552856\n","--------------------------------------------------------------------------------\n","Epoch: 52 Loss Value: 129.69595764160155\n","Time taken: 12.099349737167358\n","--------------------------------------------------------------------------------\n","Epoch: 53 Loss Value: 128.2214520263672\n","Time taken: 12.049281358718872\n","--------------------------------------------------------------------------------\n","Epoch: 54 Loss Value: 127.23404411315919\n","Time taken: 12.131435871124268\n","--------------------------------------------------------------------------------\n","Epoch: 55 Loss Value: 126.40013038635254\n","Time taken: 12.198075294494629\n","--------------------------------------------------------------------------------\n","Epoch: 56 Loss Value: 125.94582946777344\n","Time taken: 12.198692798614502\n","--------------------------------------------------------------------------------\n","Epoch: 57 Loss Value: 124.14231048583984\n","Time taken: 12.178410053253174\n","--------------------------------------------------------------------------------\n","Epoch: 58 Loss Value: 121.99232948303222\n","Time taken: 12.20184850692749\n","--------------------------------------------------------------------------------\n","Epoch: 59 Loss Value: 120.7193465423584\n","Time taken: 12.503095149993896\n","--------------------------------------------------------------------------------\n","Epoch: 60 Loss Value: 121.08396438598633\n","Time taken: 12.356011629104614\n","--------------------------------------------------------------------------------\n","Epoch: 61 Loss Value: 119.21465156555176\n","Time taken: 12.254488468170166\n","--------------------------------------------------------------------------------\n","Epoch: 62 Loss Value: 118.53997352600098\n","Time taken: 12.302582025527954\n","--------------------------------------------------------------------------------\n","Epoch: 63 Loss Value: 116.45122184753419\n","Time taken: 13.070052146911621\n","--------------------------------------------------------------------------------\n","Epoch: 64 Loss Value: 116.38147575378417\n","Time taken: 12.389676809310913\n","--------------------------------------------------------------------------------\n","Epoch: 65 Loss Value: 117.28871551513672\n","Time taken: 12.542412996292114\n","--------------------------------------------------------------------------------\n","Epoch: 66 Loss Value: 116.16958236694336\n","Time taken: 12.519383668899536\n","--------------------------------------------------------------------------------\n","Epoch: 67 Loss Value: 113.30304008483887\n","Time taken: 12.628541707992554\n","--------------------------------------------------------------------------------\n","Epoch: 68 Loss Value: 113.08688591003418\n","Time taken: 12.519979238510132\n","--------------------------------------------------------------------------------\n","Epoch: 69 Loss Value: 113.48740852355957\n","Time taken: 12.61599326133728\n","--------------------------------------------------------------------------------\n","Epoch: 70 Loss Value: 111.70188339233398\n","Time taken: 12.606357336044312\n","--------------------------------------------------------------------------------\n","Epoch: 71 Loss Value: 110.66400978088379\n","Time taken: 12.644241094589233\n","--------------------------------------------------------------------------------\n","Epoch: 72 Loss Value: 111.6824494934082\n","Time taken: 12.754275560379028\n","--------------------------------------------------------------------------------\n","Epoch: 73 Loss Value: 110.67173301696778\n","Time taken: 12.684977054595947\n","--------------------------------------------------------------------------------\n","Epoch: 74 Loss Value: 108.882177734375\n","Time taken: 12.79935622215271\n","--------------------------------------------------------------------------------\n","Epoch: 75 Loss Value: 108.7889917755127\n","Time taken: 12.72028112411499\n","--------------------------------------------------------------------------------\n","Epoch: 76 Loss Value: 108.78056175231933\n","Time taken: 12.769230127334595\n","--------------------------------------------------------------------------------\n","Epoch: 77 Loss Value: 106.86836570739746\n","Time taken: 12.741396188735962\n","--------------------------------------------------------------------------------\n","Epoch: 78 Loss Value: 106.6362754058838\n","Time taken: 12.943485736846924\n","--------------------------------------------------------------------------------\n","Epoch: 79 Loss Value: 105.74418182373047\n","Time taken: 12.941398859024048\n","--------------------------------------------------------------------------------\n","Epoch: 80 Loss Value: 105.34428489685058\n","Time taken: 12.951220989227295\n","--------------------------------------------------------------------------------\n","Epoch: 81 Loss Value: 105.34267807006836\n","Time taken: 12.956035614013672\n","--------------------------------------------------------------------------------\n","Epoch: 82 Loss Value: 104.3950862121582\n","Time taken: 12.958685159683228\n","--------------------------------------------------------------------------------\n","Epoch: 83 Loss Value: 102.79716171264648\n","Time taken: 13.096132516860962\n","--------------------------------------------------------------------------------\n","Epoch: 84 Loss Value: 102.4249863433838\n","Time taken: 13.991261959075928\n","--------------------------------------------------------------------------------\n","Epoch: 85 Loss Value: 102.95423454284668\n","Time taken: 13.080362796783447\n","--------------------------------------------------------------------------------\n","Epoch: 86 Loss Value: 101.6701220703125\n","Time taken: 13.040276288986206\n","--------------------------------------------------------------------------------\n","Epoch: 87 Loss Value: 103.0476611328125\n","Time taken: 13.244256496429443\n","--------------------------------------------------------------------------------\n","Epoch: 88 Loss Value: 102.38580230712891\n","Time taken: 13.170274019241333\n","--------------------------------------------------------------------------------\n","Epoch: 89 Loss Value: 100.92359634399413\n","Time taken: 13.173040628433228\n","--------------------------------------------------------------------------------\n","Epoch: 90 Loss Value: 100.80276069641113\n","Time taken: 13.226308107376099\n","--------------------------------------------------------------------------------\n","Epoch: 91 Loss Value: 99.80888267517089\n","Time taken: 13.120670318603516\n","--------------------------------------------------------------------------------\n","Epoch: 92 Loss Value: 100.16594398498535\n","Time taken: 13.18002462387085\n","--------------------------------------------------------------------------------\n","Epoch: 93 Loss Value: 96.89938888549804\n","Time taken: 13.207926034927368\n","--------------------------------------------------------------------------------\n","Epoch: 94 Loss Value: 98.09302345275879\n","Time taken: 13.52424669265747\n","--------------------------------------------------------------------------------\n","Epoch: 95 Loss Value: 98.05665191650391\n","Time taken: 13.684509754180908\n","--------------------------------------------------------------------------------\n","Epoch: 96 Loss Value: 97.15426162719727\n","Time taken: 14.427587509155273\n","--------------------------------------------------------------------------------\n","Epoch: 97 Loss Value: 96.67558822631835\n","Time taken: 14.362848043441772\n","--------------------------------------------------------------------------------\n","Epoch: 98 Loss Value: 96.94550743103028\n","Time taken: 14.666290283203125\n","--------------------------------------------------------------------------------\n","Epoch: 99 Loss Value: 94.75529190063476\n","Time taken: 14.6433846950531\n","--------------------------------------------------------------------------------\n","Epoch: 100 Loss Value: 94.91041061401367\n","Time taken: 14.30775237083435\n","--------------------------------------------------------------------------------\n","Epoch: 101 Loss Value: 95.15700912475586\n","Time taken: 13.879387855529785\n","--------------------------------------------------------------------------------\n","Epoch: 102 Loss Value: 94.39881607055663\n","Time taken: 14.023507833480835\n","--------------------------------------------------------------------------------\n","Epoch: 103 Loss Value: 95.45226463317871\n","Time taken: 13.98807406425476\n","--------------------------------------------------------------------------------\n","Epoch: 104 Loss Value: 94.00689826965332\n","Time taken: 14.056560516357422\n","--------------------------------------------------------------------------------\n","Epoch: 105 Loss Value: 93.17369819641114\n","Time taken: 14.134249210357666\n","--------------------------------------------------------------------------------\n","Epoch: 106 Loss Value: 93.2544719696045\n","Time taken: 14.187232732772827\n","--------------------------------------------------------------------------------\n","Epoch: 107 Loss Value: 91.71059509277343\n","Time taken: 14.150885820388794\n","--------------------------------------------------------------------------------\n","Epoch: 108 Loss Value: 91.12889282226563\n","Time taken: 14.161037921905518\n","--------------------------------------------------------------------------------\n","Epoch: 109 Loss Value: 91.36860969543457\n","Time taken: 14.383450746536255\n","--------------------------------------------------------------------------------\n","Epoch: 110 Loss Value: 92.38837070465088\n","Time taken: 15.124406814575195\n","--------------------------------------------------------------------------------\n","Epoch: 111 Loss Value: 91.11693222045898\n","Time taken: 14.202755451202393\n","--------------------------------------------------------------------------------\n","Epoch: 112 Loss Value: 90.42539672851562\n","Time taken: 14.380918979644775\n","--------------------------------------------------------------------------------\n","Epoch: 113 Loss Value: 90.19732734680176\n","Time taken: 14.38205862045288\n","--------------------------------------------------------------------------------\n","Epoch: 114 Loss Value: 89.99412517547607\n","Time taken: 14.332224130630493\n","--------------------------------------------------------------------------------\n","Epoch: 115 Loss Value: 89.16720565795899\n","Time taken: 14.273758888244629\n","--------------------------------------------------------------------------------\n","Epoch: 116 Loss Value: 88.36022521972656\n","Time taken: 14.366982460021973\n","--------------------------------------------------------------------------------\n","Epoch: 117 Loss Value: 88.28617668151855\n","Time taken: 14.315863132476807\n","--------------------------------------------------------------------------------\n","Epoch: 118 Loss Value: 88.32520305633545\n","Time taken: 14.350903749465942\n","--------------------------------------------------------------------------------\n","Epoch: 119 Loss Value: 88.33074527740479\n","Time taken: 14.436565637588501\n","--------------------------------------------------------------------------------\n","Epoch: 120 Loss Value: 88.21372856140137\n","Time taken: 14.35368013381958\n","--------------------------------------------------------------------------------\n","Epoch: 121 Loss Value: 87.43546409606934\n","Time taken: 14.258307218551636\n","--------------------------------------------------------------------------------\n","Epoch: 122 Loss Value: 87.18473545074463\n","Time taken: 14.397320985794067\n","--------------------------------------------------------------------------------\n","Epoch: 123 Loss Value: 86.07255661010743\n","Time taken: 14.422618627548218\n","--------------------------------------------------------------------------------\n","Epoch: 124 Loss Value: 85.74595237731934\n","Time taken: 14.43992280960083\n","--------------------------------------------------------------------------------\n","Epoch: 125 Loss Value: 86.21909439086915\n","Time taken: 14.521846771240234\n","--------------------------------------------------------------------------------\n","Epoch: 126 Loss Value: 86.48130630493164\n","Time taken: 14.467835426330566\n","--------------------------------------------------------------------------------\n","Epoch: 127 Loss Value: 85.14404766082764\n","Time taken: 14.757458448410034\n","--------------------------------------------------------------------------------\n","Epoch: 128 Loss Value: 85.6612564086914\n","Time taken: 14.528137922286987\n","--------------------------------------------------------------------------------\n","Epoch: 129 Loss Value: 83.63071239471435\n","Time taken: 14.695709466934204\n","--------------------------------------------------------------------------------\n","Epoch: 130 Loss Value: 83.69517055511474\n","Time taken: 14.952672481536865\n","--------------------------------------------------------------------------------\n","Epoch: 131 Loss Value: 84.03976600646973\n","Time taken: 14.922444343566895\n","--------------------------------------------------------------------------------\n","Epoch: 132 Loss Value: 83.28110069274902\n","Time taken: 14.735575914382935\n","--------------------------------------------------------------------------------\n","Epoch: 133 Loss Value: 83.74111190795898\n","Time taken: 14.639038562774658\n","--------------------------------------------------------------------------------\n","Epoch: 134 Loss Value: 83.63381050109864\n","Time taken: 14.773627996444702\n","--------------------------------------------------------------------------------\n","Epoch: 135 Loss Value: 82.34651153564454\n","Time taken: 14.855660915374756\n","--------------------------------------------------------------------------------\n","Epoch: 136 Loss Value: 81.7541989517212\n","Time taken: 14.883414268493652\n","--------------------------------------------------------------------------------\n","Epoch: 137 Loss Value: 82.36486587524413\n","Time taken: 14.868417263031006\n","--------------------------------------------------------------------------------\n","Epoch: 138 Loss Value: 82.20031414031982\n","Time taken: 14.968696355819702\n","--------------------------------------------------------------------------------\n","Epoch: 139 Loss Value: 83.09190929412841\n","Time taken: 14.937305212020874\n","--------------------------------------------------------------------------------\n","Epoch: 140 Loss Value: 82.47561855316162\n","Time taken: 15.009620666503906\n","--------------------------------------------------------------------------------\n","Epoch: 141 Loss Value: 81.2194953918457\n","Time taken: 15.121794939041138\n","--------------------------------------------------------------------------------\n","Epoch: 142 Loss Value: 80.4181950378418\n","Time taken: 16.11261248588562\n","--------------------------------------------------------------------------------\n","Epoch: 143 Loss Value: 81.00522315979003\n","Time taken: 15.034639120101929\n","--------------------------------------------------------------------------------\n","Epoch: 144 Loss Value: 79.8268664932251\n","Time taken: 15.132663249969482\n","--------------------------------------------------------------------------------\n","Epoch: 145 Loss Value: 80.59430870056153\n","Time taken: 15.21996784210205\n","--------------------------------------------------------------------------------\n","Epoch: 146 Loss Value: 80.20469997406006\n","Time taken: 15.387579917907715\n","--------------------------------------------------------------------------------\n","Epoch: 147 Loss Value: 80.63957305908202\n","Time taken: 15.306373596191406\n","--------------------------------------------------------------------------------\n","Epoch: 148 Loss Value: 79.59718070983887\n","Time taken: 15.242201328277588\n","--------------------------------------------------------------------------------\n","Epoch: 149 Loss Value: 78.97944145202636\n","Time taken: 15.285927057266235\n","--------------------------------------------------------------------------------\n","Epoch: 150 Loss Value: 78.80306705474854\n","Time taken: 15.354228734970093\n","--------------------------------------------------------------------------------\n","Epoch: 151 Loss Value: 79.3348749923706\n","Time taken: 15.480777740478516\n","--------------------------------------------------------------------------------\n","Epoch: 152 Loss Value: 78.92149337768555\n","Time taken: 15.394318103790283\n","--------------------------------------------------------------------------------\n","Epoch: 153 Loss Value: 78.01161712646484\n","Time taken: 15.38180947303772\n","--------------------------------------------------------------------------------\n","Epoch: 154 Loss Value: 78.99252510070801\n","Time taken: 15.446614980697632\n","--------------------------------------------------------------------------------\n","Epoch: 155 Loss Value: 78.23768306732178\n","Time taken: 15.501013040542603\n","--------------------------------------------------------------------------------\n","Epoch: 156 Loss Value: 77.5872314453125\n","Time taken: 15.547523975372314\n","--------------------------------------------------------------------------------\n","Epoch: 157 Loss Value: 76.71263904571533\n","Time taken: 15.524760723114014\n","--------------------------------------------------------------------------------\n","Epoch: 158 Loss Value: 76.94175373077393\n","Time taken: 15.451377391815186\n","--------------------------------------------------------------------------------\n","Epoch: 159 Loss Value: 77.20647724151611\n","Time taken: 15.59766674041748\n","--------------------------------------------------------------------------------\n","Epoch: 160 Loss Value: 76.7757154083252\n","Time taken: 15.625087261199951\n","--------------------------------------------------------------------------------\n","Epoch: 161 Loss Value: 76.86960529327392\n","Time taken: 15.718598365783691\n","--------------------------------------------------------------------------------\n","Epoch: 162 Loss Value: 76.64438011169433\n","Time taken: 15.839311599731445\n","--------------------------------------------------------------------------------\n","Epoch: 163 Loss Value: 77.28686122894287\n","Time taken: 15.606522798538208\n","--------------------------------------------------------------------------------\n","Epoch: 164 Loss Value: 76.52619094848633\n","Time taken: 15.619163036346436\n","--------------------------------------------------------------------------------\n","Epoch: 165 Loss Value: 76.14842529296875\n","Time taken: 15.696138858795166\n","--------------------------------------------------------------------------------\n","Epoch: 166 Loss Value: 75.88283847808837\n","Time taken: 15.87628960609436\n","--------------------------------------------------------------------------------\n","Epoch: 167 Loss Value: 76.092099609375\n","Time taken: 16.003018140792847\n","--------------------------------------------------------------------------------\n","Epoch: 168 Loss Value: 76.15929821014404\n","Time taken: 15.875489234924316\n","--------------------------------------------------------------------------------\n","Epoch: 169 Loss Value: 75.9087451171875\n","Time taken: 16.0389244556427\n","--------------------------------------------------------------------------------\n","Epoch: 170 Loss Value: 75.0266418838501\n","Time taken: 16.06834101676941\n","--------------------------------------------------------------------------------\n","Epoch: 171 Loss Value: 75.09001274108887\n","Time taken: 16.242608070373535\n","--------------------------------------------------------------------------------\n","Epoch: 172 Loss Value: 76.54113456726074\n","Time taken: 16.26466131210327\n","--------------------------------------------------------------------------------\n","Epoch: 173 Loss Value: 75.728256149292\n","Time taken: 16.057533025741577\n","--------------------------------------------------------------------------------\n","Epoch: 174 Loss Value: 75.99457176208496\n","Time taken: 16.10914635658264\n","--------------------------------------------------------------------------------\n","Epoch: 175 Loss Value: 75.01018093109131\n","Time taken: 16.096408367156982\n","--------------------------------------------------------------------------------\n","Epoch: 176 Loss Value: 75.07577758789063\n","Time taken: 16.361558437347412\n","--------------------------------------------------------------------------------\n","Epoch: 177 Loss Value: 74.5406095123291\n","Time taken: 16.3368821144104\n","--------------------------------------------------------------------------------\n","Epoch: 178 Loss Value: 74.52724754333497\n","Time taken: 16.25431990623474\n","--------------------------------------------------------------------------------\n","Epoch: 179 Loss Value: 74.03929203033448\n","Time taken: 16.24946165084839\n","--------------------------------------------------------------------------------\n","Epoch: 180 Loss Value: 74.04873611450195\n","Time taken: 16.34404230117798\n","--------------------------------------------------------------------------------\n","Epoch: 181 Loss Value: 74.2495365524292\n","Time taken: 16.51712679862976\n","--------------------------------------------------------------------------------\n","Epoch: 182 Loss Value: 73.57117622375489\n","Time taken: 16.48556113243103\n","--------------------------------------------------------------------------------\n","Epoch: 183 Loss Value: 73.19178115844727\n","Time taken: 17.746472358703613\n","--------------------------------------------------------------------------------\n","Epoch: 184 Loss Value: 72.76613098144531\n","Time taken: 16.496992826461792\n","--------------------------------------------------------------------------------\n","Epoch: 185 Loss Value: 73.29921825408935\n","Time taken: 16.49704933166504\n","--------------------------------------------------------------------------------\n","Epoch: 186 Loss Value: 72.20008251190185\n","Time taken: 16.735931396484375\n","--------------------------------------------------------------------------------\n","Epoch: 187 Loss Value: 71.76192848205567\n","Time taken: 16.650972604751587\n","--------------------------------------------------------------------------------\n","Epoch: 188 Loss Value: 72.40069946289063\n","Time taken: 16.589521646499634\n","--------------------------------------------------------------------------------\n","Epoch: 189 Loss Value: 71.29597190856934\n","Time taken: 16.62823176383972\n","--------------------------------------------------------------------------------\n","Epoch: 190 Loss Value: 71.77490951538086\n","Time taken: 16.92678451538086\n","--------------------------------------------------------------------------------\n","Epoch: 191 Loss Value: 71.68565181732178\n","Time taken: 16.860539436340332\n","--------------------------------------------------------------------------------\n","Epoch: 192 Loss Value: 70.47422386169434\n","Time taken: 16.845332384109497\n","--------------------------------------------------------------------------------\n","Epoch: 193 Loss Value: 72.33615470886231\n","Time taken: 16.794644355773926\n","--------------------------------------------------------------------------------\n","Epoch: 194 Loss Value: 70.97875320434571\n","Time taken: 16.898619174957275\n","--------------------------------------------------------------------------------\n","Epoch: 195 Loss Value: 71.57252475738525\n","Time taken: 16.95812153816223\n","--------------------------------------------------------------------------------\n","Epoch: 196 Loss Value: 70.5674563217163\n","Time taken: 16.96497893333435\n","--------------------------------------------------------------------------------\n","Epoch: 197 Loss Value: 70.59255962371826\n","Time taken: 16.968873262405396\n","--------------------------------------------------------------------------------\n","Epoch: 198 Loss Value: 70.47471851348877\n","Time taken: 17.010119199752808\n","--------------------------------------------------------------------------------\n","Epoch: 199 Loss Value: 69.72808567047119\n","Time taken: 16.94079089164734\n","--------------------------------------------------------------------------------\n","Epoch: 200 Loss Value: 69.37273468017578\n","Time taken: 17.058029651641846\n","--------------------------------------------------------------------------------\n","Epoch: 201 Loss Value: 69.97483684539795\n","Time taken: 17.383964776992798\n","--------------------------------------------------------------------------------\n","Epoch: 202 Loss Value: 69.75732917785645\n","Time taken: 17.202068090438843\n","--------------------------------------------------------------------------------\n","Epoch: 203 Loss Value: 69.22192306518555\n","Time taken: 17.198692798614502\n","--------------------------------------------------------------------------------\n","Epoch: 204 Loss Value: 68.52082614898681\n","Time taken: 17.268235206604004\n","--------------------------------------------------------------------------------\n","Epoch: 205 Loss Value: 69.29055595397949\n","Time taken: 17.321412801742554\n","--------------------------------------------------------------------------------\n","Epoch: 206 Loss Value: 69.21341213226319\n","Time taken: 17.40851068496704\n","--------------------------------------------------------------------------------\n","Epoch: 207 Loss Value: 68.22749168395995\n","Time taken: 17.885026216506958\n","--------------------------------------------------------------------------------\n","Epoch: 208 Loss Value: 69.05552715301513\n","Time taken: 17.576338291168213\n","--------------------------------------------------------------------------------\n","Epoch: 209 Loss Value: 68.4822615814209\n","Time taken: 17.26449990272522\n","--------------------------------------------------------------------------------\n","Epoch: 210 Loss Value: 67.93990737915038\n","Time taken: 17.437700033187866\n","--------------------------------------------------------------------------------\n","Epoch: 211 Loss Value: 67.91709251403809\n","Time taken: 17.36993908882141\n","--------------------------------------------------------------------------------\n","Epoch: 212 Loss Value: 68.29719539642333\n","Time taken: 17.51974606513977\n","--------------------------------------------------------------------------------\n","Epoch: 213 Loss Value: 67.11578910827637\n","Time taken: 17.453171491622925\n","--------------------------------------------------------------------------------\n","Epoch: 214 Loss Value: 68.06870639801025\n","Time taken: 17.5983145236969\n","--------------------------------------------------------------------------------\n","Epoch: 215 Loss Value: 67.19650337219238\n","Time taken: 17.70450210571289\n","--------------------------------------------------------------------------------\n","Epoch: 216 Loss Value: 66.95536392211915\n","Time taken: 17.764768600463867\n","--------------------------------------------------------------------------------\n","Epoch: 217 Loss Value: 66.35053771972656\n","Time taken: 17.622326850891113\n","--------------------------------------------------------------------------------\n","Epoch: 218 Loss Value: 66.96257579803466\n","Time taken: 17.761802673339844\n","--------------------------------------------------------------------------------\n","Epoch: 219 Loss Value: 67.00204387664795\n","Time taken: 17.749445915222168\n","--------------------------------------------------------------------------------\n","Epoch: 220 Loss Value: 66.84852214813232\n","Time taken: 17.785496950149536\n","--------------------------------------------------------------------------------\n","Epoch: 221 Loss Value: 66.60305076599121\n","Time taken: 17.954533576965332\n","--------------------------------------------------------------------------------\n","Epoch: 222 Loss Value: 66.16204639434814\n","Time taken: 17.79694128036499\n","--------------------------------------------------------------------------------\n","Epoch: 223 Loss Value: 66.11360721588134\n","Time taken: 17.891955375671387\n","--------------------------------------------------------------------------------\n","Epoch: 224 Loss Value: 67.10730854034423\n","Time taken: 18.005744695663452\n","--------------------------------------------------------------------------------\n","Epoch: 225 Loss Value: 66.32914379119873\n","Time taken: 18.019912004470825\n","--------------------------------------------------------------------------------\n","Epoch: 226 Loss Value: 65.13221111297608\n","Time taken: 18.34422278404236\n","--------------------------------------------------------------------------------\n","Epoch: 227 Loss Value: 65.54840347290039\n","Time taken: 18.14670205116272\n","--------------------------------------------------------------------------------\n","Epoch: 228 Loss Value: 65.03708061218262\n","Time taken: 18.643862009048462\n","--------------------------------------------------------------------------------\n","Epoch: 229 Loss Value: 65.77006916046143\n","Time taken: 18.133127450942993\n","--------------------------------------------------------------------------------\n","Epoch: 230 Loss Value: 65.81393756866456\n","Time taken: 18.048980712890625\n","--------------------------------------------------------------------------------\n","Epoch: 231 Loss Value: 64.5623804473877\n","Time taken: 18.1176655292511\n","--------------------------------------------------------------------------------\n","Epoch: 232 Loss Value: 64.63307346343994\n","Time taken: 18.229387283325195\n","--------------------------------------------------------------------------------\n","Epoch: 233 Loss Value: 63.73951656341553\n","Time taken: 19.9889395236969\n","--------------------------------------------------------------------------------\n","Epoch: 234 Loss Value: 64.77702575683594\n","Time taken: 18.40920925140381\n","--------------------------------------------------------------------------------\n","Epoch: 235 Loss Value: 63.778544692993165\n","Time taken: 18.249053955078125\n","--------------------------------------------------------------------------------\n","Epoch: 236 Loss Value: 63.636344566345215\n","Time taken: 18.325232982635498\n","--------------------------------------------------------------------------------\n","Epoch: 237 Loss Value: 63.91374504089355\n","Time taken: 18.465424299240112\n","--------------------------------------------------------------------------------\n","Epoch: 238 Loss Value: 63.85672233581543\n","Time taken: 18.551015377044678\n","--------------------------------------------------------------------------------\n","Epoch: 239 Loss Value: 63.86378265380859\n","Time taken: 18.558483123779297\n","--------------------------------------------------------------------------------\n","Epoch: 240 Loss Value: 63.708302116394044\n","Time taken: 18.55435585975647\n","--------------------------------------------------------------------------------\n","Epoch: 241 Loss Value: 64.19334804534913\n","Time taken: 18.78470015525818\n","--------------------------------------------------------------------------------\n","Epoch: 242 Loss Value: 63.505709953308106\n","Time taken: 18.596681118011475\n","--------------------------------------------------------------------------------\n","Epoch: 243 Loss Value: 63.35094951629639\n","Time taken: 18.623020887374878\n","--------------------------------------------------------------------------------\n","Epoch: 244 Loss Value: 62.120272407531736\n","Time taken: 18.54655146598816\n","--------------------------------------------------------------------------------\n","Epoch: 245 Loss Value: 61.990920524597165\n","Time taken: 18.58305859565735\n","--------------------------------------------------------------------------------\n","Epoch: 246 Loss Value: 63.00406574249268\n","Time taken: 18.747880458831787\n","--------------------------------------------------------------------------------\n","Epoch: 247 Loss Value: 62.35259475708008\n","Time taken: 18.57587194442749\n","--------------------------------------------------------------------------------\n","Epoch: 248 Loss Value: 62.35214256286621\n","Time taken: 18.56732726097107\n","--------------------------------------------------------------------------------\n","Epoch: 249 Loss Value: 61.53001441955566\n","Time taken: 18.784687280654907\n","--------------------------------------------------------------------------------\n","Epoch: 250 Loss Value: 62.00709678649903\n","Time taken: 18.884944438934326\n","--------------------------------------------------------------------------------\n","Epoch: 251 Loss Value: 61.658320579528805\n","Time taken: 18.831191539764404\n","--------------------------------------------------------------------------------\n","Epoch: 252 Loss Value: 61.42780948638916\n","Time taken: 18.78496813774109\n","--------------------------------------------------------------------------------\n","Epoch: 253 Loss Value: 62.12590167999267\n","Time taken: 18.74844717979431\n","--------------------------------------------------------------------------------\n","Epoch: 254 Loss Value: 61.02218067169189\n","Time taken: 19.04097008705139\n","--------------------------------------------------------------------------------\n","Epoch: 255 Loss Value: 61.82484405517578\n","Time taken: 18.98812198638916\n","--------------------------------------------------------------------------------\n","Epoch: 256 Loss Value: 60.63660400390625\n","Time taken: 18.97879457473755\n","--------------------------------------------------------------------------------\n","Epoch: 257 Loss Value: 60.0839058303833\n","Time taken: 19.093061208724976\n","--------------------------------------------------------------------------------\n","Epoch: 258 Loss Value: 59.51758968353271\n","Time taken: 19.190168142318726\n","--------------------------------------------------------------------------------\n","Epoch: 259 Loss Value: 60.099352989196774\n","Time taken: 19.35555863380432\n","--------------------------------------------------------------------------------\n","Epoch: 260 Loss Value: 59.480897483825686\n","Time taken: 19.082426071166992\n","--------------------------------------------------------------------------------\n","Epoch: 261 Loss Value: 58.73437335968018\n","Time taken: 19.16128373146057\n","--------------------------------------------------------------------------------\n","Epoch: 262 Loss Value: 59.925071449279784\n","Time taken: 19.123456239700317\n","--------------------------------------------------------------------------------\n","Epoch: 263 Loss Value: 60.15213153839111\n","Time taken: 19.288646936416626\n","--------------------------------------------------------------------------------\n","Epoch: 264 Loss Value: 59.47709300994873\n","Time taken: 19.266385793685913\n","--------------------------------------------------------------------------------\n","Epoch: 265 Loss Value: 59.638015289306644\n","Time taken: 19.277461528778076\n","--------------------------------------------------------------------------------\n","Epoch: 266 Loss Value: 60.032013702392575\n","Time taken: 19.2971134185791\n","--------------------------------------------------------------------------------\n","Epoch: 267 Loss Value: 59.12517246246338\n","Time taken: 19.461076736450195\n","--------------------------------------------------------------------------------\n","Epoch: 268 Loss Value: 58.22851749420166\n","Time taken: 19.438800573349\n","--------------------------------------------------------------------------------\n","Epoch: 269 Loss Value: 58.70699645996094\n","Time taken: 19.33536195755005\n","--------------------------------------------------------------------------------\n","Epoch: 270 Loss Value: 58.55617488861084\n","Time taken: 19.65070867538452\n","--------------------------------------------------------------------------------\n","Epoch: 271 Loss Value: 58.28939842224121\n","Time taken: 19.671950340270996\n","--------------------------------------------------------------------------------\n","Epoch: 272 Loss Value: 58.635913352966305\n","Time taken: 19.50008463859558\n","--------------------------------------------------------------------------------\n","Epoch: 273 Loss Value: 57.63520957946778\n","Time taken: 19.55653738975525\n","--------------------------------------------------------------------------------\n","Epoch: 274 Loss Value: 58.130144920349125\n","Time taken: 19.6913640499115\n","--------------------------------------------------------------------------------\n","Epoch: 275 Loss Value: 57.79086124420166\n","Time taken: 20.092432260513306\n","--------------------------------------------------------------------------------\n","Epoch: 276 Loss Value: 58.16241672515869\n","Time taken: 19.842674493789673\n","--------------------------------------------------------------------------------\n","Epoch: 277 Loss Value: 58.120353851318356\n","Time taken: 19.8820903301239\n","--------------------------------------------------------------------------------\n","Epoch: 278 Loss Value: 57.4015609741211\n","Time taken: 19.924306392669678\n","--------------------------------------------------------------------------------\n","Epoch: 279 Loss Value: 57.56825096130371\n","Time taken: 20.118587732315063\n","--------------------------------------------------------------------------------\n","Epoch: 280 Loss Value: 57.27880916595459\n","Time taken: 20.008037090301514\n","--------------------------------------------------------------------------------\n","Epoch: 281 Loss Value: 57.673499488830565\n","Time taken: 19.979793787002563\n","--------------------------------------------------------------------------------\n","Epoch: 282 Loss Value: 56.63468864440918\n","Time taken: 20.2484233379364\n","--------------------------------------------------------------------------------\n","Epoch: 283 Loss Value: 56.88654399871826\n","Time taken: 20.328902006149292\n","--------------------------------------------------------------------------------\n","Epoch: 284 Loss Value: 57.53302387237549\n","Time taken: 20.163816452026367\n","--------------------------------------------------------------------------------\n","Epoch: 285 Loss Value: 56.87971908569336\n","Time taken: 20.45582365989685\n","--------------------------------------------------------------------------------\n","Epoch: 286 Loss Value: 56.66333236694336\n","Time taken: 20.21078372001648\n","--------------------------------------------------------------------------------\n","Epoch: 287 Loss Value: 56.486907424926756\n","Time taken: 20.40234375\n","--------------------------------------------------------------------------------\n","Epoch: 288 Loss Value: 55.87905536651611\n","Time taken: 20.53683066368103\n","--------------------------------------------------------------------------------\n","Epoch: 289 Loss Value: 56.62980125427246\n","Time taken: 20.426963567733765\n","--------------------------------------------------------------------------------\n","Epoch: 290 Loss Value: 55.16600353240967\n","Time taken: 20.576536417007446\n","--------------------------------------------------------------------------------\n","Epoch: 291 Loss Value: 55.3898087310791\n","Time taken: 20.711655855178833\n","--------------------------------------------------------------------------------\n","Epoch: 292 Loss Value: 55.22148658752442\n","Time taken: 20.60880208015442\n","--------------------------------------------------------------------------------\n","Epoch: 293 Loss Value: 55.11303840637207\n","Time taken: 20.70200228691101\n","--------------------------------------------------------------------------------\n","Epoch: 294 Loss Value: 54.82546642303467\n","Time taken: 20.806151151657104\n","--------------------------------------------------------------------------------\n","Epoch: 295 Loss Value: 54.46824096679688\n","Time taken: 22.892444610595703\n","--------------------------------------------------------------------------------\n","Epoch: 296 Loss Value: 54.772861557006834\n","Time taken: 20.81755232810974\n","--------------------------------------------------------------------------------\n","Epoch: 297 Loss Value: 54.66121398925781\n","Time taken: 20.96480441093445\n","--------------------------------------------------------------------------------\n","Epoch: 298 Loss Value: 53.50042751312256\n","Time taken: 20.83803963661194\n","--------------------------------------------------------------------------------\n","Epoch: 299 Loss Value: 54.1539822769165\n","Time taken: 21.077580213546753\n","--------------------------------------------------------------------------------\n","Epoch: 300 Loss Value: 54.92585872650147\n","Time taken: 20.987043857574463\n","--------------------------------------------------------------------------------\n","Epoch: 301 Loss Value: 54.16582130432129\n","Time taken: 20.964455127716064\n","--------------------------------------------------------------------------------\n","Epoch: 302 Loss Value: 53.396380195617674\n","Time taken: 20.897571802139282\n","--------------------------------------------------------------------------------\n","Epoch: 303 Loss Value: 53.676645851135255\n","Time taken: 21.10951519012451\n","--------------------------------------------------------------------------------\n","Epoch: 304 Loss Value: 53.86091022491455\n","Time taken: 21.04102659225464\n","--------------------------------------------------------------------------------\n","Epoch: 305 Loss Value: 54.279781608581544\n","Time taken: 21.35584044456482\n","--------------------------------------------------------------------------------\n","Epoch: 306 Loss Value: 53.544633598327636\n","Time taken: 21.190778017044067\n","--------------------------------------------------------------------------------\n","Epoch: 307 Loss Value: 53.392208938598635\n","Time taken: 21.379120588302612\n","--------------------------------------------------------------------------------\n","Epoch: 308 Loss Value: 51.82492733001709\n","Time taken: 21.147657871246338\n","--------------------------------------------------------------------------------\n","Epoch: 309 Loss Value: 52.62031593322754\n","Time taken: 21.390681505203247\n","--------------------------------------------------------------------------------\n","Epoch: 310 Loss Value: 51.90263027191162\n","Time taken: 21.298495531082153\n","--------------------------------------------------------------------------------\n","Epoch: 311 Loss Value: 52.164727210998535\n","Time taken: 21.340447902679443\n","--------------------------------------------------------------------------------\n","Epoch: 312 Loss Value: 51.49191761016846\n","Time taken: 21.697445392608643\n","--------------------------------------------------------------------------------\n","Epoch: 313 Loss Value: 50.32997119903565\n","Time taken: 21.802578687667847\n","--------------------------------------------------------------------------------\n","Epoch: 314 Loss Value: 50.29850856781006\n","Time taken: 22.13079810142517\n","--------------------------------------------------------------------------------\n","Epoch: 315 Loss Value: 51.228568954467775\n","Time taken: 21.6324565410614\n","--------------------------------------------------------------------------------\n","Epoch: 316 Loss Value: 51.275429801940916\n","Time taken: 21.4980046749115\n","--------------------------------------------------------------------------------\n","Epoch: 317 Loss Value: 50.378083839416504\n","Time taken: 21.511796236038208\n","--------------------------------------------------------------------------------\n","Epoch: 318 Loss Value: 50.11838283538818\n","Time taken: 21.638580799102783\n","--------------------------------------------------------------------------------\n","Epoch: 319 Loss Value: 49.83462465286255\n","Time taken: 21.67532753944397\n","--------------------------------------------------------------------------------\n","Epoch: 320 Loss Value: 49.47864168167114\n","Time taken: 21.849100351333618\n","--------------------------------------------------------------------------------\n","Epoch: 321 Loss Value: 49.64500095367432\n","Time taken: 21.798122882843018\n","--------------------------------------------------------------------------------\n","Epoch: 322 Loss Value: 49.134811573028564\n","Time taken: 21.782713174819946\n","--------------------------------------------------------------------------------\n","Epoch: 323 Loss Value: 49.370606060028074\n","Time taken: 21.759178400039673\n","--------------------------------------------------------------------------------\n","Epoch: 324 Loss Value: 48.911705322265625\n","Time taken: 21.728620290756226\n","--------------------------------------------------------------------------------\n","Epoch: 325 Loss Value: 48.97666675567627\n","Time taken: 21.85225510597229\n","--------------------------------------------------------------------------------\n","Epoch: 326 Loss Value: 49.21336996078491\n","Time taken: 21.741477727890015\n","--------------------------------------------------------------------------------\n","Epoch: 327 Loss Value: 49.0826741027832\n","Time taken: 21.75699782371521\n","--------------------------------------------------------------------------------\n","Epoch: 328 Loss Value: 49.02683032989502\n","Time taken: 21.947187185287476\n","--------------------------------------------------------------------------------\n","Epoch: 329 Loss Value: 48.45954944610596\n","Time taken: 21.969257831573486\n","--------------------------------------------------------------------------------\n","Epoch: 330 Loss Value: 47.42746448516846\n","Time taken: 21.81550645828247\n","--------------------------------------------------------------------------------\n","Epoch: 331 Loss Value: 47.87764663696289\n","Time taken: 21.8613383769989\n","--------------------------------------------------------------------------------\n","Epoch: 332 Loss Value: 48.12198934555054\n","Time taken: 22.038129329681396\n","--------------------------------------------------------------------------------\n","Epoch: 333 Loss Value: 47.535582447052\n","Time taken: 22.00922679901123\n","--------------------------------------------------------------------------------\n","Epoch: 334 Loss Value: 47.45800649642944\n","Time taken: 22.32588291168213\n","--------------------------------------------------------------------------------\n","Epoch: 335 Loss Value: 47.80644380569458\n","Time taken: 22.092995643615723\n","--------------------------------------------------------------------------------\n","Epoch: 336 Loss Value: 47.52192461013794\n","Time taken: 22.212568998336792\n","--------------------------------------------------------------------------------\n","Epoch: 337 Loss Value: 46.686369972229\n","Time taken: 22.16768980026245\n","--------------------------------------------------------------------------------\n","Epoch: 338 Loss Value: 47.259866580963134\n","Time taken: 22.23030996322632\n","--------------------------------------------------------------------------------\n","Epoch: 339 Loss Value: 46.775293579101564\n","Time taken: 22.290024042129517\n","--------------------------------------------------------------------------------\n","Epoch: 340 Loss Value: 47.10800369262695\n","Time taken: 22.364240884780884\n","--------------------------------------------------------------------------------\n","Epoch: 341 Loss Value: 46.741594257354734\n","Time taken: 22.284044981002808\n","--------------------------------------------------------------------------------\n","Epoch: 342 Loss Value: 45.92657356262207\n","Time taken: 22.7721745967865\n","--------------------------------------------------------------------------------\n","Epoch: 343 Loss Value: 45.967283420562744\n","Time taken: 22.607131958007812\n","--------------------------------------------------------------------------------\n","Epoch: 344 Loss Value: 45.387662925720214\n","Time taken: 22.566298484802246\n","--------------------------------------------------------------------------------\n","Epoch: 345 Loss Value: 45.89104963302612\n","Time taken: 22.596933364868164\n","--------------------------------------------------------------------------------\n","Epoch: 346 Loss Value: 45.67949625015259\n","Time taken: 22.699581146240234\n","--------------------------------------------------------------------------------\n","Epoch: 347 Loss Value: 45.77329914093018\n","Time taken: 22.84700059890747\n","--------------------------------------------------------------------------------\n","Epoch: 348 Loss Value: 45.51932168960571\n","Time taken: 22.768924951553345\n","--------------------------------------------------------------------------------\n","Epoch: 349 Loss Value: 44.733524265289304\n","Time taken: 22.54165291786194\n","--------------------------------------------------------------------------------\n","Epoch: 350 Loss Value: 44.63589437484741\n","Time taken: 22.69192624092102\n","--------------------------------------------------------------------------------\n","Epoch: 351 Loss Value: 44.26291763305664\n","Time taken: 22.706637620925903\n","--------------------------------------------------------------------------------\n","Epoch: 352 Loss Value: 44.7569454574585\n","Time taken: 22.718286514282227\n","--------------------------------------------------------------------------------\n","Epoch: 353 Loss Value: 43.99554594039917\n","Time taken: 22.802294969558716\n","--------------------------------------------------------------------------------\n","Epoch: 354 Loss Value: 43.39570688247681\n","Time taken: 22.911005973815918\n","--------------------------------------------------------------------------------\n","Epoch: 355 Loss Value: 43.73205299377442\n","Time taken: 22.89087152481079\n","--------------------------------------------------------------------------------\n","Epoch: 356 Loss Value: 43.62130466461181\n","Time taken: 22.915756702423096\n","--------------------------------------------------------------------------------\n","Epoch: 357 Loss Value: 43.717737922668455\n","Time taken: 22.93770694732666\n","--------------------------------------------------------------------------------\n","Epoch: 358 Loss Value: 43.26476152420044\n","Time taken: 23.042051315307617\n","--------------------------------------------------------------------------------\n","Epoch: 359 Loss Value: 42.991256351470945\n","Time taken: 23.11144471168518\n","--------------------------------------------------------------------------------\n","Epoch: 360 Loss Value: 42.83871482849121\n","Time taken: 23.177343606948853\n","--------------------------------------------------------------------------------\n","Epoch: 361 Loss Value: 42.287540893554684\n","Time taken: 23.370973825454712\n","--------------------------------------------------------------------------------\n","Epoch: 362 Loss Value: 42.67537002563476\n","Time taken: 23.082855939865112\n","--------------------------------------------------------------------------------\n","Epoch: 363 Loss Value: 41.06448394775391\n","Time taken: 23.190369367599487\n","--------------------------------------------------------------------------------\n","Epoch: 364 Loss Value: 42.44270919799805\n","Time taken: 23.256428241729736\n","--------------------------------------------------------------------------------\n","Epoch: 365 Loss Value: 41.842441635131834\n","Time taken: 23.21516990661621\n","--------------------------------------------------------------------------------\n","Epoch: 366 Loss Value: 42.00846565246582\n","Time taken: 23.32034945487976\n","--------------------------------------------------------------------------------\n","Epoch: 367 Loss Value: 41.70370391845703\n","Time taken: 23.367924451828003\n","--------------------------------------------------------------------------------\n","Epoch: 368 Loss Value: 41.96480230331421\n","Time taken: 23.445725679397583\n","--------------------------------------------------------------------------------\n","Epoch: 369 Loss Value: 42.306716480255126\n","Time taken: 23.543365716934204\n","--------------------------------------------------------------------------------\n","Epoch: 370 Loss Value: 40.94677633285522\n","Time taken: 23.564804553985596\n","--------------------------------------------------------------------------------\n","Epoch: 371 Loss Value: 41.46702264785767\n","Time taken: 23.65869927406311\n","--------------------------------------------------------------------------------\n","Epoch: 372 Loss Value: 41.90108345031738\n","Time taken: 23.68640422821045\n","--------------------------------------------------------------------------------\n","Epoch: 373 Loss Value: 41.02981019973755\n","Time taken: 26.259812593460083\n","--------------------------------------------------------------------------------\n","Epoch: 374 Loss Value: 40.9575874710083\n","Time taken: 23.987024784088135\n","--------------------------------------------------------------------------------\n","Epoch: 375 Loss Value: 40.72392696380615\n","Time taken: 23.885765552520752\n","--------------------------------------------------------------------------------\n","Epoch: 376 Loss Value: 41.05417587280274\n","Time taken: 23.689619541168213\n","--------------------------------------------------------------------------------\n","Epoch: 377 Loss Value: 40.77540065765381\n","Time taken: 23.762245178222656\n","--------------------------------------------------------------------------------\n","Epoch: 378 Loss Value: 39.605735511779784\n","Time taken: 23.948652982711792\n","--------------------------------------------------------------------------------\n","Epoch: 379 Loss Value: 39.0480259513855\n","Time taken: 23.882887601852417\n","--------------------------------------------------------------------------------\n","Epoch: 380 Loss Value: 40.11558019638061\n","Time taken: 23.918325662612915\n","--------------------------------------------------------------------------------\n","Epoch: 381 Loss Value: 39.58868049621582\n","Time taken: 24.008903741836548\n","--------------------------------------------------------------------------------\n","Epoch: 382 Loss Value: 39.03388397216797\n","Time taken: 24.016351222991943\n","--------------------------------------------------------------------------------\n","Epoch: 383 Loss Value: 39.06891941070557\n","Time taken: 23.95716381072998\n","--------------------------------------------------------------------------------\n","Epoch: 384 Loss Value: 39.305167751312254\n","Time taken: 24.074090719223022\n","--------------------------------------------------------------------------------\n","Epoch: 385 Loss Value: 39.28677726745605\n","Time taken: 24.21060562133789\n","--------------------------------------------------------------------------------\n","Epoch: 386 Loss Value: 38.86625886917114\n","Time taken: 24.09618616104126\n","--------------------------------------------------------------------------------\n","Epoch: 387 Loss Value: 38.740299682617184\n","Time taken: 24.142192125320435\n","--------------------------------------------------------------------------------\n","Epoch: 388 Loss Value: 38.74265979766846\n","Time taken: 24.190030097961426\n","--------------------------------------------------------------------------------\n","Epoch: 389 Loss Value: 39.96688224792481\n","Time taken: 24.01181721687317\n","--------------------------------------------------------------------------------\n","Epoch: 390 Loss Value: 40.52419095993042\n","Time taken: 24.076358795166016\n","--------------------------------------------------------------------------------\n","Epoch: 391 Loss Value: 39.76191858291626\n","Time taken: 24.246615886688232\n","--------------------------------------------------------------------------------\n","Epoch: 392 Loss Value: 39.646347522735596\n","Time taken: 24.26769781112671\n","--------------------------------------------------------------------------------\n","Epoch: 393 Loss Value: 39.789425945281984\n","Time taken: 24.20010256767273\n","--------------------------------------------------------------------------------\n","Epoch: 394 Loss Value: 38.43335994720459\n","Time taken: 24.403658390045166\n","--------------------------------------------------------------------------------\n","Epoch: 395 Loss Value: 38.538951320648195\n","Time taken: 24.52522611618042\n","--------------------------------------------------------------------------------\n","Epoch: 396 Loss Value: 37.99611415863037\n","Time taken: 24.265960693359375\n","--------------------------------------------------------------------------------\n","Epoch: 397 Loss Value: 37.46489198684692\n","Time taken: 24.324256896972656\n","--------------------------------------------------------------------------------\n","Epoch: 398 Loss Value: 37.90742511749268\n","Time taken: 24.56995701789856\n","--------------------------------------------------------------------------------\n","Epoch: 399 Loss Value: 37.70940683364868\n","Time taken: 24.478996753692627\n","--------------------------------------------------------------------------------\n","Epoch: 400 Loss Value: 37.443784427642825\n","Time taken: 24.686797857284546\n","--------------------------------------------------------------------------------\n","Epoch: 401 Loss Value: 36.57950469970703\n","Time taken: 24.553444623947144\n","--------------------------------------------------------------------------------\n","Epoch: 402 Loss Value: 36.87330351829529\n","Time taken: 24.607548475265503\n","--------------------------------------------------------------------------------\n","Epoch: 403 Loss Value: 35.18227638244629\n","Time taken: 24.666255950927734\n","--------------------------------------------------------------------------------\n","Epoch: 404 Loss Value: 36.51424774169922\n","Time taken: 24.669811248779297\n","--------------------------------------------------------------------------------\n","Epoch: 405 Loss Value: 36.2672323513031\n","Time taken: 24.80319380760193\n","--------------------------------------------------------------------------------\n","Epoch: 406 Loss Value: 37.19281318664551\n","Time taken: 24.651365518569946\n","--------------------------------------------------------------------------------\n","Epoch: 407 Loss Value: 35.81693452835083\n","Time taken: 24.88715410232544\n","--------------------------------------------------------------------------------\n","Epoch: 408 Loss Value: 37.19003177642822\n","Time taken: 24.879001140594482\n","--------------------------------------------------------------------------------\n","Epoch: 409 Loss Value: 35.53968942642212\n","Time taken: 24.987308979034424\n","--------------------------------------------------------------------------------\n","Epoch: 410 Loss Value: 35.30628870010376\n","Time taken: 24.826322555541992\n","--------------------------------------------------------------------------------\n","Epoch: 411 Loss Value: 35.12426097869873\n","Time taken: 25.00295853614807\n","--------------------------------------------------------------------------------\n","Epoch: 412 Loss Value: 35.94683889389038\n","Time taken: 25.354226112365723\n","--------------------------------------------------------------------------------\n","Epoch: 413 Loss Value: 35.791140727996826\n","Time taken: 25.238161087036133\n","--------------------------------------------------------------------------------\n","Epoch: 414 Loss Value: 34.888393592834475\n","Time taken: 25.512546062469482\n","--------------------------------------------------------------------------------\n","Epoch: 415 Loss Value: 35.942570247650146\n","Time taken: 25.500149726867676\n","--------------------------------------------------------------------------------\n","Epoch: 416 Loss Value: 34.541812915802005\n","Time taken: 25.239497900009155\n","--------------------------------------------------------------------------------\n","Epoch: 417 Loss Value: 35.63820072174072\n","Time taken: 25.350172519683838\n","--------------------------------------------------------------------------------\n","Epoch: 418 Loss Value: 34.650119361877444\n","Time taken: 25.35841989517212\n","--------------------------------------------------------------------------------\n","Epoch: 419 Loss Value: 33.481776008605955\n","Time taken: 25.330742120742798\n","--------------------------------------------------------------------------------\n","Epoch: 420 Loss Value: 33.575677185058595\n","Time taken: 25.2977237701416\n","--------------------------------------------------------------------------------\n","Epoch: 421 Loss Value: 33.7905880355835\n","Time taken: 25.558741092681885\n","--------------------------------------------------------------------------------\n","Epoch: 422 Loss Value: 33.613186807632445\n","Time taken: 25.388423681259155\n","--------------------------------------------------------------------------------\n","Epoch: 423 Loss Value: 33.3652042388916\n","Time taken: 25.5057156085968\n","--------------------------------------------------------------------------------\n","Epoch: 424 Loss Value: 32.66475934028625\n","Time taken: 25.533226490020752\n","--------------------------------------------------------------------------------\n","Epoch: 425 Loss Value: 33.11819296836853\n","Time taken: 25.740121126174927\n","--------------------------------------------------------------------------------\n","Epoch: 426 Loss Value: 32.769290676116945\n","Time taken: 25.412429094314575\n","--------------------------------------------------------------------------------\n","Epoch: 427 Loss Value: 32.57118824005127\n","Time taken: 25.654704332351685\n","--------------------------------------------------------------------------------\n","Epoch: 428 Loss Value: 32.27497882843018\n","Time taken: 25.62008571624756\n","--------------------------------------------------------------------------------\n","Epoch: 429 Loss Value: 32.23889883041382\n","Time taken: 25.637366771697998\n","--------------------------------------------------------------------------------\n","Epoch: 430 Loss Value: 32.11897670745849\n","Time taken: 25.801883697509766\n","--------------------------------------------------------------------------------\n","Epoch: 431 Loss Value: 32.84188381195068\n","Time taken: 25.858205556869507\n","--------------------------------------------------------------------------------\n","Epoch: 432 Loss Value: 31.47354772567749\n","Time taken: 25.744914770126343\n","--------------------------------------------------------------------------------\n","Epoch: 433 Loss Value: 30.696747245788575\n","Time taken: 25.919906854629517\n","--------------------------------------------------------------------------------\n","Epoch: 434 Loss Value: 31.671107444763184\n","Time taken: 25.869579315185547\n","--------------------------------------------------------------------------------\n","Epoch: 435 Loss Value: 31.353186779022217\n","Time taken: 25.815097332000732\n","--------------------------------------------------------------------------------\n","Epoch: 436 Loss Value: 31.596825847625734\n","Time taken: 25.95477533340454\n","--------------------------------------------------------------------------------\n","Epoch: 437 Loss Value: 31.359712409973145\n","Time taken: 26.12966799736023\n","--------------------------------------------------------------------------------\n","Epoch: 438 Loss Value: 30.673216762542726\n","Time taken: 25.955315351486206\n","--------------------------------------------------------------------------------\n","Epoch: 439 Loss Value: 29.23115439414978\n","Time taken: 26.113359212875366\n","--------------------------------------------------------------------------------\n","Epoch: 440 Loss Value: 29.95682734489441\n","Time taken: 26.07273244857788\n","--------------------------------------------------------------------------------\n","Epoch: 441 Loss Value: 30.75178921699524\n","Time taken: 26.12185001373291\n","--------------------------------------------------------------------------------\n","Epoch: 442 Loss Value: 30.178097944259644\n","Time taken: 26.402956247329712\n","--------------------------------------------------------------------------------\n","Epoch: 443 Loss Value: 29.6734042930603\n","Time taken: 26.177159786224365\n","--------------------------------------------------------------------------------\n","Epoch: 444 Loss Value: 30.424242086410523\n","Time taken: 26.229215621948242\n","--------------------------------------------------------------------------------\n","Epoch: 445 Loss Value: 29.4367476272583\n","Time taken: 26.506664037704468\n","--------------------------------------------------------------------------------\n","Epoch: 446 Loss Value: 30.073762874603272\n","Time taken: 26.496411323547363\n","--------------------------------------------------------------------------------\n","Epoch: 447 Loss Value: 29.13780220031738\n","Time taken: 26.439191579818726\n","--------------------------------------------------------------------------------\n","Epoch: 448 Loss Value: 29.300623474121092\n","Time taken: 26.541390419006348\n","--------------------------------------------------------------------------------\n","Epoch: 449 Loss Value: 29.09402272224426\n","Time taken: 26.476152181625366\n","--------------------------------------------------------------------------------\n","Epoch: 450 Loss Value: 28.022805461883546\n","Time taken: 26.514052152633667\n","--------------------------------------------------------------------------------\n","Epoch: 451 Loss Value: 27.516675872802736\n","Time taken: 26.639774084091187\n","--------------------------------------------------------------------------------\n","Epoch: 452 Loss Value: 28.19970675468445\n","Time taken: 26.519030332565308\n","--------------------------------------------------------------------------------\n","Epoch: 453 Loss Value: 27.447181997299193\n","Time taken: 26.574471473693848\n","--------------------------------------------------------------------------------\n","Epoch: 454 Loss Value: 27.942773504257204\n","Time taken: 26.684460163116455\n","--------------------------------------------------------------------------------\n","Epoch: 455 Loss Value: 26.65962794303894\n","Time taken: 26.70463991165161\n","--------------------------------------------------------------------------------\n","Epoch: 456 Loss Value: 27.57062663078308\n","Time taken: 26.902371883392334\n","--------------------------------------------------------------------------------\n","Epoch: 457 Loss Value: 27.908397703170778\n","Time taken: 26.807358503341675\n","--------------------------------------------------------------------------------\n","Epoch: 458 Loss Value: 26.348949308395387\n","Time taken: 26.77318549156189\n","--------------------------------------------------------------------------------\n","Epoch: 459 Loss Value: 27.516294145584105\n","Time taken: 26.772802352905273\n","--------------------------------------------------------------------------------\n","Epoch: 460 Loss Value: 26.32357120513916\n","Time taken: 26.858040809631348\n","--------------------------------------------------------------------------------\n","Epoch: 461 Loss Value: 26.693529844284058\n","Time taken: 26.866997003555298\n","--------------------------------------------------------------------------------\n","Epoch: 462 Loss Value: 26.771912469863892\n","Time taken: 26.973951816558838\n","--------------------------------------------------------------------------------\n","Epoch: 463 Loss Value: 27.656076898574828\n","Time taken: 27.946473360061646\n","--------------------------------------------------------------------------------\n","Epoch: 464 Loss Value: 27.72150553703308\n","Time taken: 27.01565194129944\n","--------------------------------------------------------------------------------\n","Epoch: 465 Loss Value: 27.093897104263306\n","Time taken: 27.04022455215454\n","--------------------------------------------------------------------------------\n","Epoch: 466 Loss Value: 26.734444236755373\n","Time taken: 27.159804105758667\n","--------------------------------------------------------------------------------\n","Epoch: 467 Loss Value: 25.61376796722412\n","Time taken: 27.173338174819946\n","--------------------------------------------------------------------------------\n","Epoch: 468 Loss Value: 26.35639157295227\n","Time taken: 27.310688257217407\n","--------------------------------------------------------------------------------\n","Epoch: 469 Loss Value: 27.320238904953\n","Time taken: 27.25274634361267\n","--------------------------------------------------------------------------------\n","Epoch: 470 Loss Value: 26.70216197013855\n","Time taken: 30.348876476287842\n","--------------------------------------------------------------------------------\n","Epoch: 471 Loss Value: 26.923072605133058\n","Time taken: 27.17560601234436\n","--------------------------------------------------------------------------------\n","Epoch: 472 Loss Value: 26.520007152557373\n","Time taken: 27.316423892974854\n","--------------------------------------------------------------------------------\n","Epoch: 473 Loss Value: 26.321971015930174\n","Time taken: 27.318743228912354\n","--------------------------------------------------------------------------------\n","Epoch: 474 Loss Value: 26.376798038482665\n","Time taken: 27.297824382781982\n","--------------------------------------------------------------------------------\n","Epoch: 475 Loss Value: 26.604063434600832\n","Time taken: 27.484965801239014\n","--------------------------------------------------------------------------------\n","Epoch: 476 Loss Value: 26.023447504043578\n","Time taken: 27.505941152572632\n","--------------------------------------------------------------------------------\n","Epoch: 477 Loss Value: 26.521351680755615\n","Time taken: 27.433669805526733\n","--------------------------------------------------------------------------------\n","Epoch: 478 Loss Value: 26.187103385925294\n","Time taken: 27.636779069900513\n","--------------------------------------------------------------------------------\n","Epoch: 479 Loss Value: 26.10873508453369\n","Time taken: 27.67969274520874\n","--------------------------------------------------------------------------------\n","Epoch: 480 Loss Value: 26.016423597335816\n","Time taken: 27.556625843048096\n","--------------------------------------------------------------------------------\n","Epoch: 481 Loss Value: 26.07135066986084\n","Time taken: 27.696903705596924\n","--------------------------------------------------------------------------------\n","Epoch: 482 Loss Value: 25.93591121673584\n","Time taken: 27.60395359992981\n","--------------------------------------------------------------------------------\n","Epoch: 483 Loss Value: 25.012505416870116\n","Time taken: 27.672969341278076\n","--------------------------------------------------------------------------------\n","Epoch: 484 Loss Value: 25.08616653442383\n","Time taken: 27.85277557373047\n","--------------------------------------------------------------------------------\n","Epoch: 485 Loss Value: 24.731551399230955\n","Time taken: 27.71605920791626\n","--------------------------------------------------------------------------------\n","Epoch: 486 Loss Value: 24.625571088790892\n","Time taken: 27.815139055252075\n","--------------------------------------------------------------------------------\n","Epoch: 487 Loss Value: 24.375969028472902\n","Time taken: 27.831092834472656\n","--------------------------------------------------------------------------------\n","Epoch: 488 Loss Value: 23.980086975097656\n","Time taken: 27.71684765815735\n","--------------------------------------------------------------------------------\n","Epoch: 489 Loss Value: 24.115962228775025\n","Time taken: 27.956183671951294\n","--------------------------------------------------------------------------------\n","Epoch: 490 Loss Value: 24.34600735664368\n","Time taken: 28.227633953094482\n","--------------------------------------------------------------------------------\n","Epoch: 491 Loss Value: 24.336368713378906\n","Time taken: 27.890772819519043\n","--------------------------------------------------------------------------------\n","Epoch: 492 Loss Value: 24.2702224445343\n","Time taken: 27.940573692321777\n","--------------------------------------------------------------------------------\n","Epoch: 493 Loss Value: 24.289549379348756\n","Time taken: 28.078463315963745\n","--------------------------------------------------------------------------------\n","Epoch: 494 Loss Value: 23.30141046524048\n","Time taken: 27.957220792770386\n","--------------------------------------------------------------------------------\n","Epoch: 495 Loss Value: 23.617603759765625\n","Time taken: 28.112273931503296\n","--------------------------------------------------------------------------------\n","Epoch: 496 Loss Value: 22.828416891098023\n","Time taken: 28.13706064224243\n","--------------------------------------------------------------------------------\n","Epoch: 497 Loss Value: 23.0423145198822\n","Time taken: 28.13593077659607\n","--------------------------------------------------------------------------------\n","Epoch: 498 Loss Value: 23.6488742685318\n","Time taken: 28.18308997154236\n","--------------------------------------------------------------------------------\n","Epoch: 499 Loss Value: 23.437960815429687\n","Time taken: 28.249276638031006\n","--------------------------------------------------------------------------------\n","Epoch: 500 Loss Value: 22.415357971191405\n","Time taken: 28.25693917274475\n","--------------------------------------------------------------------------------\n","Epoch: 501 Loss Value: 23.565480952262877\n","Time taken: 28.556060552597046\n","--------------------------------------------------------------------------------\n","Epoch: 502 Loss Value: 22.51422024726868\n","Time taken: 28.36500835418701\n","--------------------------------------------------------------------------------\n","Epoch: 503 Loss Value: 23.081286396980286\n","Time taken: 28.441692352294922\n","--------------------------------------------------------------------------------\n","Epoch: 504 Loss Value: 22.195635595321654\n","Time taken: 28.470335245132446\n","--------------------------------------------------------------------------------\n","Epoch: 505 Loss Value: 22.09933645248413\n","Time taken: 28.47103261947632\n","--------------------------------------------------------------------------------\n","Epoch: 506 Loss Value: 21.90108124256134\n","Time taken: 28.486052989959717\n","--------------------------------------------------------------------------------\n","Epoch: 507 Loss Value: 22.313804144859315\n","Time taken: 28.70796537399292\n","--------------------------------------------------------------------------------\n","Epoch: 508 Loss Value: 21.934919843673708\n","Time taken: 28.665813207626343\n","--------------------------------------------------------------------------------\n","Epoch: 509 Loss Value: 22.39164423942566\n","Time taken: 28.581722497940063\n","--------------------------------------------------------------------------------\n","Epoch: 510 Loss Value: 21.11339497089386\n","Time taken: 28.653411865234375\n","--------------------------------------------------------------------------------\n","Epoch: 511 Loss Value: 22.46873881816864\n","Time taken: 28.630228281021118\n","--------------------------------------------------------------------------------\n","Epoch: 512 Loss Value: 21.360815629959106\n","Time taken: 28.910022258758545\n","--------------------------------------------------------------------------------\n","Epoch: 513 Loss Value: 21.309947710037232\n","Time taken: 28.726268529891968\n","--------------------------------------------------------------------------------\n","Epoch: 514 Loss Value: 21.86977415561676\n","Time taken: 28.803980350494385\n","--------------------------------------------------------------------------------\n","Epoch: 515 Loss Value: 20.723694162368773\n","Time taken: 28.779223918914795\n","--------------------------------------------------------------------------------\n","Epoch: 516 Loss Value: 20.154291381835936\n","Time taken: 28.943302154541016\n","--------------------------------------------------------------------------------\n","Epoch: 517 Loss Value: 21.3831515789032\n","Time taken: 28.963945865631104\n","--------------------------------------------------------------------------------\n","Epoch: 518 Loss Value: 19.748537340164184\n","Time taken: 29.05738353729248\n","--------------------------------------------------------------------------------\n","Epoch: 519 Loss Value: 20.771488761901857\n","Time taken: 28.87006378173828\n","--------------------------------------------------------------------------------\n","Epoch: 520 Loss Value: 21.149238996505737\n","Time taken: 29.019634008407593\n","--------------------------------------------------------------------------------\n","Epoch: 521 Loss Value: 20.86155327320099\n","Time taken: 29.21883988380432\n","--------------------------------------------------------------------------------\n","Epoch: 522 Loss Value: 21.65571969985962\n","Time taken: 29.31010675430298\n","--------------------------------------------------------------------------------\n","Epoch: 523 Loss Value: 21.430028228759767\n","Time taken: 29.18166160583496\n","--------------------------------------------------------------------------------\n","Epoch: 524 Loss Value: 21.28952756881714\n","Time taken: 28.970869779586792\n","--------------------------------------------------------------------------------\n","Epoch: 525 Loss Value: 19.97343832015991\n","Time taken: 28.692253589630127\n","--------------------------------------------------------------------------------\n","Epoch: 526 Loss Value: 21.375423412323\n","Time taken: 28.900574207305908\n","--------------------------------------------------------------------------------\n","Epoch: 527 Loss Value: 19.729183769226076\n","Time taken: 28.96042776107788\n","--------------------------------------------------------------------------------\n","Epoch: 528 Loss Value: 20.393733973503114\n","Time taken: 28.994384050369263\n","--------------------------------------------------------------------------------\n","Epoch: 529 Loss Value: 18.585079126358032\n","Time taken: 29.18366026878357\n","--------------------------------------------------------------------------------\n","Epoch: 530 Loss Value: 19.73128499507904\n","Time taken: 29.227445125579834\n","--------------------------------------------------------------------------------\n","Epoch: 531 Loss Value: 20.441195607185364\n","Time taken: 29.271941661834717\n","--------------------------------------------------------------------------------\n","Epoch: 532 Loss Value: 19.812473587989807\n","Time taken: 29.443743228912354\n","--------------------------------------------------------------------------------\n","Epoch: 533 Loss Value: 19.299315633773805\n","Time taken: 29.463595628738403\n","--------------------------------------------------------------------------------\n","Epoch: 534 Loss Value: 19.88489122390747\n","Time taken: 29.245317697525024\n","--------------------------------------------------------------------------------\n","Epoch: 535 Loss Value: 19.072655577659607\n","Time taken: 29.44795536994934\n","--------------------------------------------------------------------------------\n","Epoch: 536 Loss Value: 19.17237087249756\n","Time taken: 29.511298179626465\n","--------------------------------------------------------------------------------\n","Epoch: 537 Loss Value: 19.074310488700867\n","Time taken: 29.60534143447876\n","--------------------------------------------------------------------------------\n","Epoch: 538 Loss Value: 19.58841677188873\n","Time taken: 29.599895238876343\n","--------------------------------------------------------------------------------\n","Epoch: 539 Loss Value: 18.41958394050598\n","Time taken: 29.622950077056885\n","--------------------------------------------------------------------------------\n","Epoch: 540 Loss Value: 19.425619587898254\n","Time taken: 29.781159162521362\n","--------------------------------------------------------------------------------\n","Epoch: 541 Loss Value: 18.995214412212373\n","Time taken: 29.716938734054565\n","--------------------------------------------------------------------------------\n","Epoch: 542 Loss Value: 18.482792072296142\n","Time taken: 29.826745986938477\n","--------------------------------------------------------------------------------\n","Epoch: 543 Loss Value: 17.13815716743469\n","Time taken: 29.9296772480011\n","--------------------------------------------------------------------------------\n","Epoch: 544 Loss Value: 18.241408426761627\n","Time taken: 29.984023571014404\n","--------------------------------------------------------------------------------\n","Epoch: 545 Loss Value: 18.22658239841461\n","Time taken: 29.973516702651978\n","--------------------------------------------------------------------------------\n","Epoch: 546 Loss Value: 18.613476243019104\n","Time taken: 29.883893966674805\n","--------------------------------------------------------------------------------\n","Epoch: 547 Loss Value: 17.801945004463196\n","Time taken: 29.650057077407837\n","--------------------------------------------------------------------------------\n","Epoch: 548 Loss Value: 18.260236735343934\n","Time taken: 29.849695444107056\n","--------------------------------------------------------------------------------\n","Epoch: 549 Loss Value: 17.544615893363954\n","Time taken: 29.943929195404053\n","--------------------------------------------------------------------------------\n","Epoch: 550 Loss Value: 18.12873143196106\n","Time taken: 29.938541650772095\n","--------------------------------------------------------------------------------\n","Epoch: 551 Loss Value: 17.548257904052733\n","Time taken: 30.1318576335907\n","--------------------------------------------------------------------------------\n","Epoch: 552 Loss Value: 17.930451760292055\n","Time taken: 30.01340103149414\n","--------------------------------------------------------------------------------\n","Epoch: 553 Loss Value: 17.59640061378479\n","Time taken: 30.293601989746094\n","--------------------------------------------------------------------------------\n","Epoch: 554 Loss Value: 18.67453305244446\n","Time taken: 30.53820013999939\n","--------------------------------------------------------------------------------\n","Epoch: 555 Loss Value: 17.720765280723572\n","Time taken: 30.211706399917603\n","--------------------------------------------------------------------------------\n","Epoch: 556 Loss Value: 17.764832525253297\n","Time taken: 30.247623682022095\n","--------------------------------------------------------------------------------\n","Epoch: 557 Loss Value: 18.47710814476013\n","Time taken: 30.274376153945923\n","--------------------------------------------------------------------------------\n","Epoch: 558 Loss Value: 19.431339192390443\n","Time taken: 30.454347133636475\n","--------------------------------------------------------------------------------\n","Epoch: 559 Loss Value: 18.455473432540895\n","Time taken: 30.7081561088562\n","--------------------------------------------------------------------------------\n","Epoch: 560 Loss Value: 17.49768684864044\n","Time taken: 30.36546277999878\n","--------------------------------------------------------------------------------\n","Epoch: 561 Loss Value: 17.42079156398773\n","Time taken: 30.501244068145752\n","--------------------------------------------------------------------------------\n","Epoch: 562 Loss Value: 17.908147387504577\n","Time taken: 30.60003972053528\n","--------------------------------------------------------------------------------\n","Epoch: 563 Loss Value: 16.77716416835785\n","Time taken: 30.79753613471985\n","--------------------------------------------------------------------------------\n","Epoch: 564 Loss Value: 17.710067286491395\n","Time taken: 30.996623277664185\n","--------------------------------------------------------------------------------\n","Epoch: 565 Loss Value: 16.898347201347352\n","Time taken: 30.814824104309082\n","--------------------------------------------------------------------------------\n","Epoch: 566 Loss Value: 17.336285240650177\n","Time taken: 30.82998776435852\n","--------------------------------------------------------------------------------\n","Epoch: 567 Loss Value: 18.069799799919128\n","Time taken: 30.924055576324463\n","--------------------------------------------------------------------------------\n","Epoch: 568 Loss Value: 16.38806849002838\n","Time taken: 30.982463598251343\n","--------------------------------------------------------------------------------\n","Epoch: 569 Loss Value: 15.84454975605011\n","Time taken: 30.908759355545044\n","--------------------------------------------------------------------------------\n","Epoch: 570 Loss Value: 17.35645938873291\n","Time taken: 30.97613024711609\n","--------------------------------------------------------------------------------\n","Epoch: 571 Loss Value: 16.60976351737976\n","Time taken: 31.07865071296692\n","--------------------------------------------------------------------------------\n","Epoch: 572 Loss Value: 16.174857816696168\n","Time taken: 31.143015384674072\n","--------------------------------------------------------------------------------\n","Epoch: 573 Loss Value: 16.858620495796202\n","Time taken: 31.130458116531372\n","--------------------------------------------------------------------------------\n","Epoch: 574 Loss Value: 15.837380290031433\n","Time taken: 31.275787591934204\n","--------------------------------------------------------------------------------\n","Epoch: 575 Loss Value: 16.29943660020828\n","Time taken: 31.18070340156555\n","--------------------------------------------------------------------------------\n","Epoch: 576 Loss Value: 16.846314792633056\n","Time taken: 31.218467235565186\n","--------------------------------------------------------------------------------\n","Epoch: 577 Loss Value: 16.486295387744903\n","Time taken: 31.32126545906067\n","--------------------------------------------------------------------------------\n","Epoch: 578 Loss Value: 16.337096061706543\n","Time taken: 31.296304941177368\n","--------------------------------------------------------------------------------\n","Epoch: 579 Loss Value: 16.655981287956237\n","Time taken: 31.30433487892151\n","--------------------------------------------------------------------------------\n","Epoch: 580 Loss Value: 15.489729664325715\n","Time taken: 31.404728174209595\n","--------------------------------------------------------------------------------\n","Epoch: 581 Loss Value: 15.658678121566773\n","Time taken: 31.37128734588623\n","--------------------------------------------------------------------------------\n","Epoch: 582 Loss Value: 16.73781171798706\n","Time taken: 31.52325701713562\n","--------------------------------------------------------------------------------\n","Epoch: 583 Loss Value: 15.909385981559753\n","Time taken: 31.362590789794922\n","--------------------------------------------------------------------------------\n","Epoch: 584 Loss Value: 16.17214901447296\n","Time taken: 31.67556619644165\n","--------------------------------------------------------------------------------\n","Epoch: 585 Loss Value: 16.723764686584474\n","Time taken: 31.537684202194214\n","--------------------------------------------------------------------------------\n","Epoch: 586 Loss Value: 15.560301256179809\n","Time taken: 31.484208345413208\n","--------------------------------------------------------------------------------\n","Epoch: 587 Loss Value: 16.24052581310272\n","Time taken: 31.482139348983765\n","--------------------------------------------------------------------------------\n","Epoch: 588 Loss Value: 15.5395996427536\n","Time taken: 31.74776315689087\n","--------------------------------------------------------------------------------\n","Epoch: 589 Loss Value: 15.731187901496888\n","Time taken: 31.517311334609985\n","--------------------------------------------------------------------------------\n","Epoch: 590 Loss Value: 16.210498147010803\n","Time taken: 31.787023067474365\n","--------------------------------------------------------------------------------\n","Epoch: 591 Loss Value: 14.91749969959259\n","Time taken: 35.71311902999878\n","--------------------------------------------------------------------------------\n","Epoch: 592 Loss Value: 14.672715287208558\n","Time taken: 31.68902587890625\n","--------------------------------------------------------------------------------\n","Epoch: 593 Loss Value: 15.022572755813599\n","Time taken: 31.766385793685913\n","--------------------------------------------------------------------------------\n","Epoch: 594 Loss Value: 15.797229161262512\n","Time taken: 31.95246171951294\n","--------------------------------------------------------------------------------\n","Epoch: 595 Loss Value: 14.73149067401886\n","Time taken: 31.979027271270752\n","--------------------------------------------------------------------------------\n","Epoch: 596 Loss Value: 15.326771366596223\n","Time taken: 32.46171736717224\n","--------------------------------------------------------------------------------\n","Epoch: 597 Loss Value: 14.871577739715576\n","Time taken: 32.4393470287323\n","--------------------------------------------------------------------------------\n","Epoch: 598 Loss Value: 14.160144016742706\n","Time taken: 32.156100273132324\n","--------------------------------------------------------------------------------\n","Epoch: 599 Loss Value: 14.423486106395721\n","Time taken: 32.16825342178345\n","--------------------------------------------------------------------------------\n","Epoch: 600 Loss Value: 14.740418560504914\n","Time taken: 32.235036849975586\n","--------------------------------------------------------------------------------\n","Epoch: 601 Loss Value: 14.441266477108002\n","Time taken: 32.37025594711304\n","--------------------------------------------------------------------------------\n","Epoch: 602 Loss Value: 14.438706603050232\n","Time taken: 32.21192812919617\n","--------------------------------------------------------------------------------\n","Epoch: 603 Loss Value: 14.784676060676574\n","Time taken: 32.24312663078308\n","--------------------------------------------------------------------------------\n","Epoch: 604 Loss Value: 13.709569594860078\n","Time taken: 32.70755076408386\n","--------------------------------------------------------------------------------\n","Epoch: 605 Loss Value: 14.219784417152404\n","Time taken: 32.50953388214111\n","--------------------------------------------------------------------------------\n","Epoch: 606 Loss Value: 14.03628829240799\n","Time taken: 32.54527568817139\n","--------------------------------------------------------------------------------\n","Epoch: 607 Loss Value: 14.218069987297058\n","Time taken: 32.544819593429565\n","--------------------------------------------------------------------------------\n","Epoch: 608 Loss Value: 13.220330035686493\n","Time taken: 32.44852042198181\n","--------------------------------------------------------------------------------\n","Epoch: 609 Loss Value: 13.603698446750641\n","Time taken: 32.593918561935425\n","--------------------------------------------------------------------------------\n","Epoch: 610 Loss Value: 13.714760320186615\n","Time taken: 32.64857769012451\n","--------------------------------------------------------------------------------\n","Epoch: 611 Loss Value: 13.734708666801453\n","Time taken: 32.773276805877686\n","--------------------------------------------------------------------------------\n","Epoch: 612 Loss Value: 12.74115961074829\n","Time taken: 32.88147783279419\n","--------------------------------------------------------------------------------\n","Epoch: 613 Loss Value: 14.099986996650696\n","Time taken: 33.08427119255066\n","--------------------------------------------------------------------------------\n","Epoch: 614 Loss Value: 13.864430694580077\n","Time taken: 32.828606367111206\n","--------------------------------------------------------------------------------\n","Epoch: 615 Loss Value: 13.105136842727662\n","Time taken: 32.917344093322754\n","--------------------------------------------------------------------------------\n","Epoch: 616 Loss Value: 12.698189764022827\n","Time taken: 32.958051443099976\n","--------------------------------------------------------------------------------\n","Epoch: 617 Loss Value: 12.907808487415315\n","Time taken: 33.02862238883972\n","--------------------------------------------------------------------------------\n","Epoch: 618 Loss Value: 12.209068419933319\n","Time taken: 33.51343131065369\n","--------------------------------------------------------------------------------\n","Epoch: 619 Loss Value: 13.473372704982758\n","Time taken: 33.531923055648804\n","--------------------------------------------------------------------------------\n","Epoch: 620 Loss Value: 13.145911202430725\n","Time taken: 33.41477656364441\n","--------------------------------------------------------------------------------\n","Epoch: 621 Loss Value: 12.998813798427582\n","Time taken: 33.23145890235901\n","--------------------------------------------------------------------------------\n","Epoch: 622 Loss Value: 13.101948068141937\n","Time taken: 33.51288104057312\n","--------------------------------------------------------------------------------\n","Epoch: 623 Loss Value: 12.79723735332489\n","Time taken: 33.29472088813782\n","--------------------------------------------------------------------------------\n","Epoch: 624 Loss Value: 12.875025951862336\n","Time taken: 33.41829538345337\n","--------------------------------------------------------------------------------\n","Epoch: 625 Loss Value: 12.951247713565827\n","Time taken: 33.4433171749115\n","--------------------------------------------------------------------------------\n","Epoch: 626 Loss Value: 13.383615273237229\n","Time taken: 33.28664779663086\n","--------------------------------------------------------------------------------\n","Epoch: 627 Loss Value: 12.833367948532105\n","Time taken: 33.45734786987305\n","--------------------------------------------------------------------------------\n","Epoch: 628 Loss Value: 12.60057604789734\n","Time taken: 33.33361220359802\n","--------------------------------------------------------------------------------\n","Epoch: 629 Loss Value: 12.797903089523315\n","Time taken: 33.395501375198364\n","--------------------------------------------------------------------------------\n","Epoch: 630 Loss Value: 12.221615676879884\n","Time taken: 33.323683738708496\n","--------------------------------------------------------------------------------\n","Epoch: 631 Loss Value: 12.234570682048798\n","Time taken: 33.32523536682129\n","--------------------------------------------------------------------------------\n","Epoch: 632 Loss Value: 12.34921148777008\n","Time taken: 33.55377197265625\n","--------------------------------------------------------------------------------\n","Epoch: 633 Loss Value: 12.296715025901795\n","Time taken: 33.2624146938324\n","--------------------------------------------------------------------------------\n","Epoch: 634 Loss Value: 12.410131211280822\n","Time taken: 33.27272009849548\n","--------------------------------------------------------------------------------\n","Epoch: 635 Loss Value: 11.793434491157532\n","Time taken: 33.22425055503845\n","--------------------------------------------------------------------------------\n","Epoch: 636 Loss Value: 12.587178795337676\n","Time taken: 33.21060347557068\n","--------------------------------------------------------------------------------\n","Epoch: 637 Loss Value: 12.449180328845978\n","Time taken: 33.39560103416443\n","--------------------------------------------------------------------------------\n","Epoch: 638 Loss Value: 12.562506575584411\n","Time taken: 33.47054982185364\n","--------------------------------------------------------------------------------\n","Epoch: 639 Loss Value: 12.19491000175476\n","Time taken: 33.322683811187744\n","--------------------------------------------------------------------------------\n","Epoch: 640 Loss Value: 12.341003015041352\n","Time taken: 33.407480001449585\n","--------------------------------------------------------------------------------\n","Epoch: 641 Loss Value: 12.06658418416977\n","Time taken: 33.66557955741882\n","--------------------------------------------------------------------------------\n","Epoch: 642 Loss Value: 11.764821140766143\n","Time taken: 33.54503607749939\n","--------------------------------------------------------------------------------\n","Epoch: 643 Loss Value: 11.076361470222473\n","Time taken: 33.56985068321228\n","--------------------------------------------------------------------------------\n","Epoch: 644 Loss Value: 12.584553966522217\n","Time taken: 33.732422828674316\n","--------------------------------------------------------------------------------\n","Epoch: 645 Loss Value: 12.435227298736573\n","Time taken: 33.63302826881409\n","--------------------------------------------------------------------------------\n","Epoch: 646 Loss Value: 13.012779211997985\n","Time taken: 33.630967140197754\n","--------------------------------------------------------------------------------\n","Epoch: 647 Loss Value: 11.875155291557313\n","Time taken: 33.688663482666016\n","--------------------------------------------------------------------------------\n","Epoch: 648 Loss Value: 11.59209457397461\n","Time taken: 33.533881187438965\n","--------------------------------------------------------------------------------\n","Epoch: 649 Loss Value: 11.397059850692749\n","Time taken: 33.72399067878723\n","--------------------------------------------------------------------------------\n","Epoch: 650 Loss Value: 11.818806011676788\n","Time taken: 33.73491930961609\n","--------------------------------------------------------------------------------\n","Epoch: 651 Loss Value: 12.225219237804414\n","Time taken: 33.97813963890076\n","--------------------------------------------------------------------------------\n","Epoch: 652 Loss Value: 12.086523458957672\n","Time taken: 33.71694564819336\n","--------------------------------------------------------------------------------\n","Epoch: 653 Loss Value: 11.052499747276306\n","Time taken: 34.042093992233276\n","--------------------------------------------------------------------------------\n","Epoch: 654 Loss Value: 11.15125986456871\n","Time taken: 33.977285861968994\n","--------------------------------------------------------------------------------\n","Epoch: 655 Loss Value: 11.352440861463547\n","Time taken: 33.97768592834473\n","--------------------------------------------------------------------------------\n","Epoch: 656 Loss Value: 12.018323432207108\n","Time taken: 34.243152379989624\n","--------------------------------------------------------------------------------\n","Epoch: 657 Loss Value: 11.668001012802124\n","Time taken: 34.061103105545044\n","--------------------------------------------------------------------------------\n","Epoch: 658 Loss Value: 12.084276354312896\n","Time taken: 34.16403293609619\n","--------------------------------------------------------------------------------\n","Epoch: 659 Loss Value: 11.455924546718597\n","Time taken: 34.387829065322876\n","--------------------------------------------------------------------------------\n","Epoch: 660 Loss Value: 11.290561850070953\n","Time taken: 34.14521026611328\n","--------------------------------------------------------------------------------\n","Epoch: 661 Loss Value: 11.447216436862945\n","Time taken: 34.05468440055847\n","--------------------------------------------------------------------------------\n","Epoch: 662 Loss Value: 11.262369863986969\n","Time taken: 33.88287019729614\n","--------------------------------------------------------------------------------\n","Epoch: 663 Loss Value: 12.060438401699066\n","Time taken: 34.14745378494263\n","--------------------------------------------------------------------------------\n","Epoch: 664 Loss Value: 11.115342953205108\n","Time taken: 34.170727252960205\n","--------------------------------------------------------------------------------\n","Epoch: 665 Loss Value: 11.366737649440765\n","Time taken: 34.25095558166504\n","--------------------------------------------------------------------------------\n","Epoch: 666 Loss Value: 10.65868302822113\n","Time taken: 34.076181173324585\n","--------------------------------------------------------------------------------\n","Epoch: 667 Loss Value: 10.972375872135162\n","Time taken: 34.174434661865234\n","--------------------------------------------------------------------------------\n","Epoch: 668 Loss Value: 10.81615371465683\n","Time taken: 34.35733699798584\n","--------------------------------------------------------------------------------\n","Epoch: 669 Loss Value: 11.374024978876115\n","Time taken: 34.42085909843445\n","--------------------------------------------------------------------------------\n","Epoch: 670 Loss Value: 11.11698384284973\n","Time taken: 34.38671588897705\n","--------------------------------------------------------------------------------\n","Epoch: 671 Loss Value: 11.155119252204894\n","Time taken: 34.33133935928345\n","--------------------------------------------------------------------------------\n","Epoch: 672 Loss Value: 10.972469391822814\n","Time taken: 34.61764645576477\n","--------------------------------------------------------------------------------\n","Epoch: 673 Loss Value: 10.784501658678055\n","Time taken: 34.43220257759094\n","--------------------------------------------------------------------------------\n","Epoch: 674 Loss Value: 11.326936435699462\n","Time taken: 34.60551452636719\n","--------------------------------------------------------------------------------\n","Epoch: 675 Loss Value: 10.604897822141647\n","Time taken: 34.58466339111328\n","--------------------------------------------------------------------------------\n","Epoch: 676 Loss Value: 10.49995707988739\n","Time taken: 34.67301845550537\n","--------------------------------------------------------------------------------\n","Epoch: 677 Loss Value: 10.442020674943924\n","Time taken: 34.745280742645264\n","--------------------------------------------------------------------------------\n","Epoch: 678 Loss Value: 10.832172856330871\n","Time taken: 34.788702726364136\n","--------------------------------------------------------------------------------\n","Epoch: 679 Loss Value: 10.82057251214981\n","Time taken: 34.82633686065674\n","--------------------------------------------------------------------------------\n","Epoch: 680 Loss Value: 10.696742758750915\n","Time taken: 34.77476930618286\n","--------------------------------------------------------------------------------\n","Epoch: 681 Loss Value: 10.76923714876175\n","Time taken: 34.96196269989014\n","--------------------------------------------------------------------------------\n","Epoch: 682 Loss Value: 10.496499774456025\n","Time taken: 35.045114040374756\n","--------------------------------------------------------------------------------\n","Epoch: 683 Loss Value: 10.400117864608765\n","Time taken: 35.13172149658203\n","--------------------------------------------------------------------------------\n","Epoch: 684 Loss Value: 10.703530054092408\n","Time taken: 35.190773487091064\n","--------------------------------------------------------------------------------\n","Epoch: 685 Loss Value: 10.651609136462211\n","Time taken: 34.965173959732056\n","--------------------------------------------------------------------------------\n","Epoch: 686 Loss Value: 10.207252538204193\n","Time taken: 35.46274399757385\n","--------------------------------------------------------------------------------\n","Epoch: 687 Loss Value: 10.725503726005554\n","Time taken: 35.23375749588013\n","--------------------------------------------------------------------------------\n","Epoch: 688 Loss Value: 9.474861664772034\n","Time taken: 35.039604902267456\n","--------------------------------------------------------------------------------\n","Epoch: 689 Loss Value: 10.548392590880393\n","Time taken: 35.07022547721863\n","--------------------------------------------------------------------------------\n","Epoch: 690 Loss Value: 10.34655750155449\n","Time taken: 35.08254837989807\n","--------------------------------------------------------------------------------\n","Epoch: 691 Loss Value: 10.32484057545662\n","Time taken: 35.48779845237732\n","--------------------------------------------------------------------------------\n","Epoch: 692 Loss Value: 10.053741858005523\n","Time taken: 35.31089186668396\n","--------------------------------------------------------------------------------\n","Epoch: 693 Loss Value: 9.884890978336335\n","Time taken: 35.70385122299194\n","--------------------------------------------------------------------------------\n","Epoch: 694 Loss Value: 10.472919049263\n","Time taken: 35.9675178527832\n","--------------------------------------------------------------------------------\n","Epoch: 695 Loss Value: 10.438519477844238\n","Time taken: 36.50117349624634\n","--------------------------------------------------------------------------------\n","Epoch: 696 Loss Value: 10.406220388412475\n","Time taken: 36.74813985824585\n","--------------------------------------------------------------------------------\n","Epoch: 697 Loss Value: 9.827148762941361\n","Time taken: 36.5466570854187\n","--------------------------------------------------------------------------------\n","Epoch: 698 Loss Value: 10.762885811328887\n","Time taken: 36.64903545379639\n","--------------------------------------------------------------------------------\n","Epoch: 699 Loss Value: 10.793119398355485\n","Time taken: 36.66001749038696\n","--------------------------------------------------------------------------------\n","Epoch: 700 Loss Value: 9.998791654109954\n","Time taken: 36.830573081970215\n","--------------------------------------------------------------------------------\n","Epoch: 701 Loss Value: 10.762773940563202\n","Time taken: 36.81531882286072\n","--------------------------------------------------------------------------------\n","Epoch: 702 Loss Value: 9.968116629123688\n","Time taken: 36.80859923362732\n","--------------------------------------------------------------------------------\n","Epoch: 703 Loss Value: 9.382685556411744\n","Time taken: 36.834415435791016\n","--------------------------------------------------------------------------------\n","Epoch: 704 Loss Value: 10.494725871086121\n","Time taken: 37.7194185256958\n","--------------------------------------------------------------------------------\n","Epoch: 705 Loss Value: 9.917547130584717\n","Time taken: 36.93497681617737\n","--------------------------------------------------------------------------------\n","Epoch: 706 Loss Value: 9.285421978235245\n","Time taken: 37.059693813323975\n","--------------------------------------------------------------------------------\n","Epoch: 707 Loss Value: 9.71335124850273\n","Time taken: 37.071619272232056\n","--------------------------------------------------------------------------------\n","Epoch: 708 Loss Value: 9.277242097854614\n","Time taken: 37.30768632888794\n","--------------------------------------------------------------------------------\n","Epoch: 709 Loss Value: 9.315152604579925\n","Time taken: 37.23094129562378\n","--------------------------------------------------------------------------------\n","Epoch: 710 Loss Value: 9.11206843972206\n","Time taken: 37.232086181640625\n","--------------------------------------------------------------------------------\n","Epoch: 711 Loss Value: 9.727543888092042\n","Time taken: 37.4387993812561\n","--------------------------------------------------------------------------------\n","Epoch: 712 Loss Value: 9.051080481410027\n","Time taken: 37.64307999610901\n","--------------------------------------------------------------------------------\n","Epoch: 713 Loss Value: 9.933058669567108\n","Time taken: 37.68258738517761\n","--------------------------------------------------------------------------------\n","Epoch: 714 Loss Value: 9.449044324159622\n","Time taken: 37.67528009414673\n","--------------------------------------------------------------------------------\n","Epoch: 715 Loss Value: 8.887849732637406\n","Time taken: 37.64496326446533\n","--------------------------------------------------------------------------------\n","Epoch: 716 Loss Value: 10.002537088394165\n","Time taken: 37.77743196487427\n","--------------------------------------------------------------------------------\n","Epoch: 717 Loss Value: 9.800147397518158\n","Time taken: 37.82526779174805\n","--------------------------------------------------------------------------------\n","Epoch: 718 Loss Value: 9.8593017578125\n","Time taken: 37.65644550323486\n","--------------------------------------------------------------------------------\n","Epoch: 719 Loss Value: 8.838594592809677\n","Time taken: 37.7537887096405\n","--------------------------------------------------------------------------------\n","Epoch: 720 Loss Value: 8.888639279603957\n","Time taken: 37.89369606971741\n","--------------------------------------------------------------------------------\n","Epoch: 721 Loss Value: 9.872448178529739\n","Time taken: 37.81140851974487\n","--------------------------------------------------------------------------------\n","Epoch: 722 Loss Value: 9.32157740831375\n","Time taken: 37.792086362838745\n","--------------------------------------------------------------------------------\n","Epoch: 723 Loss Value: 8.805421382188797\n","Time taken: 37.772499561309814\n","--------------------------------------------------------------------------------\n","Epoch: 724 Loss Value: 9.099397058486938\n","Time taken: 37.83072018623352\n","--------------------------------------------------------------------------------\n","Epoch: 725 Loss Value: 8.343631792068482\n","Time taken: 37.76681327819824\n","--------------------------------------------------------------------------------\n","Epoch: 726 Loss Value: 8.976178410053253\n","Time taken: 37.78986382484436\n","--------------------------------------------------------------------------------\n","Epoch: 727 Loss Value: 8.761138100624084\n","Time taken: 38.25399923324585\n","--------------------------------------------------------------------------------\n","Epoch: 728 Loss Value: 9.47634460926056\n","Time taken: 106.5367956161499\n","--------------------------------------------------------------------------------\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"eSNOkTzVEoLO","colab_type":"code","colab":{}},"source":["def greedy_decode(model, src, src_mask, max_len, start_symbol):\n","    memory = model.encode(src, src_mask)\n","    ys = torch.ones(1, 1).fill_(start_symbol).type_as(src.data)\n","    for i in range(max_len-1):\n","        out = model.decode(memory, src_mask, \n","                           Variable(ys), \n","                           Variable(subsequent_mask(ys.size(1))\n","                                    .type_as(src.data)))\n","        prob = model.generator(out[:, -1])\n","        _, next_word = torch.max(prob, dim = 1)\n","        next_word = next_word.data[0]\n","        ys = torch.cat([ys, \n","                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1)\n","    return ys"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Kq5eD4uOF65q","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"4a21c207-53cb-48a9-f9e6-a36408665ec7","executionInfo":{"status":"ok","timestamp":1586271487387,"user_tz":-330,"elapsed":822,"user":{"displayName":"deepak goyal","photoUrl":"https://lh5.googleusercontent.com/-3gfEZruD9Sk/AAAAAAAAAAI/AAAAAAAACvg/54H4lMsdOgA/s64/photo.jpg","userId":"05164064759516400423"}}},"source":["model.eval()"],"execution_count":49,"outputs":[{"output_type":"execute_result","data":{"text/plain":["EncoderDecoder(\n","  (encoder): Encoder(\n","    (layers): ModuleList(\n","      (0): EncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linears): ModuleList(\n","            (0): Linear(in_features=512, out_features=512, bias=True)\n","            (1): Linear(in_features=512, out_features=512, bias=True)\n","            (2): Linear(in_features=512, out_features=512, bias=True)\n","            (3): Linear(in_features=512, out_features=512, bias=True)\n","          )\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (sublayer): ModuleList(\n","          (0): SublayerConnection(\n","            (norm): LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): SublayerConnection(\n","            (norm): LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (1): EncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linears): ModuleList(\n","            (0): Linear(in_features=512, out_features=512, bias=True)\n","            (1): Linear(in_features=512, out_features=512, bias=True)\n","            (2): Linear(in_features=512, out_features=512, bias=True)\n","            (3): Linear(in_features=512, out_features=512, bias=True)\n","          )\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (sublayer): ModuleList(\n","          (0): SublayerConnection(\n","            (norm): LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): SublayerConnection(\n","            (norm): LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (2): EncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linears): ModuleList(\n","            (0): Linear(in_features=512, out_features=512, bias=True)\n","            (1): Linear(in_features=512, out_features=512, bias=True)\n","            (2): Linear(in_features=512, out_features=512, bias=True)\n","            (3): Linear(in_features=512, out_features=512, bias=True)\n","          )\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (sublayer): ModuleList(\n","          (0): SublayerConnection(\n","            (norm): LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): SublayerConnection(\n","            (norm): LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (3): EncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linears): ModuleList(\n","            (0): Linear(in_features=512, out_features=512, bias=True)\n","            (1): Linear(in_features=512, out_features=512, bias=True)\n","            (2): Linear(in_features=512, out_features=512, bias=True)\n","            (3): Linear(in_features=512, out_features=512, bias=True)\n","          )\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (sublayer): ModuleList(\n","          (0): SublayerConnection(\n","            (norm): LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): SublayerConnection(\n","            (norm): LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (4): EncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linears): ModuleList(\n","            (0): Linear(in_features=512, out_features=512, bias=True)\n","            (1): Linear(in_features=512, out_features=512, bias=True)\n","            (2): Linear(in_features=512, out_features=512, bias=True)\n","            (3): Linear(in_features=512, out_features=512, bias=True)\n","          )\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (sublayer): ModuleList(\n","          (0): SublayerConnection(\n","            (norm): LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): SublayerConnection(\n","            (norm): LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (5): EncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linears): ModuleList(\n","            (0): Linear(in_features=512, out_features=512, bias=True)\n","            (1): Linear(in_features=512, out_features=512, bias=True)\n","            (2): Linear(in_features=512, out_features=512, bias=True)\n","            (3): Linear(in_features=512, out_features=512, bias=True)\n","          )\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (sublayer): ModuleList(\n","          (0): SublayerConnection(\n","            (norm): LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): SublayerConnection(\n","            (norm): LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (norm): LayerNorm()\n","  )\n","  (decoder): Decoder(\n","    (layers): ModuleList(\n","      (0): DecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linears): ModuleList(\n","            (0): Linear(in_features=512, out_features=512, bias=True)\n","            (1): Linear(in_features=512, out_features=512, bias=True)\n","            (2): Linear(in_features=512, out_features=512, bias=True)\n","            (3): Linear(in_features=512, out_features=512, bias=True)\n","          )\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (src_attn): MultiHeadedAttention(\n","          (linears): ModuleList(\n","            (0): Linear(in_features=512, out_features=512, bias=True)\n","            (1): Linear(in_features=512, out_features=512, bias=True)\n","            (2): Linear(in_features=512, out_features=512, bias=True)\n","            (3): Linear(in_features=512, out_features=512, bias=True)\n","          )\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (sublayer): ModuleList(\n","          (0): SublayerConnection(\n","            (norm): LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): SublayerConnection(\n","            (norm): LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): SublayerConnection(\n","            (norm): LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (1): DecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linears): ModuleList(\n","            (0): Linear(in_features=512, out_features=512, bias=True)\n","            (1): Linear(in_features=512, out_features=512, bias=True)\n","            (2): Linear(in_features=512, out_features=512, bias=True)\n","            (3): Linear(in_features=512, out_features=512, bias=True)\n","          )\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (src_attn): MultiHeadedAttention(\n","          (linears): ModuleList(\n","            (0): Linear(in_features=512, out_features=512, bias=True)\n","            (1): Linear(in_features=512, out_features=512, bias=True)\n","            (2): Linear(in_features=512, out_features=512, bias=True)\n","            (3): Linear(in_features=512, out_features=512, bias=True)\n","          )\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (sublayer): ModuleList(\n","          (0): SublayerConnection(\n","            (norm): LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): SublayerConnection(\n","            (norm): LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): SublayerConnection(\n","            (norm): LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (2): DecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linears): ModuleList(\n","            (0): Linear(in_features=512, out_features=512, bias=True)\n","            (1): Linear(in_features=512, out_features=512, bias=True)\n","            (2): Linear(in_features=512, out_features=512, bias=True)\n","            (3): Linear(in_features=512, out_features=512, bias=True)\n","          )\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (src_attn): MultiHeadedAttention(\n","          (linears): ModuleList(\n","            (0): Linear(in_features=512, out_features=512, bias=True)\n","            (1): Linear(in_features=512, out_features=512, bias=True)\n","            (2): Linear(in_features=512, out_features=512, bias=True)\n","            (3): Linear(in_features=512, out_features=512, bias=True)\n","          )\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (sublayer): ModuleList(\n","          (0): SublayerConnection(\n","            (norm): LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): SublayerConnection(\n","            (norm): LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): SublayerConnection(\n","            (norm): LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (3): DecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linears): ModuleList(\n","            (0): Linear(in_features=512, out_features=512, bias=True)\n","            (1): Linear(in_features=512, out_features=512, bias=True)\n","            (2): Linear(in_features=512, out_features=512, bias=True)\n","            (3): Linear(in_features=512, out_features=512, bias=True)\n","          )\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (src_attn): MultiHeadedAttention(\n","          (linears): ModuleList(\n","            (0): Linear(in_features=512, out_features=512, bias=True)\n","            (1): Linear(in_features=512, out_features=512, bias=True)\n","            (2): Linear(in_features=512, out_features=512, bias=True)\n","            (3): Linear(in_features=512, out_features=512, bias=True)\n","          )\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (sublayer): ModuleList(\n","          (0): SublayerConnection(\n","            (norm): LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): SublayerConnection(\n","            (norm): LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): SublayerConnection(\n","            (norm): LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (4): DecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linears): ModuleList(\n","            (0): Linear(in_features=512, out_features=512, bias=True)\n","            (1): Linear(in_features=512, out_features=512, bias=True)\n","            (2): Linear(in_features=512, out_features=512, bias=True)\n","            (3): Linear(in_features=512, out_features=512, bias=True)\n","          )\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (src_attn): MultiHeadedAttention(\n","          (linears): ModuleList(\n","            (0): Linear(in_features=512, out_features=512, bias=True)\n","            (1): Linear(in_features=512, out_features=512, bias=True)\n","            (2): Linear(in_features=512, out_features=512, bias=True)\n","            (3): Linear(in_features=512, out_features=512, bias=True)\n","          )\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (sublayer): ModuleList(\n","          (0): SublayerConnection(\n","            (norm): LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): SublayerConnection(\n","            (norm): LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): SublayerConnection(\n","            (norm): LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (5): DecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linears): ModuleList(\n","            (0): Linear(in_features=512, out_features=512, bias=True)\n","            (1): Linear(in_features=512, out_features=512, bias=True)\n","            (2): Linear(in_features=512, out_features=512, bias=True)\n","            (3): Linear(in_features=512, out_features=512, bias=True)\n","          )\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (src_attn): MultiHeadedAttention(\n","          (linears): ModuleList(\n","            (0): Linear(in_features=512, out_features=512, bias=True)\n","            (1): Linear(in_features=512, out_features=512, bias=True)\n","            (2): Linear(in_features=512, out_features=512, bias=True)\n","            (3): Linear(in_features=512, out_features=512, bias=True)\n","          )\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (sublayer): ModuleList(\n","          (0): SublayerConnection(\n","            (norm): LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): SublayerConnection(\n","            (norm): LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): SublayerConnection(\n","            (norm): LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (norm): LayerNorm()\n","  )\n","  (src_embed): Sequential(\n","    (0): Embeddings(\n","      (lut): Embedding(7816, 512)\n","    )\n","    (1): PositionalEncoding(\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","  )\n","  (tgt_embed): Sequential(\n","    (0): Embeddings(\n","      (lut): Embedding(7816, 512)\n","    )\n","    (1): PositionalEncoding(\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","  )\n","  (generator): Generator(\n","    (proj): Linear(in_features=512, out_features=7816, bias=True)\n","  )\n",")"]},"metadata":{"tags":[]},"execution_count":49}]},{"cell_type":"code","metadata":{"id":"dAj4kZlOF96b","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"0be804cb-a2e4-43d2-c666-dca74fe163af","executionInfo":{"status":"ok","timestamp":1586271497569,"user_tz":-330,"elapsed":6607,"user":{"displayName":"deepak goyal","photoUrl":"https://lh5.googleusercontent.com/-3gfEZruD9Sk/AAAAAAAAAAI/AAAAAAAACvg/54H4lMsdOgA/s64/photo.jpg","userId":"05164064759516400423"}}},"source":["for i in range(10):\n","    batch=batches[i]\n","    for i in range(5):\n","        source=batch.src[i].view(-1,12).to(device)\n","        source_mask=batch.src_mask[i].view(1,-1,12).to(device)\n","        output=greedy_decode(model,source,source_mask,10,1)\n","        src=batch.src[i].view(-1)\n","        trg=batch.trg[i].view(-1)\n","        pred=output.view(-1)\n","        # print(src.size())\n","        # for id in src:\n","        #     print(id)\n","        src_sentence=[voc.index2word[id.item()] for id in src]\n","        trg_sentence=[voc.index2word[id.item()] for id in trg]\n","        pred_sentence=[voc.index2word[id.item()] for id in pred]\n","        print(src_sentence)\n","        print(trg_sentence)\n","        print(pred_sentence)\n","        print(\"-\"*80)\n","#         print(\"-\"*80)\n","#         print(str(output)+\" \"+str(batch.src[i])+\" \"+str(batch.trg[i]))\n","    "],"execution_count":50,"outputs":[{"output_type":"stream","text":["['SOS', 'oh', 'yeah', '?', '.', 'have', 'you', 'seen', 'any', '?', 'EOS', 'PAD']\n","['SOS', 'i', 've', 'seen', 'a', 'few', '.', 'EOS', 'PAD', 'PAD', 'PAD']\n","['SOS', 'you', 're', 'going', 'to', 'to', 'to', 'to', 'to', 'to']\n","--------------------------------------------------------------------------------\n","['SOS', 'we', 'gotta', 'be', 'close', '.', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'what', '?', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'you', 're', 'going', 'to', 'to', 'to', 'to', 'to', 'to']\n","--------------------------------------------------------------------------------\n","['SOS', 'to', 'go', 'up', '.', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'to', 'go', 'up', '.', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'you', 're', 'going', 'to', 'to', 'to', 'to', 'to', 'to']\n","--------------------------------------------------------------------------------\n","['SOS', 'yeah', '.', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'alright', '?', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'you', 're', 'going', 'to', 'to', 'to', 'to', 'to', 'to']\n","--------------------------------------------------------------------------------\n","['SOS', 'i', 'was', 'the', 'next', 'man', '.', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'ships', 'that', 'pass', 'in', 'the', 'night', '.', '.', '.', 'EOS']\n","['SOS', 'you', 're', 'going', 'to', 'to', 'to', 'to', 'to', 'to']\n","--------------------------------------------------------------------------------\n","['SOS', 'what', 'about', 'your', 'mother', '?', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'how', 'do', 'you', 'know', 'about', 'my', 'mother', '?', 'EOS', 'PAD']\n","['SOS', 'you', 're', 'going', 'to', 'to', 'to', 'to', 'to', 'to']\n","--------------------------------------------------------------------------------\n","['SOS', 'that', 's', 'it', '.', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'good', '.', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'you', 're', 'going', 'to', 'to', 'to', 'to', 'to', 'to']\n","--------------------------------------------------------------------------------\n","['SOS', 'go', 'the', 'fuck', 'away', 'rosie', '.', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'please', 'honey', 'let', 'me', 'in', '.', 'EOS', 'PAD', 'PAD', 'PAD']\n","['SOS', 'you', 're', 'going', 'to', 'to', 'to', 'to', 'to', 'to']\n","--------------------------------------------------------------------------------\n","['SOS', 'i', 'don', 't', 'want', 'to', 'need', 'you', '!', 'EOS', 'PAD', 'PAD']\n","['SOS', 'why', '?', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'you', 're', 'going', 'to', 'to', 'to', 'to', 'to', 'to']\n","--------------------------------------------------------------------------------\n","['SOS', 'you', 'and', 'morgan', 'throw', '?', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'no', 'i', 'had', 'to', 'talk', 'him', 'down', '.', 'EOS', 'PAD']\n","['SOS', 'you', 're', 'going', 'to', 'to', 'to', 'to', 'to', 'to']\n","--------------------------------------------------------------------------------\n","['SOS', 'you', 'can', 't', 'buy', 'that', 'kind', 'of', 'publicity', '.', 'EOS', 'PAD']\n","['SOS', 'that', 's', 'good', 'news', '.', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'you', 're', 'going', 'to', 'to', 'to', 'to', 'to', 'to']\n","--------------------------------------------------------------------------------\n","['SOS', 'cut', 'it', 'back', 'a', 'little', 'bit', '.', 'EOS', 'PAD', 'PAD', 'PAD']\n","['SOS', 'no', 'fucking', 'way', '.', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'you', 're', 'going', 'to', 'to', 'to', 'to', 'to', 'to']\n","--------------------------------------------------------------------------------\n","['SOS', 'but', 'you', 're', 'planning', 'to', 'be', '?', 'EOS', 'PAD', 'PAD', 'PAD']\n","['SOS', 'i', 'don', 't', 'know', '.', 'possibly', '.', 'EOS', 'PAD', 'PAD']\n","['SOS', 'you', 're', 'going', 'to', 'to', 'to', 'to', 'to', 'to']\n","--------------------------------------------------------------------------------\n","['SOS', 'yeah', '.', 'should', 'be', 'on', 'the', 'road', '.', 'EOS', 'PAD', 'PAD']\n","['SOS', 'yeah', 'ha', 'hell', 'of', 'a', 'night', 'huh', '?', 'EOS', 'PAD']\n","['SOS', 'you', 're', 'going', 'to', 'to', 'to', 'to', 'to', 'to']\n","--------------------------------------------------------------------------------\n","['SOS', 'yeah', '.', 'with', 'potential', '.', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'friends', 'with', 'potential', '.', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'you', 're', 'going', 'to', 'to', 'to', 'to', 'to', 'to']\n","--------------------------------------------------------------------------------\n","['SOS', 'what', '?', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'did', 'you', '?', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'you', 're', 'going', 'to', 'to', 'to', 'to', 'to', 'to']\n","--------------------------------------------------------------------------------\n","['SOS', 'first', 'time', 'i', 'ever', 'heard', 'that', '.', 'EOS', 'PAD', 'PAD', 'PAD']\n","['SOS', 'i', 'was', 'expecting', 'someone', 'else', '.', 'EOS', 'PAD', 'PAD', 'PAD']\n","['SOS', 'you', 're', 'going', 'to', 'to', 'to', 'to', 'to', 'to']\n","--------------------------------------------------------------------------------\n","['SOS', 'i', 'want', 'my', 'car', '.', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'you', 'got', 'the', 'money', '?', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'you', 're', 'going', 'to', 'to', 'to', 'to', 'to', 'to']\n","--------------------------------------------------------------------------------\n","['SOS', '.', '.', '.', 'you', 'there', '?', 'norman', '?', 'EOS', 'PAD', 'PAD']\n","['SOS', 'harry', '?', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'you', 're', 'going', 'to', 'to', 'to', 'to', 'to', 'to']\n","--------------------------------------------------------------------------------\n","['SOS', 'at', 'least', 'we', 'found', 'the', 'diamonds', '.', 'EOS', 'PAD', 'PAD', 'PAD']\n","['SOS', 'at', 'least', '!', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'you', 're', 'going', 'to', 'to', 'to', 'to', 'to', 'to']\n","--------------------------------------------------------------------------------\n","['SOS', 'these', 'boys', 'aren', 't', 'playing', 'around', '.', 'EOS', 'PAD', 'PAD', 'PAD']\n","['SOS', 'you', 've', 'got', 'to', 'stop', 'them', '.', 'please', '!', 'EOS']\n","['SOS', 'you', 're', 'going', 'to', 'to', 'to', 'to', 'to', 'to']\n","--------------------------------------------------------------------------------\n","['SOS', 'i', 'know', '.', 'a', 'purple', 'heart', '.', 'EOS', 'PAD', 'PAD', 'PAD']\n","['SOS', 'exactly', '.', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'you', 're', 'going', 'to', 'to', 'to', 'to', 'to', 'to']\n","--------------------------------------------------------------------------------\n","['SOS', 'is', 'she', 'all', 'right', '?', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'she', 's', 'fine', '.', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'you', 're', 'going', 'to', 'to', 'to', 'to', 'to', 'to']\n","--------------------------------------------------------------------------------\n","['SOS', 'where', '?', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'the', 'tall', 'trees', '.', '.', '.', 'EOS', 'PAD', 'PAD', 'PAD']\n","['SOS', 'you', 're', 'going', 'to', 'to', 'to', 'to', 'to', 'to']\n","--------------------------------------------------------------------------------\n","['SOS', 'your', 'life', 'and', 'that', 'of', 'others', '!', 'EOS', 'PAD', 'PAD', 'PAD']\n","['SOS', 'if', 'they', 'agree', 'to', 'follow', 'me', 'yes', '.', 'EOS', 'PAD']\n","['SOS', 'you', 're', 'going', 'to', 'to', 'to', 'to', 'to', 'to']\n","--------------------------------------------------------------------------------\n","['SOS', 'okay', '?', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'yeah', '!', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'you', 're', 'going', 'to', 'to', 'to', 'to', 'to', 'to']\n","--------------------------------------------------------------------------------\n","['SOS', 'what', 'you', 'mean', 'talk', 'to', 'her', '?', 'EOS', 'PAD', 'PAD', 'PAD']\n","['SOS', 'you', 'know', 'what', 'he', 'mean', 'dude', '.', 'EOS', 'PAD', 'PAD']\n","['SOS', 'you', 're', 'going', 'to', 'to', 'to', 'to', 'to', 'to']\n","--------------------------------------------------------------------------------\n","['SOS', 'she', 'got', 'married', 'last', 'month', '.', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'married', '?', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'you', 're', 'going', 'to', 'to', 'to', 'to', 'to', 'to']\n","--------------------------------------------------------------------------------\n","['SOS', 'i', 'need', 'an', 'exit', '!', 'fast', '!', 'EOS', 'PAD', 'PAD', 'PAD']\n","['SOS', 'cypher', '?', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'you', 're', 'going', 'to', 'to', 'to', 'to', 'to', 'to']\n","--------------------------------------------------------------------------------\n","['SOS', 'yes', '.', 'exactly', '.', 'major', 'rufus', '.', 'EOS', 'PAD', 'PAD', 'PAD']\n","['SOS', 'thanks', '.', 'and', 'what', 'are', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'you', 're', 'going', 'to', 'to', 'to', 'to', 'to', 'to']\n","--------------------------------------------------------------------------------\n","['SOS', 'oh', 'god', '.', '.', '.you', 'knew', '.', 'EOS', 'PAD', 'PAD', 'PAD']\n","['SOS', 'i', 'was', 'down', 'in', 'adamant', '.', '.', '.', 'EOS', 'PAD']\n","['SOS', 'you', 're', 'going', 'to', 'to', 'to', 'to', 'to', 'to']\n","--------------------------------------------------------------------------------\n","['SOS', 'thank', 'you', '.', 'but', 'who', 'sits', 'there', '?', 'EOS', 'PAD', 'PAD']\n","['SOS', 'my', 'brother', 's', 'wife', '.', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'you', 're', 'going', 'to', 'to', 'to', 'to', 'to', 'to']\n","--------------------------------------------------------------------------------\n","['SOS', 'not', 'yet', '.', 'but', 'we', 'll', 'find', 'him', '.', 'EOS', 'PAD']\n","['SOS', 'dance', '?', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'you', 're', 'going', 'to', 'to', 'to', 'to', 'to', 'to']\n","--------------------------------------------------------------------------------\n","['SOS', 'it', 's', 'his', 'nature', '.', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'but', 'what', 'if', 'he', 'doesn', 't', 'EOS', 'PAD', 'PAD', 'PAD']\n","['SOS', 'you', 're', 'going', 'to', 'to', 'to', 'to', 'to', 'to']\n","--------------------------------------------------------------------------------\n","['SOS', 'and', 'all', 'those', 'like', 'you', '.', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'you', 'mean', 'black', 'people', '?', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'you', 're', 'going', 'to', 'to', 'to', 'to', 'to', 'to']\n","--------------------------------------------------------------------------------\n","['SOS', 'let', 'them', 'look', '.', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'it', 'is', 'trouble', 'for', 'me', '.', 'EOS', 'PAD', 'PAD', 'PAD']\n","['SOS', 'you', 're', 'going', 'to', 'to', 'to', 'to', 'to', 'to']\n","--------------------------------------------------------------------------------\n","['SOS', 'your', 'new', 'car', '.', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'four', 'wheel', 'drive', '?', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'you', 're', 'going', 'to', 'to', 'to', 'to', 'to', 'to']\n","--------------------------------------------------------------------------------\n","['SOS', 'he', 'didn', 't', 'come', 'back', 'here', '?', 'EOS', 'PAD', 'PAD', 'PAD']\n","['SOS', 'sam', '.', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'you', 're', 'going', 'to', 'to', 'to', 'to', 'to', 'to']\n","--------------------------------------------------------------------------------\n","['SOS', 'mr', '.', 'parker', '.', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'hi', 'professor', '.', 'what', 's', 'up', '?', 'EOS', 'PAD', 'PAD']\n","['SOS', 'you', 're', 'going', 'to', 'to', 'to', 'to', 'to', 'to']\n","--------------------------------------------------------------------------------\n","['SOS', 'let', 's', 'just', 'walk', 'okay', '?', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'all', 'right', '.', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'you', 're', 'going', 'to', 'to', 'to', 'to', 'to', 'to']\n","--------------------------------------------------------------------------------\n","['SOS', 'like', 'helping', 'a', 'sick', 'kid', '?', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'no', 'for', 'the', 'asylum', '.', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'you', 're', 'going', 'to', 'to', 'to', 'to', 'to', 'to']\n","--------------------------------------------------------------------------------\n","['SOS', 'i', 'coulda', 'shot', 'you', 'you', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'this', 'could', 'end', 'up', 'saving', 'your', 'life', 'EOS', 'PAD', 'PAD']\n","['SOS', 'you', 're', 'going', 'to', 'to', 'to', 'to', 'to', 'to']\n","--------------------------------------------------------------------------------\n","['SOS', 'you', 're', 'staring', 'at', 'me', '.', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'no', 'i', 'm', 'not', '.', 'i', 'm', 'not', 'star', 'EOS']\n","['SOS', 'you', 're', 'going', 'to', 'to', 'to', 'to', 'to', 'to']\n","--------------------------------------------------------------------------------\n","['SOS', 'bye', 'mrs', 'sutphin', '.', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'bye', 'bird', 'brain', '.', 'see', 'ya', 'scotty', '.', 'EOS', 'PAD']\n","['SOS', 'you', 're', 'going', 'to', 'to', 'to', 'to', 'to', 'to']\n","--------------------------------------------------------------------------------\n","['SOS', 'and', 'do', 'you', 'know', 'why', '?', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'why', '?', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'you', 're', 'going', 'to', 'to', 'to', 'to', 'to', 'to']\n","--------------------------------------------------------------------------------\n","['SOS', 'whom', 'you', 'address', '.', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'the', 'future', 'king', '!', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'you', 're', 'going', 'to', 'to', 'to', 'to', 'to', 'to']\n","--------------------------------------------------------------------------------\n","['SOS', 'that', 'is', 'one', 'ugly', 'sunrise', '.', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'it', 'really', 'is', '.', 'did', 'you', 'find', 'anything', '?', 'EOS']\n","['SOS', 'you', 're', 'going', 'to', 'to', 'to', 'to', 'to', 'to']\n","--------------------------------------------------------------------------------\n","['SOS', 'no', 'not', 'really', '.', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'interesting', '.', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'you', 're', 'going', 'to', 'to', 'to', 'to', 'to', 'to']\n","--------------------------------------------------------------------------------\n","['SOS', 'he', 'look', 'familiar', '?', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'very', '.', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'you', 're', 'going', 'to', 'to', 'to', 'to', 'to', 'to']\n","--------------------------------------------------------------------------------\n","['SOS', 'help', 'someone', '!', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'oh', 'god', '!', 'help', '!', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'you', 're', 'going', 'to', 'to', 'to', 'to', 'to', 'to']\n","--------------------------------------------------------------------------------\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"R2lay0ahGArk","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}