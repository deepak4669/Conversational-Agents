{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"transformer_cornell.ipynb","provenance":[],"collapsed_sections":["DpR-1oMt3HA4"],"authorship_tag":"ABX9TyM4VrXLKAS4mN3ybSsBkD7z"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"8i8or0mWcKPL","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"f65ccca4-ca60-4eec-e11b-6f1520171b55","executionInfo":{"status":"ok","timestamp":1586269436699,"user_tz":-330,"elapsed":1327,"user":{"displayName":"deepak goyal","photoUrl":"https://lh5.googleusercontent.com/-3gfEZruD9Sk/AAAAAAAAAAI/AAAAAAAACvg/54H4lMsdOgA/s64/photo.jpg","userId":"05164064759516400423"}}},"source":["from google.colab import drive\n","drive.mount(\"/content/drive\")"],"execution_count":50,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"R-KwziUccUv9","colab_type":"code","colab":{}},"source":["#Pre-Processing\n","import os\n","import re\n","import torch\n","import random\n","import itertools\n","\n","#Model\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","\n","import numpy as np\n","\n","# For visualising metrics\n","# from visdom import Visdom\n","\n","# For visualising gradients plot\n","import matplotlib.pyplot as plt\n","from matplotlib.lines import Line2D\n","\n","import copy\n","import math\n","import time"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bzOQagtOdqyn","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"0df0e44f-95a9-4d88-b660-8f3831b4b0b9","executionInfo":{"status":"ok","timestamp":1586269437316,"user_tz":-330,"elapsed":663,"user":{"displayName":"deepak goyal","photoUrl":"https://lh5.googleusercontent.com/-3gfEZruD9Sk/AAAAAAAAAAI/AAAAAAAACvg/54H4lMsdOgA/s64/photo.jpg","userId":"05164064759516400423"}}},"source":["device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","# device=torch.device(\"cpu\")\n","print(\"The device found: \"+str(device))"],"execution_count":52,"outputs":[{"output_type":"stream","text":["The device found: cpu\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"19BVL5GNdtbi","colab_type":"code","colab":{}},"source":["def plot_grad_flow(named_parameters):\n","    \"\"\"\n","        Plotting gradient flow across various layers\n","        Thanks to: https://discuss.pytorch.org/t/check-gradient-flow-in-network/15063/2\n","    \"\"\"   \n","    ave_grads = []\n","    layers = []\n","    for n, p in named_parameters:\n","        if(p.requires_grad) and (\"bias\" not in n):\n","            layers.append(n)\n","            ave_grads.append(p.grad.abs().mean())\n","    plt.plot(ave_grads, alpha=0.3, color=\"b\")\n","    plt.hlines(0, 0, len(ave_grads)+1, linewidth=1, color=\"k\" )\n","    plt.xticks(range(0,len(ave_grads), 1), layers, rotation=\"vertical\")\n","    plt.xlim(xmin=0, xmax=len(ave_grads))\n","    plt.xlabel(\"Layers\")\n","    plt.ylabel(\"average gradient\")\n","    plt.title(\"Gradient flow\")\n","    plt.grid(True)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XyVbmpGw26_v","colab_type":"text"},"source":["# Preprocessing\n"]},{"cell_type":"code","metadata":{"id":"qz7CbxS4dwh3","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"72bc3d62-6c03-4093-e7b3-041e1fb43b9a","executionInfo":{"status":"ok","timestamp":1586269441355,"user_tz":-330,"elapsed":756,"user":{"displayName":"deepak goyal","photoUrl":"https://lh5.googleusercontent.com/-3gfEZruD9Sk/AAAAAAAAAAI/AAAAAAAACvg/54H4lMsdOgA/s64/photo.jpg","userId":"05164064759516400423"}}},"source":["path='/content/drive/My Drive/Data'\n","dataset='cornell movie-dialogs corpus'\n","\n","data_folder=os.path.join(path,dataset)\n","\n","print(\"The final data corpus folder: \"+str(data_folder))"],"execution_count":54,"outputs":[{"output_type":"stream","text":["The final data corpus folder: /content/drive/My Drive/Data/cornell movie-dialogs corpus\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"dDKqcO2de7RM","colab_type":"code","colab":{}},"source":["def get_lines_conversations():\n","    \"\"\"\n","    Loads movie lines and conversations from the dataset.\n","    \n","    data_folder: Destination where conversations and lines are stored.\n","    \n","    movie_lines: Consist of movie lines as given by the dataset.\n","    movie_conversations: Consist of movie conversations as given by the dataset.\n","    \n","    \"\"\"\n","    movie_lines=[]\n","    movie_conversations=[]\n","\n","    with open(os.path.join(data_folder,'movie_lines.txt'),'r',encoding='iso-8859-1') as f:\n","        for line in f:\n","            movie_lines.append(line)\n","    \n","    with open(os.path.join(data_folder,'movie_conversations.txt'),'r', encoding='iso-8859-1') as f:\n","        for line in f:\n","            movie_conversations.append(line)\n","                                       \n","\n","    return movie_lines,movie_conversations"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-txSMOjQfEnx","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":173},"outputId":"1d29c23a-76c2-461f-e85a-53ee052a6e80","executionInfo":{"status":"ok","timestamp":1586269446910,"user_tz":-330,"elapsed":1349,"user":{"displayName":"deepak goyal","photoUrl":"https://lh5.googleusercontent.com/-3gfEZruD9Sk/AAAAAAAAAAI/AAAAAAAACvg/54H4lMsdOgA/s64/photo.jpg","userId":"05164064759516400423"}}},"source":["t1=time.time()\n","print(\"Extracting movie lines and movie conversations...\")\n","movie_lines,movie_conversations=get_lines_conversations()\n","\n","print(\"Number of distinct lines: \"+str(len(movie_lines)))\n","print(\"Number of conversations: \"+str(len(movie_conversations)))\n","print(\"Average Number of lines per conversations: \"+str(len(movie_lines)/len(movie_conversations)))\n","\n","print(movie_lines[0])\n","print(movie_conversations[0])\n","\n","print(\"Extracting took place in: \"+str(time.time()-t1))"],"execution_count":56,"outputs":[{"output_type":"stream","text":["Extracting movie lines and movie conversations...\n","Number of distinct lines: 304713\n","Number of conversations: 83097\n","Average Number of lines per conversations: 3.6669554857576085\n","L1045 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ They do not!\n","\n","u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L194', 'L195', 'L196', 'L197']\n","\n","Extracting took place in: 0.45116233825683594\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3LXCFv_qfMsN","colab_type":"code","colab":{}},"source":["exceptions=[]\n","def loadLines(movie_lines,fields):\n","    lines={}\n","    for lineid in range(len(movie_lines)):\n","        \n","        line=movie_lines[lineid]\n","        values=line.split(\" +++$+++ \")\n","        \n","        \n","        lineVals={}\n","        \n","        # print(\"values\"+str(len(values)))\n","        # print(\"fields\"+str(len(fields)))\n","              \n","        for i,field in enumerate(fields):\n","            try:\n","                lineVals[field]=values[i]\n","            except:\n","                print(\"Exception: \"+str(len(values)))\n","                exceptions.append(lineid)\n","        \n","        lines[lineVals['lineID']]=lineVals\n","    \n","    return lines\n","\n","def loadConversations(movie_conversations,lines,fields):\n","    conversations=[]\n","    \n","    for convo in movie_conversations:\n","        values=convo.split(\" +++$+++ \")\n","        conVals={}\n","       \n","        for i,field in enumerate(fields):\n","            conVals[field]=values[i]\n","        \n","        lineIDs=eval(conVals[\"utteranceIDs\"])\n","        \n","        conVals[\"lines\"]=[]\n","        \n","        for lineID in lineIDs:\n","            conVals[\"lines\"].append(lines[lineID])\n","        conversations.append(conVals)\n","        \n","    return conversations\n","\n","def sentencePairs(conversations):\n","    qr_pairs=[]\n","    \n","    for conversation in conversations:\n","        for i in range(len(conversation[\"lines\"])-1):\n","            query=conversation[\"lines\"][i][\"text\"].strip()\n","            response=conversation[\"lines\"][i+1][\"text\"].strip()\n","            \n","            if query and response:\n","                qr_pairs.append([query,response])\n","        \n","    return qr_pairs"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"J2dngu2Xyvzt","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":69},"outputId":"87b7039f-784f-4ed2-83f2-df8ccb28d7ff","executionInfo":{"status":"ok","timestamp":1586269450997,"user_tz":-330,"elapsed":4438,"user":{"displayName":"deepak goyal","photoUrl":"https://lh5.googleusercontent.com/-3gfEZruD9Sk/AAAAAAAAAAI/AAAAAAAACvg/54H4lMsdOgA/s64/photo.jpg","userId":"05164064759516400423"}}},"source":["t1=time.time()\n","print(\"Separating meaningfull information for our model...\")\n","\n","lines={}\n","conversations=[]\n","qr_pairs=[]\n","\n","movie_lines_fields=[\"lineID\",\"characterID\",\"movieID\",\"character\",\"text\"]\n","movie_convo_fields=[\"charcaterID\",\"character2ID\",\"movieID\",\"utteranceIDs\"]\n","\n","lines=loadLines(movie_lines,movie_lines_fields)\n","conversations=loadConversations(movie_conversations,lines,movie_convo_fields)\n","qr_pairs=sentencePairs(conversations)\n","\n","print(\"The number of query-response pairs are: \"+str(len(qr_pairs)))\n","print(\"Separation took place in: \"+str(time.time()-t1))"],"execution_count":58,"outputs":[{"output_type":"stream","text":["Separating meaningfull information for our model...\n","The number of query-response pairs are: 221282\n","Separation took place in: 3.3366730213165283\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"NFMCnpuO2jpr","colab_type":"code","colab":{}},"source":["PAD_Token=0\n","START_Token=1\n","END_Token=2\n","\n","class Vocabulary:\n","    def __init__(self):\n","        self.trimmed=False\n","        self.word2count={}\n","        self.index2word={PAD_Token:\"PAD\",START_Token:\"SOS\",END_Token:\"EOS\"}\n","        self.word2index={\"PAD\":PAD_Token,\"SOS\":START_Token,\"EOS\":END_Token}\n","        self.num_words=3\n","        \n","    def addSentence(self,sentence):\n","        for word in sentence.split(\" \"):\n","            self.addWord(word)\n","    def addWord(self,word):\n","        if word not in self.word2index:\n","            self.word2index[word]=self.num_words\n","            self.index2word[self.num_words]=word\n","            self.word2count[word]=1\n","            self.num_words=self.num_words+1\n","        else:\n","            self.word2count[word]+=1\n","            \n","    def trim(self,min_count):\n","        \n","        if self.trimmed:\n","            return\n","        self.trimmed=True\n","        \n","        keep_words=[]\n","        \n","        for word,freq in self.word2count.items():\n","            if freq>=min_count:\n","                keep_words.append(word)\n","        \n","        self.word2count={}\n","        self.index2word={PAD_Token:\"PAD\",START_Token:\"SOS\",END_Token:\"EOS\"}\n","        self.word2index={\"PAD\":PAD_Token,\"SOS\":START_Token,\"EOS\":END_Token}\n","        self.num_words=3\n","        \n","        for word in keep_words:\n","            self.addWord(word)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GeScbB7iy0AC","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"outputId":"ccb379cb-1be8-4b6d-ba0b-82a4af17c1ae","executionInfo":{"status":"ok","timestamp":1586269457986,"user_tz":-330,"elapsed":10136,"user":{"displayName":"deepak goyal","photoUrl":"https://lh5.googleusercontent.com/-3gfEZruD9Sk/AAAAAAAAAAI/AAAAAAAACvg/54H4lMsdOgA/s64/photo.jpg","userId":"05164064759516400423"}}},"source":["Max_Length=10\n","\n","def normalizeString(s):\n","    s=s.lower().strip()\n","    s=re.sub(r\"([.!?])\", r\" \\1\", s)\n","    s=re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n","    s=re.sub(r\"\\s+\", r\" \", s).strip()\n","    return s\n","\n","def readVocs(qr_pairs):\n","    \n","    for qr_pair in qr_pairs:\n","        qr_pair[0]=normalizeString(qr_pair[0])\n","        qr_pair[1]=normalizeString(qr_pair[1])\n","    \n","    voc=Vocabulary()\n","    return voc,qr_pairs\n","\n","def filterPair(pair):\n","    return len(pair[0].split(\" \"))<Max_Length and len(pair[1].split(\" \"))<Max_Length\n","\n","def filterPairs(qr_pairs):\n","    return [pair for pair in qr_pairs if filterPair(pair)]\n","\n","def prepareDataset(qr_pairs):\n","    voc, qr_pairs=readVocs(qr_pairs)\n","    qr_pairs=filterPairs(qr_pairs)\n","       \n","    for pair in qr_pairs:\n","        voc.addSentence(pair[0])\n","        voc.addSentence(pair[1])\n","#     print(\"Number\"+str(voc.num_words))\n","    return voc,qr_pairs\n","\n","t1=time.time()\n","print(\"Preparing dataset and corresponding vocabulary...\")\n","voc, pairs=prepareDataset(qr_pairs)\n","print(\"Preparation took place in: \"+str(time.time()-t1))"],"execution_count":60,"outputs":[{"output_type":"stream","text":["Preparing dataset and corresponding vocabulary...\n","Preparation took place in: 6.960947751998901\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"UUBpnFjQ2SCS","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"outputId":"fcdef47a-20db-4126-b912-5bb722f050c8","executionInfo":{"status":"ok","timestamp":1586269457987,"user_tz":-330,"elapsed":9605,"user":{"displayName":"deepak goyal","photoUrl":"https://lh5.googleusercontent.com/-3gfEZruD9Sk/AAAAAAAAAAI/AAAAAAAACvg/54H4lMsdOgA/s64/photo.jpg","userId":"05164064759516400423"}}},"source":["Min_Count=3\n","\n","def trimRareWords(voc,qr_pairs):\n","    \n","    voc.trim(Min_Count)\n","    keep_pairs=[]\n","    \n","    for pair in qr_pairs:\n","        input_sentence=pair[0]\n","        output_sentence=pair[1]\n","        \n","        keep_input=True\n","        keep_output=True\n","        \n","        for word in input_sentence.split(\" \"):\n","            if word not in voc.word2index:\n","                keep_input=False\n","                break\n","        \n","        for word in output_sentence.split(\" \"):\n","            if word not in voc.word2index:\n","                keep_output=False\n","                break\n","                \n","        if keep_input and keep_output:\n","            keep_pairs.append(pair)\n","            \n","    return keep_pairs\n","\n","t1=time.time()\n","print(\"Trimming rare words from vocabulary and dataset..\")\n","\n","pairs=trimRareWords(voc,pairs)\n","\n","print(\"Trimming took place in: \"+str(time.time()-t1))"],"execution_count":61,"outputs":[{"output_type":"stream","text":["Trimming rare words from vocabulary and dataset..\n","Trimming took place in: 0.1542530059814453\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"VjOEe2nx2sbZ","colab_type":"code","colab":{}},"source":["def indexesFromSentence(voc,sentence):\n","    tokenised_sentence=[]\n","    tokenised_sentence.append(START_Token)\n","    \n","    for word in sentence.split(\" \"):\n","        tokenised_sentence.append(voc.word2index[word])\n","        \n","    tokenised_sentence.append(END_Token)\n","    \n","    assert len(tokenised_sentence)<=Max_Length+2\n","    for _ in range(Max_Length+2-len(tokenised_sentence)):\n","        tokenised_sentence.append(PAD_Token)\n","        \n","    return tokenised_sentence\n","\n","def binaryMatrix(l,value=PAD_Token):\n","    m=[]\n","    for i,seq in enumerate(l):\n","        m.append([])\n","        for token in seq:\n","            if token==value:\n","                m[i].append(0)\n","            else:\n","                m[i].append(1)\n","        \n","    return m\n","\n","def inputVar(voc,l):\n","    \n","    indexes_batch=[indexesFromSentence(voc,sentence) for sentence in l]\n","    input_lengths=torch.tensor([len(index) for index in indexes_batch])\n","    padVar=torch.LongTensor(indexes_batch)\n","    return input_lengths,padVar\n","\n","def outputVar(voc,l):\n","    indexes_batch=[indexesFromSentence(voc,sentence) for sentence in l]\n","    max_target_len=torch.tensor([len(index) for index in indexes_batch])\n","    mask=binaryMatrix(indexes_batch)\n","    mask=torch.ByteTensor(mask)\n","    padVar=torch.LongTensor(indexes_batch)\n","    return max_target_len, mask, padVar\n","\n","def batch2TrainData(voc,pair_batch):\n","    #sort function see \n","    input_batch=[]\n","    output_batch=[]\n","\n","    for pair in pair_batch:\n","        input_batch.append(pair[0])\n","        output_batch.append(pair[1])\n","                                  \n","    \n","    input_lengths,tokenised_input=inputVar(voc,input_batch)\n","    max_out_length,mask,tokenised_output=outputVar(voc,output_batch)\n","    return input_lengths,tokenised_input,max_out_length,mask,tokenised_output"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"f-wd2hB-2y4b","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":416},"outputId":"bdb6211e-6e45-4d40-f76a-d33141332a05","executionInfo":{"status":"ok","timestamp":1586269457994,"user_tz":-330,"elapsed":7656,"user":{"displayName":"deepak goyal","photoUrl":"https://lh5.googleusercontent.com/-3gfEZruD9Sk/AAAAAAAAAAI/AAAAAAAACvg/54H4lMsdOgA/s64/photo.jpg","userId":"05164064759516400423"}}},"source":["print(\"Number of query-response pairs after all the preprocessing: \"+str(len(pairs)))\n","\n","#Sample batch\n","batch=[random.choice(pairs) for _ in range(5)]\n","input_lengths,tokenised_input,max_out_length,mask,tokenised_output=batch2TrainData(voc,batch)\n","\n","print(\"Input length: \"+str(input_lengths)+\" Size: \"+str(input_lengths.shape))\n","print(\"-\"*80)\n","print(\"Tokenised Input: \"+str(tokenised_input)+\" Size: \"+str(tokenised_input.shape))\n","print(\"-\"*80)\n","print(\"Max out length: \"+str(max_out_length)+\" Size: \"+str(max_out_length.shape))\n","print(\"-\"*80)\n","print(\"Mask: \"+str(mask)+\" Size: \"+str(mask.shape))\n","print(\"-\"*80)\n","print(\"Tokenised Output: \"+str(tokenised_output)+\" Size: \"+str(tokenised_output.shape))\n","print(\"-\"*80)"],"execution_count":63,"outputs":[{"output_type":"stream","text":["Number of query-response pairs after all the preprocessing: 53113\n","Input length: tensor([12, 12, 12, 12, 12]) Size: torch.Size([5])\n","--------------------------------------------------------------------------------\n","Tokenised Input: tensor([[   1,    7,   14,   67,    6,    2,    0,    0,    0,    0,    0,    0],\n","        [   1,  780,   66,  780,  201, 7402,   40,   76,    4,    2,    0,    0],\n","        [   1, 1386, 1433,    4,    2,    0,    0,    0,    0,    0,    0,    0],\n","        [   1,    5,   68,    7,  158,   53, 4804,    6,    2,    0,    0,    0],\n","        [   1,   61,  102,    4,    2,    0,    0,    0,    0,    0,    0,    0]]) Size: torch.Size([5, 12])\n","--------------------------------------------------------------------------------\n","Max out length: tensor([12, 12, 12, 12, 12]) Size: torch.Size([5])\n","--------------------------------------------------------------------------------\n","Mask: tensor([[1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]], dtype=torch.uint8) Size: torch.Size([5, 12])\n","--------------------------------------------------------------------------------\n","Tokenised Output: tensor([[   1,   34,    4,    2,    0,    0,    0,    0,    0,    0,    0,    0],\n","        [   1, 5722, 5722,    4,    2,    0,    0,    0,    0,    0,    0,    0],\n","        [   1,  100,  383,    7,  572,    4,    2,    0,    0,    0,    0,    0],\n","        [   1,  691,    4,    4,    4,   27,  553,  134,    4,    2,    0,    0],\n","        [   1,  383,    7,    4,   53,  322, 6586,    4,    2,    0,    0,    0]]) Size: torch.Size([5, 12])\n","--------------------------------------------------------------------------------\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"DpR-1oMt3HA4","colab_type":"text"},"source":["# Model 1"]},{"cell_type":"code","metadata":{"id":"RJnnpKgt3Nyg","colab_type":"code","colab":{}},"source":["class EncoderDecoder(nn.Module):\n","    \"\"\"\n","    A standard Encoder-Decoder architecture. Base for this and many \n","    other models.\n","    \"\"\"\n","    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n","        super(EncoderDecoder, self).__init__()\n","        self.encoder = encoder\n","        self.decoder = decoder\n","        self.src_embed = src_embed\n","        self.tgt_embed = tgt_embed\n","        self.generator = generator\n","        \n","    def forward(self, src, tgt, src_mask, tgt_mask):\n","        \"Take in and process masked src and target sequences.\"\n","        return self.decode(self.encode(src, src_mask), src_mask,\n","                            tgt, tgt_mask)\n","    \n","    def encode(self, src, src_mask):\n","        return self.encoder(self.src_embed(src), src_mask)\n","    \n","    def decode(self, memory, src_mask, tgt, tgt_mask):\n","        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cRE6Ls4k_lvE","colab_type":"code","colab":{}},"source":["class Generator(nn.Module):\n","    \"Define standard linear + softmax generation step.\"\n","    def __init__(self, d_model, vocab):\n","        super(Generator, self).__init__()\n","        self.proj = nn.Linear(d_model, vocab)\n","\n","    def forward(self, x):\n","        return F.log_softmax(self.proj(x), dim=-1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aPiLhBPP_oQL","colab_type":"code","colab":{}},"source":["def clones(module, N):\n","    \"Produce N identical layers.\"\n","    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3ApjC3XD_quP","colab_type":"code","colab":{}},"source":["class Encoder(nn.Module):\n","    \"Core encoder is a stack of N layers\"\n","    def __init__(self, layer, N):\n","        super(Encoder, self).__init__()\n","        self.layers = clones(layer, N)\n","        self.norm = LayerNorm(layer.size)\n","        \n","    def forward(self, x, mask):\n","        \"Pass the input (and mask) through each layer in turn.\"\n","        for layer in self.layers:\n","            x = layer(x, mask)\n","        return self.norm(x)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5wUahzPb_s88","colab_type":"code","colab":{}},"source":["class LayerNorm(nn.Module):\n","    \"Construct a layernorm module (See citation for details).\"\n","    def __init__(self, features, eps=1e-6):\n","        super(LayerNorm, self).__init__()\n","        self.a_2 = nn.Parameter(torch.ones(features))\n","        self.b_2 = nn.Parameter(torch.zeros(features))\n","        self.eps = eps\n","\n","    def forward(self, x):\n","        mean = x.mean(-1, keepdim=True)\n","        std = x.std(-1, keepdim=True)\n","        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zowO7FHp_v5s","colab_type":"code","colab":{}},"source":["class SublayerConnection(nn.Module):\n","    \"\"\"\n","    A residual connection followed by a layer norm.\n","    Note for code simplicity the norm is first as opposed to last.\n","    \"\"\"\n","    def __init__(self, size, dropout):\n","        super(SublayerConnection, self).__init__()\n","        self.norm = LayerNorm(size)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x, sublayer):\n","        \"Apply residual connection to any sublayer with the same size.\"\n","        return x + self.dropout(sublayer(self.norm(x)))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_nkoHVj4_yiG","colab_type":"code","colab":{}},"source":["class EncoderLayer(nn.Module):\n","    \"Encoder is made up of self-attn and feed forward (defined below)\"\n","    def __init__(self, size, self_attn, feed_forward, dropout):\n","        super(EncoderLayer, self).__init__()\n","        self.self_attn = self_attn\n","        self.feed_forward = feed_forward\n","        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n","        self.size = size\n","\n","    def forward(self, x, mask):\n","        \"Follow Figure 1 (left) for connections.\"\n","        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n","        return self.sublayer[1](x, self.feed_forward)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BIJVFhXm_1FF","colab_type":"code","colab":{}},"source":["class Decoder(nn.Module):\n","    \"Generic N layer decoder with masking.\"\n","    def __init__(self, layer, N):\n","        super(Decoder, self).__init__()\n","        self.layers = clones(layer, N)\n","        self.norm = LayerNorm(layer.size)\n","        \n","    def forward(self, x, memory, src_mask, tgt_mask):\n","        for layer in self.layers:\n","            x = layer(x, memory, src_mask, tgt_mask)\n","        return self.norm(x)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZKn03mRp_3V2","colab_type":"code","colab":{}},"source":["class DecoderLayer(nn.Module):\n","    \"Decoder is made of self-attn, src-attn, and feed forward (defined below)\"\n","    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n","        super(DecoderLayer, self).__init__()\n","        self.size = size\n","        self.self_attn = self_attn\n","        self.src_attn = src_attn\n","        self.feed_forward = feed_forward\n","        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n"," \n","    def forward(self, x, memory, src_mask, tgt_mask):\n","        \"Follow Figure 1 (right) for connections.\"\n","        m = memory\n","        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n","        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n","        return self.sublayer[2](x, self.feed_forward)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KddgfVJx_6DC","colab_type":"code","colab":{}},"source":["def subsequent_mask(size):\n","    \"Mask out subsequent positions.\"\n","    attn_shape = (1, size, size)\n","    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n","    return torch.from_numpy(subsequent_mask) == 0"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BYWVMJoh_8-k","colab_type":"code","colab":{}},"source":["def attention(query, key, value, mask=None, dropout=None):\n","    \"Compute 'Scaled Dot Product Attention'\"\n","    d_k = query.size(-1)\n","    scores = torch.matmul(query, key.transpose(-2, -1)) \\\n","             / math.sqrt(d_k)\n","    if mask is not None:\n","        scores = scores.masked_fill(mask == 0, -1e9)\n","    p_attn = F.softmax(scores, dim = -1)\n","    if dropout is not None:\n","        p_attn = dropout(p_attn)\n","    return torch.matmul(p_attn, value), p_attn"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"veKkKGgb__q9","colab_type":"code","colab":{}},"source":["class MultiHeadedAttention(nn.Module):\n","    def __init__(self, h, d_model, dropout=0.1):\n","        \"Take in model size and number of heads.\"\n","        super(MultiHeadedAttention, self).__init__()\n","        assert d_model % h == 0\n","        # We assume d_v always equals d_k\n","        self.d_k = d_model // h\n","        self.h = h\n","        self.linears = clones(nn.Linear(d_model, d_model), 4)\n","        self.attn = None\n","        self.dropout = nn.Dropout(p=dropout)\n","        \n","    def forward(self, query, key, value, mask=None):\n","        \"Implements Figure 2\"\n","        if mask is not None:\n","            # Same mask applied to all h heads.\n","            mask = mask.unsqueeze(1)\n","        nbatches = query.size(0)\n","        \n","        # 1) Do all the linear projections in batch from d_model => h x d_k \n","        query, key, value = \\\n","            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n","             for l, x in zip(self.linears, (query, key, value))]\n","        \n","        # 2) Apply attention on all the projected vectors in batch. \n","        x, self.attn = attention(query, key, value, mask=mask, \n","                                 dropout=self.dropout)\n","        \n","        # 3) \"Concat\" using a view and apply a final linear. \n","        x = x.transpose(1, 2).contiguous() \\\n","             .view(nbatches, -1, self.h * self.d_k)\n","        return self.linears[-1](x)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aSqvuoJVAE_i","colab_type":"code","colab":{}},"source":["class PositionwiseFeedForward(nn.Module):\n","    \"Implements FFN equation.\"\n","    def __init__(self, d_model, d_ff, dropout=0.1):\n","        super(PositionwiseFeedForward, self).__init__()\n","        self.w_1 = nn.Linear(d_model, d_ff)\n","        self.w_2 = nn.Linear(d_ff, d_model)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        return self.w_2(self.dropout(F.relu(self.w_1(x))))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ft2ByV62AHlf","colab_type":"code","colab":{}},"source":["class Embeddings(nn.Module):\n","    def __init__(self, d_model, vocab):\n","        super(Embeddings, self).__init__()\n","        self.lut = nn.Embedding(vocab, d_model)\n","        self.d_model = d_model\n","\n","    def forward(self, x):\n","        return self.lut(x) * math.sqrt(self.d_model)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"flsGFWNWAKso","colab_type":"code","colab":{}},"source":["class PositionalEncoding(nn.Module):\n","    \"Implement the PE function.\"\n","    def __init__(self, d_model, dropout, max_len=5000):\n","        super(PositionalEncoding, self).__init__()\n","        self.dropout = nn.Dropout(p=dropout)\n","        \n","        # Compute the positional encodings once in log space.\n","        pe = torch.zeros(max_len, d_model)\n","        position = torch.arange(0, max_len).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2) *\n","                             -(math.log(10000.0) / d_model))\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","        pe = pe.unsqueeze(0)\n","        self.register_buffer('pe', pe)\n","        \n","    def forward(self, x):\n","        x = x + Variable(self.pe[:, :x.size(1)], \n","                         requires_grad=False)\n","        return self.dropout(x)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"veyYVrUlANvh","colab_type":"code","colab":{}},"source":["def make_model(src_vocab, tgt_vocab, N=6, \n","               d_model=512, d_ff=2048, h=8, dropout=0.1):\n","    \"Helper: Construct a model from hyperparameters.\"\n","    c = copy.deepcopy\n","    attn = MultiHeadedAttention(h, d_model)\n","    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n","    position = PositionalEncoding(d_model, dropout)\n","    model = EncoderDecoder(\n","        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n","        Decoder(DecoderLayer(d_model, c(attn), c(attn), \n","                             c(ff), dropout), N),\n","        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),\n","        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),\n","        Generator(d_model, tgt_vocab))\n","    \n","    # This was important from their code. \n","    # Initialize parameters with Glorot / fan_avg.\n","    for p in model.parameters():\n","        if p.dim() > 1:\n","            nn.init.xavier_uniform(p)\n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v5xV-42LAX6d","colab_type":"text"},"source":["# Model 2"]},{"cell_type":"code","metadata":{"id":"uHIbkFgbAhpa","colab_type":"code","colab":{}},"source":["class EncoderDecoder(nn.Module):\n","    \n","    def __init__(self,encoder,decoder,source_embed,target_embed,generator):\n","        super().__init__()\n","        \n","        self.encoder=encoder\n","        self.decoder=decoder\n","        \n","        self.source_embed=source_embed\n","        self.target_embed=target_embed\n","        \n","        self.generator=generator # Linear + Log_softmax\n","        \n","    def forward(self,source,target,source_mask,target_mask):\n","        return self.decode(self.encode(source,source_mask),source_mask,target,target_mask)\n","    \n","    def encode(self,source,source_mask):\n","        return self.encoder(self.source_embed(source),source_mask)\n","    \n","    def decode(self,memory, source_mask,target,target_mask):\n","        return self.decoder(self.target_embed(target),memory,source_mask,target_mask)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ewcrG0AeCHQc","colab_type":"code","colab":{}},"source":["class Generator(nn.Module):\n","    \n","    def __init__(self,d_model,vocab_size):\n","        super().__init__()\n","        self.projection=nn.Linear(d_model,vocab_size)\n","        \n","    def forward(self,decoder_output):\n","        return F.log_softmax(self.projection(decoder_output),dim=-1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jNIgAbiYCKg0","colab_type":"code","colab":{}},"source":["def clones(module,N):\n","    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qgHHuuzYCNL8","colab_type":"code","colab":{}},"source":["class Encoder(nn.Module):\n","    \n","    def __init__(self,layer,N):\n","        super().__init__()\n","        \n","        self.layers=clones(layer,N)\n","        self.norm=LayerNorm(layer.size)\n","    \n","    def forward(self,x,mask):\n","        \n","        for layer in self.layers:\n","            x=layer(x,mask)\n","        \n","        return self.norm(x)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dQRX2bAMCPlO","colab_type":"code","colab":{}},"source":["class LayerNorm(nn.Module):\n","    \n","    def __init__(self,features,eps=1e-6):\n","        super().__init__()\n","        self.a_2=nn.Parameter(torch.ones(features))\n","        self.b_2=nn.Parameter(torch.zeros(features))\n","        self.eps=eps\n","        \n","    def forward(self,x):\n","        mean=x.mean(-1,keepdim=True)\n","        std=x.std(-1,keepdim=True)\n","        return self.a_2*(x-mean)/(std+self.eps)+self.b_2"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-sIDH01lCSsv","colab_type":"code","colab":{}},"source":["class SublayerConnection(nn.Module):\n","    \n","    def __init__(self,size,dropout):\n","        super().__init__()\n","        \n","        self.dropout=nn.Dropout(dropout)\n","        self.norm=LayerNorm(size)\n","        \n","    def forward(self,x,sublayer):\n","        return x+self.dropout(sublayer(self.norm(x)))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1zQ_LsoCCVpO","colab_type":"code","colab":{}},"source":["class EncoderLayer(nn.Module):\n","    def __init__(self,size,self_attn,feed_forward,dropout):\n","        super().__init__()\n","        \n","        self.attn=self_attn\n","        self.feed_forward=feed_forward\n","        self.sublayer=clones(SublayerConnection(size,dropout),2)\n","        self.size=size\n","        \n","    def forward(self,x,mask):\n","        \n","        x=self.sublayer[0](x,lambda x: self.attn(x,x,x,mask))\n","        return self.sublayer[1](x,self.feed_forward)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cZjOa0vwCY5H","colab_type":"code","colab":{}},"source":["class Decoder(nn.Module):\n","    \n","    def __init__(self,layer,N):\n","        super().__init__()\n","        \n","        self.layers=clones(layer,N)\n","        self.norm=LayerNorm(layer.size)\n","    \n","    def forward(self,x,memory,curr_mask,tgt_mask):\n","        \n","        for layer in self.layers:\n","            x=layer(x,memory,curr_mask,tgt_mask)\n","            \n","        return self.norm(x)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FDHFdRvRCbgt","colab_type":"code","colab":{}},"source":["class DecoderLayer(nn.Module):\n","    \n","    def __init__(self,size,self_attn,src_attn,feed_forward,dropout):\n","        super().__init__()\n","        \n","        self.size=size\n","        self.self_attn=self_attn\n","        self.src_attn=src_attn\n","        self.feed_forward=feed_forward\n","        \n","        self.sublayer=clones(SublayerConnection(size,dropout),3)\n","        \n","    def forward(self,x,memory,src_mask,tgt_mask):\n","        \n","        m=memory\n","        x=self.sublayer[0](x,lambda x:self.self_attn(x,x,x,tgt_mask))\n","        x=self.sublayer[1](x,lambda x: self.src_attn(x,m,m,src_mask))\n","        return self.sublayer[2](x,self.feed_forward)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OWbh8piBCeC_","colab_type":"code","colab":{}},"source":["def attention(query,key,value,mask=None,dropout=None):\n","    \n","    d_k=query.size(-1)\n","\n","    scores=torch.matmul(query,key.transpose(-2,-1))/math.sqrt(d_k)\n","    \n","    if mask is not None:\n","        scores=scores.masked_fill(mask==0,-1e9)\n","        \n","    p_attn=F.softmax(scores,dim=-1)\n","    \n","    if dropout is not None:\n","        p_attn=dropout(p_attn)\n","        \n","    return torch.matmul(p_attn,value),p_attn"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZtHuOzFTCgie","colab_type":"code","colab":{}},"source":["class MultiHeadedAttention(nn.Module):\n","    \n","    def __init__(self,h,d_model,dropout=0.1):\n","        super().__init__()\n","        \n","        assert d_model%h==0\n","        \n","        self.d_k=d_model//h\n","        self.h=h\n","        self.linears=clones(nn.Linear(d_model,d_model),4)\n","        self.attn=None\n","        self.dropout=nn.Dropout(dropout)\n","        \n","    def forward(self,query,key,values,mask=None):\n","        \n","        if mask is not None:\n","            mask=mask.unsqueeze(1)\n","            \n","        nbatches=query.size(0)\n","        \n","        query,key,values=[l(x).view(nbatches,-1,self.h,self.d_k).transpose(1,2) for l, x in zip(self.linears,(query,key,values))]\n","        \n","        x,self.attn=attention(query,key,values,mask=mask,dropout=self.dropout)\n","        \n","        x=x.transpose(1,2).contiguous().view(nbatches,-1,self.h*self.d_k)\n","        \n","        return self.linears[-1](x)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GD9vxJrECkSp","colab_type":"code","colab":{}},"source":["class PositionwiseFeedForward(nn.Module):\n","    \n","    def __init__(self,d_model,d_ff,dropout=0.1):\n","        super().__init__()\n","        \n","        self.w_1=nn.Linear(d_model,d_ff)\n","        self.w_2=nn.Linear(d_ff,d_model)\n","        self.dropout=nn.Dropout(dropout)\n","        \n","    def forward(self,x):\n","        return self.w_2(self.dropout(F.relu(self.w_1(x))))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8Qna2tDeCmue","colab_type":"code","colab":{}},"source":["class Embeddings(nn.Module):\n","    \n","    def __init__(self,d_model,vocab):\n","        super().__init__()\n","        \n","        self.embed=nn.Embedding(vocab,d_model)\n","        self.d_model=d_model\n","    \n","    def forward(self,x):\n","#         print(x.device)\n","        return self.embed(x)*math.sqrt(self.d_model)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nCEQ9n0XCo7b","colab_type":"code","colab":{}},"source":["class PositionalEncoding(nn.Module):\n","    \n","    def __init__(self,d_model,dropout,max_len=5000):\n","        super().__init__()\n","        \n","        self.dropout=nn.Dropout(dropout)\n","        pe=torch.zeros(max_len,d_model,dtype=torch.float)\n","        position=torch.arange(0.,max_len).unsqueeze(1)\n","        div_term=torch.exp(torch.arange(0.,d_model,2)*-(math.log(10000.0)/d_model))\n","        \n","        pe[:,0::2]=torch.sin(position*div_term)\n","        pe[:,1::2]=torch.cos(position*div_term)\n","        \n","        pe=pe.unsqueeze(0)\n","        self.register_buffer('pe',pe)\n","        \n","    def forward(self,x):\n","        \n","        x=x+Variable(self.pe[:,:x.size(1)],requires_grad=False)\n","        return self.dropout(x)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CkLif2nLC5U6","colab_type":"code","colab":{}},"source":["\"\"\"\n","triu function generates a copy of matrix with elemens below kth diagonal zeroed.\n","The main diagonal is zeroeth diagonal above is first(k=1) and so on.\n","\n","Eg:\n","A=[[1,2,3],[4,5,6],[7,8,9]]\n","for above matrix:\n","triu(A,k=1)\n","will give [[0,2,3],[0,0,6],[0,0,0]]\n","\"\"\"\n","\n","def subsequent_mask(size):\n","    attn_shape=(1,size,size)\n","    mask=np.triu(np.ones(attn_shape),k=1).astype('uint8')\n","    \n","    return torch.from_numpy(mask)==0"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WPOL8rdMCruh","colab_type":"code","colab":{}},"source":["def make_model2(src_vocab,tgt_vocab,N=6,d_model=512,d_ff=2048,h=8,dropout=0.1):\n","    \n","    c=copy.deepcopy\n","    attn=MultiHeadedAttention(h,d_model)\n","    ff=PositionwiseFeedForward(d_model,d_ff,dropout)\n","    position=PositionalEncoding(d_model,dropout)\n","    model=EncoderDecoder(Encoder(EncoderLayer(d_model,c(attn),c(ff),dropout),N),\n","                        Decoder(DecoderLayer(d_model,c(attn),c(attn),c(ff),dropout),N),\n","                        nn.Sequential(Embeddings(d_model,src_vocab),c(position)),\n","                        nn.Sequential(Embeddings(d_model,tgt_vocab),c(position)),\n","                        Generator(d_model,tgt_vocab))\n","    \n","    for p in model.parameters():\n","        if p.dim()>1:\n","            nn.init.xavier_uniform_(p)\n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"K6TsrSL5CuNH","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"cb0aa9b4-ef58-4dc1-a1c8-686caed0c5f2","executionInfo":{"status":"ok","timestamp":1586269724083,"user_tz":-330,"elapsed":4156,"user":{"displayName":"deepak goyal","photoUrl":"https://lh5.googleusercontent.com/-3gfEZruD9Sk/AAAAAAAAAAI/AAAAAAAACvg/54H4lMsdOgA/s64/photo.jpg","userId":"05164064759516400423"}}},"source":["sample_model=make_model2(voc.num_words,voc.num_words,1,512,2048,8,0.1)\n","sample_model.to(device)\n","# print(sample_model)"],"execution_count":79,"outputs":[{"output_type":"execute_result","data":{"text/plain":["EncoderDecoder(\n","  (encoder): Encoder(\n","    (layers): ModuleList(\n","      (0): EncoderLayer(\n","        (attn): MultiHeadedAttention(\n","          (linears): ModuleList(\n","            (0): Linear(in_features=512, out_features=512, bias=True)\n","            (1): Linear(in_features=512, out_features=512, bias=True)\n","            (2): Linear(in_features=512, out_features=512, bias=True)\n","            (3): Linear(in_features=512, out_features=512, bias=True)\n","          )\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (sublayer): ModuleList(\n","          (0): SublayerConnection(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (norm): LayerNorm()\n","          )\n","          (1): SublayerConnection(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (norm): LayerNorm()\n","          )\n","        )\n","      )\n","    )\n","    (norm): LayerNorm()\n","  )\n","  (decoder): Decoder(\n","    (layers): ModuleList(\n","      (0): DecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linears): ModuleList(\n","            (0): Linear(in_features=512, out_features=512, bias=True)\n","            (1): Linear(in_features=512, out_features=512, bias=True)\n","            (2): Linear(in_features=512, out_features=512, bias=True)\n","            (3): Linear(in_features=512, out_features=512, bias=True)\n","          )\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (src_attn): MultiHeadedAttention(\n","          (linears): ModuleList(\n","            (0): Linear(in_features=512, out_features=512, bias=True)\n","            (1): Linear(in_features=512, out_features=512, bias=True)\n","            (2): Linear(in_features=512, out_features=512, bias=True)\n","            (3): Linear(in_features=512, out_features=512, bias=True)\n","          )\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (sublayer): ModuleList(\n","          (0): SublayerConnection(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (norm): LayerNorm()\n","          )\n","          (1): SublayerConnection(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (norm): LayerNorm()\n","          )\n","          (2): SublayerConnection(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (norm): LayerNorm()\n","          )\n","        )\n","      )\n","    )\n","    (norm): LayerNorm()\n","  )\n","  (source_embed): Sequential(\n","    (0): Embeddings(\n","      (embed): Embedding(7816, 512)\n","    )\n","    (1): PositionalEncoding(\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","  )\n","  (target_embed): Sequential(\n","    (0): Embeddings(\n","      (embed): Embedding(7816, 512)\n","    )\n","    (1): PositionalEncoding(\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","  )\n","  (generator): Generator(\n","    (projection): Linear(in_features=512, out_features=7816, bias=True)\n","  )\n",")"]},"metadata":{"tags":[]},"execution_count":79}]},{"cell_type":"code","metadata":{"id":"TZ0e9Y5cCwzw","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":69},"outputId":"84e9e143-ca96-41e3-e2ed-a530193f28b0","executionInfo":{"status":"ok","timestamp":1586269782919,"user_tz":-330,"elapsed":1025,"user":{"displayName":"deepak goyal","photoUrl":"https://lh5.googleusercontent.com/-3gfEZruD9Sk/AAAAAAAAAAI/AAAAAAAACvg/54H4lMsdOgA/s64/photo.jpg","userId":"05164064759516400423"}}},"source":["#Sample Run\n","source=torch.ones(5,12,dtype=torch.long,device=device)\n","target=torch.ones(5,12,dtype=torch.long,device=device)\n","source_mask=torch.ones(5,12,12,dtype=torch.long,device=device)\n","target_mask=torch.ones(5,12,12,dtype=torch.long,device=device)\n","out=sample_model(source,target,source_mask,target_mask)\n","print(\"-\"*80)\n","print(\"Output size: \"+str(out.shape))\n","print(\"-\"*80)"],"execution_count":81,"outputs":[{"output_type":"stream","text":["--------------------------------------------------------------------------------\n","Output size: torch.Size([5, 12, 512])\n","--------------------------------------------------------------------------------\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Db6jEG3uC_76","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}