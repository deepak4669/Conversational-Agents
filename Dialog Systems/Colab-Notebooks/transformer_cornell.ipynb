{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"transformer_cornell.ipynb","provenance":[],"collapsed_sections":["v5xV-42LAX6d"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"8i8or0mWcKPL","colab_type":"code","outputId":"99709bda-c1dd-4f7a-8437-e1d4e0626ab8","executionInfo":{"status":"ok","timestamp":1586528574825,"user_tz":-330,"elapsed":31414,"user":{"displayName":"Deepak Goyal","photoUrl":"","userId":"12155914074048018284"}},"colab":{"base_uri":"https://localhost:8080/","height":124}},"source":["from google.colab import drive\n","drive.mount(\"/content/drive\")"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"U-K4mgwHE8oj","colab_type":"code","outputId":"195becf3-8317-4043-ea56-7a34f06c2668","executionInfo":{"status":"ok","timestamp":1586528592776,"user_tz":-330,"elapsed":12848,"user":{"displayName":"Deepak Goyal","photoUrl":"","userId":"12155914074048018284"}},"colab":{"base_uri":"https://localhost:8080/","height":312}},"source":["!nvidia-smi"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Fri Apr 10 14:23:04 2020       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 440.64.00    Driver Version: 418.67       CUDA Version: 10.1     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|===============================+======================+======================|\n","|   0  Tesla P4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   37C    P8     7W /  75W |      0MiB /  7611MiB |      0%      Default |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                       GPU Memory |\n","|  GPU       PID   Type   Process name                             Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"R-KwziUccUv9","colab_type":"code","colab":{}},"source":["#Pre-Processing\n","import os\n","import re\n","import torch\n","import random\n","import itertools\n","\n","#Model\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","\n","import numpy as np\n","\n","# For visualising metrics\n","# from visdom import Visdom\n","\n","# For visualising gradients plot\n","import matplotlib.pyplot as plt\n","from matplotlib.lines import Line2D\n","\n","import copy\n","import math\n","import time"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bzOQagtOdqyn","colab_type":"code","outputId":"5b55edad-c553-499a-93c4-7f1334ebb0c5","executionInfo":{"status":"ok","timestamp":1586528608943,"user_tz":-330,"elapsed":3153,"user":{"displayName":"Deepak Goyal","photoUrl":"","userId":"12155914074048018284"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","# device=torch.device(\"cpu\")\n","print(\"The device found: \"+str(device))"],"execution_count":4,"outputs":[{"output_type":"stream","text":["The device found: cuda\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"19BVL5GNdtbi","colab_type":"code","colab":{}},"source":["def plot_grad_flow(named_parameters):\n","    \"\"\"\n","        Plotting gradient flow across various layers\n","        Thanks to: https://discuss.pytorch.org/t/check-gradient-flow-in-network/15063/2\n","    \"\"\"   \n","    ave_grads = []\n","    layers = []\n","    for n, p in named_parameters:\n","        if(p.requires_grad) and (\"bias\" not in n):\n","            layers.append(n)\n","            ave_grads.append(p.grad.abs().mean())\n","    plt.plot(ave_grads, alpha=0.3, color=\"b\")\n","    plt.hlines(0, 0, len(ave_grads)+1, linewidth=1, color=\"k\" )\n","    plt.xticks(range(0,len(ave_grads), 1), layers, rotation=\"vertical\")\n","    plt.xlim(xmin=0, xmax=len(ave_grads))\n","    plt.xlabel(\"Layers\")\n","    plt.ylabel(\"average gradient\")\n","    plt.title(\"Gradient flow\")\n","    plt.grid(True)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XyVbmpGw26_v","colab_type":"text"},"source":["# Preprocessing\n"]},{"cell_type":"code","metadata":{"id":"qz7CbxS4dwh3","colab_type":"code","outputId":"9ae96147-b45b-4915-ace7-6d81fcf25923","executionInfo":{"status":"ok","timestamp":1586528621081,"user_tz":-330,"elapsed":1944,"user":{"displayName":"Deepak Goyal","photoUrl":"","userId":"12155914074048018284"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["path='/content/drive/My Drive/Data'\n","dataset='cornell movie-dialogs corpus'\n","\n","data_folder=os.path.join(path,dataset)\n","\n","print(\"The final data corpus folder: \"+str(data_folder))"],"execution_count":6,"outputs":[{"output_type":"stream","text":["The final data corpus folder: /content/drive/My Drive/Data/cornell movie-dialogs corpus\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"dDKqcO2de7RM","colab_type":"code","colab":{}},"source":["def get_lines_conversations():\n","    \"\"\"\n","    Loads movie lines and conversations from the dataset.\n","    \n","    data_folder: Destination where conversations and lines are stored.\n","    \n","    movie_lines: Consist of movie lines as given by the dataset.\n","    movie_conversations: Consist of movie conversations as given by the dataset.\n","    \n","    \"\"\"\n","    movie_lines=[]\n","    movie_conversations=[]\n","\n","    with open(os.path.join(data_folder,'movie_lines.txt'),'r',encoding='iso-8859-1') as f:\n","        for line in f:\n","            movie_lines.append(line)\n","    \n","    with open(os.path.join(data_folder,'movie_conversations.txt'),'r', encoding='iso-8859-1') as f:\n","        for line in f:\n","            movie_conversations.append(line)\n","                                       \n","\n","    return movie_lines,movie_conversations"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-txSMOjQfEnx","colab_type":"code","outputId":"de9016e9-51af-4de0-f24a-bc0cfb7391b5","executionInfo":{"status":"ok","timestamp":1586528626910,"user_tz":-330,"elapsed":4184,"user":{"displayName":"Deepak Goyal","photoUrl":"","userId":"12155914074048018284"}},"colab":{"base_uri":"https://localhost:8080/","height":173}},"source":["t1=time.time()\n","print(\"Extracting movie lines and movie conversations...\")\n","movie_lines,movie_conversations=get_lines_conversations()\n","\n","print(\"Number of distinct lines: \"+str(len(movie_lines)))\n","print(\"Number of conversations: \"+str(len(movie_conversations)))\n","print(\"Average Number of lines per conversations: \"+str(len(movie_lines)/len(movie_conversations)))\n","\n","print(movie_lines[0])\n","print(movie_conversations[0])\n","\n","print(\"Extracting took place in: \"+str(time.time()-t1))"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Extracting movie lines and movie conversations...\n","Number of distinct lines: 304713\n","Number of conversations: 83097\n","Average Number of lines per conversations: 3.6669554857576085\n","L1045 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ They do not!\n","\n","u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L194', 'L195', 'L196', 'L197']\n","\n","Extracting took place in: 2.292126417160034\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3LXCFv_qfMsN","colab_type":"code","colab":{}},"source":["exceptions=[]\n","def loadLines(movie_lines,fields):\n","    lines={}\n","    for lineid in range(len(movie_lines)):\n","        \n","        line=movie_lines[lineid]\n","        values=line.split(\" +++$+++ \")\n","        \n","        \n","        lineVals={}\n","        \n","        # print(\"values\"+str(len(values)))\n","        # print(\"fields\"+str(len(fields)))\n","              \n","        for i,field in enumerate(fields):\n","            try:\n","                lineVals[field]=values[i]\n","            except:\n","                print(\"Exception: \"+str(len(values)))\n","                exceptions.append(lineid)\n","        \n","        lines[lineVals['lineID']]=lineVals\n","    \n","    return lines\n","\n","def loadConversations(movie_conversations,lines,fields):\n","    conversations=[]\n","    \n","    for convo in movie_conversations:\n","        values=convo.split(\" +++$+++ \")\n","        conVals={}\n","       \n","        for i,field in enumerate(fields):\n","            conVals[field]=values[i]\n","        \n","        lineIDs=eval(conVals[\"utteranceIDs\"])\n","        \n","        conVals[\"lines\"]=[]\n","        \n","        for lineID in lineIDs:\n","            conVals[\"lines\"].append(lines[lineID])\n","        conversations.append(conVals)\n","        \n","    return conversations\n","\n","def sentencePairs(conversations):\n","    qr_pairs=[]\n","    \n","    for conversation in conversations:\n","        for i in range(len(conversation[\"lines\"])-1):\n","            query=conversation[\"lines\"][i][\"text\"].strip()\n","            response=conversation[\"lines\"][i+1][\"text\"].strip()\n","            \n","            if query and response:\n","                qr_pairs.append([query,response])\n","        \n","    return qr_pairs"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"J2dngu2Xyvzt","colab_type":"code","outputId":"4cecd85b-aa8f-4528-c47f-81a943c4aace","executionInfo":{"status":"ok","timestamp":1586528633020,"user_tz":-330,"elapsed":4127,"user":{"displayName":"Deepak Goyal","photoUrl":"","userId":"12155914074048018284"}},"colab":{"base_uri":"https://localhost:8080/","height":69}},"source":["t1=time.time()\n","print(\"Separating meaningfull information for our model...\")\n","\n","lines={}\n","conversations=[]\n","qr_pairs=[]\n","\n","movie_lines_fields=[\"lineID\",\"characterID\",\"movieID\",\"character\",\"text\"]\n","movie_convo_fields=[\"charcaterID\",\"character2ID\",\"movieID\",\"utteranceIDs\"]\n","\n","lines=loadLines(movie_lines,movie_lines_fields)\n","conversations=loadConversations(movie_conversations,lines,movie_convo_fields)\n","qr_pairs=sentencePairs(conversations)\n","\n","print(\"The number of query-response pairs are: \"+str(len(qr_pairs)))\n","print(\"Separation took place in: \"+str(time.time()-t1))"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Separating meaningfull information for our model...\n","The number of query-response pairs are: 221282\n","Separation took place in: 1.9914741516113281\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"NFMCnpuO2jpr","colab_type":"code","colab":{}},"source":["PAD_Token=0\n","START_Token=1\n","END_Token=2\n","\n","class Vocabulary:\n","    def __init__(self):\n","        self.trimmed=False\n","        self.word2count={}\n","        self.index2word={PAD_Token:\"PAD\",START_Token:\"SOS\",END_Token:\"EOS\"}\n","        self.word2index={\"PAD\":PAD_Token,\"SOS\":START_Token,\"EOS\":END_Token}\n","        self.num_words=3\n","        \n","    def addSentence(self,sentence):\n","        for word in sentence.split(\" \"):\n","            self.addWord(word)\n","    def addWord(self,word):\n","        if word not in self.word2index:\n","            self.word2index[word]=self.num_words\n","            self.index2word[self.num_words]=word\n","            self.word2count[word]=1\n","            self.num_words=self.num_words+1\n","        else:\n","            self.word2count[word]+=1\n","            \n","    def trim(self,min_count):\n","        \n","        if self.trimmed:\n","            return\n","        self.trimmed=True\n","        \n","        keep_words=[]\n","        \n","        for word,freq in self.word2count.items():\n","            if freq>=min_count:\n","                keep_words.append(word)\n","        \n","        self.word2count={}\n","        self.index2word={PAD_Token:\"PAD\",START_Token:\"SOS\",END_Token:\"EOS\"}\n","        self.word2index={\"PAD\":PAD_Token,\"SOS\":START_Token,\"EOS\":END_Token}\n","        self.num_words=3\n","        \n","        for word in keep_words:\n","            self.addWord(word)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GeScbB7iy0AC","colab_type":"code","outputId":"e62b4ee2-e033-4c34-dd86-c28b9eafd149","executionInfo":{"status":"ok","timestamp":1586528638824,"user_tz":-330,"elapsed":8851,"user":{"displayName":"Deepak Goyal","photoUrl":"","userId":"12155914074048018284"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["Max_Length=10\n","\n","def normalizeString(s):\n","    s=s.lower().strip()\n","    s=re.sub(r\"([.!?])\", r\" \\1\", s)\n","    s=re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n","    s=re.sub(r\"\\s+\", r\" \", s).strip()\n","    return s\n","\n","def readVocs(qr_pairs):\n","    \n","    for qr_pair in qr_pairs:\n","        qr_pair[0]=normalizeString(qr_pair[0])\n","        qr_pair[1]=normalizeString(qr_pair[1])\n","    \n","    voc=Vocabulary()\n","    return voc,qr_pairs\n","\n","def filterPair(pair):\n","    return len(pair[0].split(\" \"))<Max_Length and len(pair[1].split(\" \"))<Max_Length\n","\n","def filterPairs(qr_pairs):\n","    return [pair for pair in qr_pairs if filterPair(pair)]\n","\n","def prepareDataset(qr_pairs):\n","    voc, qr_pairs=readVocs(qr_pairs)\n","    qr_pairs=filterPairs(qr_pairs)\n","       \n","    for pair in qr_pairs:\n","        voc.addSentence(pair[0])\n","        voc.addSentence(pair[1])\n","#     print(\"Number\"+str(voc.num_words))\n","    return voc,qr_pairs\n","\n","t1=time.time()\n","print(\"Preparing dataset and corresponding vocabulary...\")\n","voc, pairs=prepareDataset(qr_pairs)\n","print(\"Preparation took place in: \"+str(time.time()-t1))"],"execution_count":12,"outputs":[{"output_type":"stream","text":["Preparing dataset and corresponding vocabulary...\n","Preparation took place in: 5.8636109828948975\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"UUBpnFjQ2SCS","colab_type":"code","outputId":"11e9fe85-901f-4a7e-e841-d01c8cd0072f","executionInfo":{"status":"ok","timestamp":1586528638826,"user_tz":-330,"elapsed":8391,"user":{"displayName":"Deepak Goyal","photoUrl":"","userId":"12155914074048018284"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["Min_Count=3\n","\n","def trimRareWords(voc,qr_pairs):\n","    \n","    voc.trim(Min_Count)\n","    keep_pairs=[]\n","    \n","    for pair in qr_pairs:\n","        input_sentence=pair[0]\n","        output_sentence=pair[1]\n","        \n","        keep_input=True\n","        keep_output=True\n","        \n","        for word in input_sentence.split(\" \"):\n","            if word not in voc.word2index:\n","                keep_input=False\n","                break\n","        \n","        for word in output_sentence.split(\" \"):\n","            if word not in voc.word2index:\n","                keep_output=False\n","                break\n","                \n","        if keep_input and keep_output:\n","            keep_pairs.append(pair)\n","            \n","    return keep_pairs\n","\n","t1=time.time()\n","print(\"Trimming rare words from vocabulary and dataset..\")\n","\n","pairs=trimRareWords(voc,pairs)\n","\n","print(\"Trimming took place in: \"+str(time.time()-t1))"],"execution_count":13,"outputs":[{"output_type":"stream","text":["Trimming rare words from vocabulary and dataset..\n","Trimming took place in: 0.12359476089477539\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"VjOEe2nx2sbZ","colab_type":"code","colab":{}},"source":["def indexesFromSentence(voc,sentence):\n","    tokenised_sentence=[]\n","    tokenised_sentence.append(START_Token)\n","    \n","    for word in sentence.split(\" \"):\n","        tokenised_sentence.append(voc.word2index[word])\n","        \n","    tokenised_sentence.append(END_Token)\n","    \n","    assert len(tokenised_sentence)<=Max_Length+2\n","    for _ in range(Max_Length+2-len(tokenised_sentence)):\n","        tokenised_sentence.append(PAD_Token)\n","        \n","    return tokenised_sentence\n","\n","def binaryMatrix(l,value=PAD_Token):\n","    m=[]\n","    for i,seq in enumerate(l):\n","        m.append([])\n","        for token in seq:\n","            if token==value:\n","                m[i].append(0)\n","            else:\n","                m[i].append(1)\n","        \n","    return m\n","\n","def inputVar(voc,l):\n","    \n","    indexes_batch=[indexesFromSentence(voc,sentence) for sentence in l]\n","    input_lengths=torch.tensor([len(index) for index in indexes_batch])\n","    padVar=torch.LongTensor(indexes_batch)\n","    return input_lengths,padVar\n","\n","def outputVar(voc,l):\n","    indexes_batch=[indexesFromSentence(voc,sentence) for sentence in l]\n","    max_target_len=torch.tensor([len(index) for index in indexes_batch])\n","    mask=binaryMatrix(indexes_batch)\n","    mask=torch.ByteTensor(mask)\n","    padVar=torch.LongTensor(indexes_batch)\n","    return max_target_len, mask, padVar\n","\n","def batch2TrainData(voc,pair_batch):\n","    #sort function see \n","    input_batch=[]\n","    output_batch=[]\n","\n","    for pair in pair_batch:\n","        input_batch.append(pair[0])\n","        output_batch.append(pair[1])\n","                                  \n","    \n","    input_lengths,tokenised_input=inputVar(voc,input_batch)\n","    max_out_length,mask,tokenised_output=outputVar(voc,output_batch)\n","    return input_lengths,tokenised_input,max_out_length,mask,tokenised_output"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"f-wd2hB-2y4b","colab_type":"code","outputId":"e21faab6-8dc3-42e8-e907-e33c47eafcdc","executionInfo":{"status":"ok","timestamp":1586528638828,"user_tz":-330,"elapsed":7340,"user":{"displayName":"Deepak Goyal","photoUrl":"","userId":"12155914074048018284"}},"colab":{"base_uri":"https://localhost:8080/","height":416}},"source":["print(\"Number of query-response pairs after all the preprocessing: \"+str(len(pairs)))\n","\n","#Sample batch\n","batch=[random.choice(pairs) for _ in range(5)]\n","input_lengths,tokenised_input,max_out_length,mask,tokenised_output=batch2TrainData(voc,batch)\n","\n","print(\"Input length: \"+str(input_lengths)+\" Size: \"+str(input_lengths.shape))\n","print(\"-\"*80)\n","print(\"Tokenised Input: \"+str(tokenised_input)+\" Size: \"+str(tokenised_input.shape))\n","print(\"-\"*80)\n","print(\"Max out length: \"+str(max_out_length)+\" Size: \"+str(max_out_length.shape))\n","print(\"-\"*80)\n","print(\"Mask: \"+str(mask)+\" Size: \"+str(mask.shape))\n","print(\"-\"*80)\n","print(\"Tokenised Output: \"+str(tokenised_output)+\" Size: \"+str(tokenised_output.shape))\n","print(\"-\"*80)"],"execution_count":15,"outputs":[{"output_type":"stream","text":["Number of query-response pairs after all the preprocessing: 53113\n","Input length: tensor([12, 12, 12, 12, 12]) Size: torch.Size([5])\n","--------------------------------------------------------------------------------\n","Tokenised Input: tensor([[   1,  147,  582,    6,    2,    0,    0,    0,    0,    0,    0,    0],\n","        [   1,   61, 2626,   21,   56,   53, 1960,    4,    2,    0,    0,    0],\n","        [   1,   25,  387,  107, 1117,  117,   25,    6,    2,    0,    0,    0],\n","        [   1,    7,  509, 1999,   83, 1229,    6,    2,    0,    0,    0,    0],\n","        [   1,    7,   14,  195, 1937,    4,    2,    0,    0,    0,    0,    0]]) Size: torch.Size([5, 12])\n","--------------------------------------------------------------------------------\n","Max out length: tensor([12, 12, 12, 12, 12]) Size: torch.Size([5])\n","--------------------------------------------------------------------------------\n","Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]], dtype=torch.uint8) Size: torch.Size([5, 12])\n","--------------------------------------------------------------------------------\n","Tokenised Output: tensor([[   1,  269,  382,    4,    4,    4,    2,    0,    0,    0,    0,    0],\n","        [   1,   25,   24,    4,    5,  115,  101,   25,  215,    4,    2,    0],\n","        [   1,   56,  827,    7,   92,    4,    2,    0,    0,    0,    0,    0],\n","        [   1, 1999,   70, 4233,    7,  197,  117, 2952,   74,   66,    2,    0],\n","        [   1, 1117,  117,   27,   38,  682,    4, 1925,    6,    2,    0,    0]]) Size: torch.Size([5, 12])\n","--------------------------------------------------------------------------------\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"DpR-1oMt3HA4","colab_type":"text"},"source":["# Model 1"]},{"cell_type":"code","metadata":{"id":"RJnnpKgt3Nyg","colab_type":"code","colab":{}},"source":["class EncoderDecoder(nn.Module):\n","    \"\"\"\n","    A standard Encoder-Decoder architecture. Base for this and many \n","    other models.\n","    \"\"\"\n","    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n","        super(EncoderDecoder, self).__init__()\n","        self.encoder = encoder\n","        self.decoder = decoder\n","        self.src_embed = src_embed\n","        self.tgt_embed = tgt_embed\n","        self.generator = generator\n","        \n","    def forward(self, src, tgt, src_mask, tgt_mask):\n","        \"Take in and process masked src and target sequences.\"\n","        return self.decode(self.encode(src, src_mask), src_mask,\n","                            tgt, tgt_mask)\n","    \n","    def encode(self, src, src_mask):\n","        return self.encoder(self.src_embed(src), src_mask)\n","    \n","    def decode(self, memory, src_mask, tgt, tgt_mask):\n","        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cRE6Ls4k_lvE","colab_type":"code","colab":{}},"source":["class Generator(nn.Module):\n","    \"Define standard linear + softmax generation step.\"\n","    def __init__(self, d_model, vocab):\n","        super(Generator, self).__init__()\n","        self.proj = nn.Linear(d_model, vocab)\n","\n","    def forward(self, x):\n","        return self.proj(x)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aPiLhBPP_oQL","colab_type":"code","colab":{}},"source":["def clones(module, N):\n","    \"Produce N identical layers.\"\n","    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3ApjC3XD_quP","colab_type":"code","colab":{}},"source":["class Encoder(nn.Module):\n","    \"Core encoder is a stack of N layers\"\n","    def __init__(self, layer, N):\n","        super(Encoder, self).__init__()\n","        self.layers = clones(layer, N)\n","        self.norm = LayerNorm(layer.size)\n","        \n","    def forward(self, x, mask):\n","        \"Pass the input (and mask) through each layer in turn.\"\n","        for layer in self.layers:\n","            x = layer(x, mask)\n","        return self.norm(x)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5wUahzPb_s88","colab_type":"code","colab":{}},"source":["class LayerNorm(nn.Module):\n","    \"Construct a layernorm module (See citation for details).\"\n","    def __init__(self, features, eps=1e-6):\n","        super(LayerNorm, self).__init__()\n","        self.a_2 = nn.Parameter(torch.ones(features))\n","        self.b_2 = nn.Parameter(torch.zeros(features))\n","        self.eps = eps\n","\n","    def forward(self, x):\n","        mean = x.mean(-1, keepdim=True)\n","        std = x.std(-1, keepdim=True)\n","        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zowO7FHp_v5s","colab_type":"code","colab":{}},"source":["class SublayerConnection(nn.Module):\n","    \"\"\"\n","    A residual connection followed by a layer norm.\n","    Note for code simplicity the norm is first as opposed to last.\n","    \"\"\"\n","    def __init__(self, size, dropout):\n","        super(SublayerConnection, self).__init__()\n","        self.norm = LayerNorm(size)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x, sublayer):\n","        \"Apply residual connection to any sublayer with the same size.\"\n","        return x + self.dropout(sublayer(self.norm(x)))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_nkoHVj4_yiG","colab_type":"code","colab":{}},"source":["class EncoderLayer(nn.Module):\n","    \"Encoder is made up of self-attn and feed forward (defined below)\"\n","    def __init__(self, size, self_attn, feed_forward, dropout):\n","        super(EncoderLayer, self).__init__()\n","        self.self_attn = self_attn\n","        self.feed_forward = feed_forward\n","        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n","        self.size = size\n","\n","    def forward(self, x, mask):\n","        \"Follow Figure 1 (left) for connections.\"\n","        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n","        return self.sublayer[1](x, self.feed_forward)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BIJVFhXm_1FF","colab_type":"code","colab":{}},"source":["class Decoder(nn.Module):\n","    \"Generic N layer decoder with masking.\"\n","    def __init__(self, layer, N):\n","        super(Decoder, self).__init__()\n","        self.layers = clones(layer, N)\n","        self.norm = LayerNorm(layer.size)\n","        \n","    def forward(self, x, memory, src_mask, tgt_mask):\n","        for layer in self.layers:\n","            x = layer(x, memory, src_mask, tgt_mask)\n","        return self.norm(x)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZKn03mRp_3V2","colab_type":"code","colab":{}},"source":["class DecoderLayer(nn.Module):\n","    \"Decoder is made of self-attn, src-attn, and feed forward (defined below)\"\n","    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n","        super(DecoderLayer, self).__init__()\n","        self.size = size\n","        self.self_attn = self_attn\n","        self.src_attn = src_attn\n","        self.feed_forward = feed_forward\n","        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n"," \n","    def forward(self, x, memory, src_mask, tgt_mask):\n","        \"Follow Figure 1 (right) for connections.\"\n","        m = memory\n","        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n","        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n","        return self.sublayer[2](x, self.feed_forward)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KddgfVJx_6DC","colab_type":"code","colab":{}},"source":["def subsequent_mask(size):\n","    \"Mask out subsequent positions.\"\n","    attn_shape = (1, size, size)\n","    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n","    return torch.from_numpy(subsequent_mask) == 0"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BYWVMJoh_8-k","colab_type":"code","colab":{}},"source":["def attention(query, key, value, mask=None, dropout=None):\n","    \"Compute 'Scaled Dot Product Attention'\"\n","    d_k = query.size(-1)\n","    scores = torch.matmul(query, key.transpose(-2, -1)) \\\n","             / math.sqrt(d_k)\n","    if mask is not None:\n","        scores = scores.masked_fill(mask == 0, -1e9)\n","    p_attn = F.softmax(scores, dim = -1)\n","    if dropout is not None:\n","        p_attn = dropout(p_attn)\n","    return torch.matmul(p_attn, value), p_attn"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"veKkKGgb__q9","colab_type":"code","colab":{}},"source":["class MultiHeadedAttention(nn.Module):\n","    def __init__(self, h, d_model, dropout=0.1):\n","        \"Take in model size and number of heads.\"\n","        super(MultiHeadedAttention, self).__init__()\n","        assert d_model % h == 0\n","        # We assume d_v always equals d_k\n","        self.d_k = d_model // h\n","        self.h = h\n","        self.linears = clones(nn.Linear(d_model, d_model), 4)\n","        self.attn = None\n","        self.dropout = nn.Dropout(p=dropout)\n","        \n","    def forward(self, query, key, value, mask=None):\n","        \"Implements Figure 2\"\n","        if mask is not None:\n","            # Same mask applied to all h heads.\n","            mask = mask.unsqueeze(1)\n","        nbatches = query.size(0)\n","        \n","        # 1) Do all the linear projections in batch from d_model => h x d_k \n","        query, key, value = \\\n","            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n","             for l, x in zip(self.linears, (query, key, value))]\n","        \n","        # 2) Apply attention on all the projected vectors in batch. \n","        x, self.attn = attention(query, key, value, mask=mask, \n","                                 dropout=self.dropout)\n","        \n","        # 3) \"Concat\" using a view and apply a final linear. \n","        x = x.transpose(1, 2).contiguous() \\\n","             .view(nbatches, -1, self.h * self.d_k)\n","        return self.linears[-1](x)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aSqvuoJVAE_i","colab_type":"code","colab":{}},"source":["class PositionwiseFeedForward(nn.Module):\n","    \"Implements FFN equation.\"\n","    def __init__(self, d_model, d_ff, dropout=0.1):\n","        super(PositionwiseFeedForward, self).__init__()\n","        self.w_1 = nn.Linear(d_model, d_ff)\n","        self.w_2 = nn.Linear(d_ff, d_model)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        return self.w_2(self.dropout(F.relu(self.w_1(x))))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ft2ByV62AHlf","colab_type":"code","colab":{}},"source":["class Embeddings(nn.Module):\n","    def __init__(self, d_model, vocab):\n","        super(Embeddings, self).__init__()\n","        self.lut = nn.Embedding(vocab, d_model)\n","        self.d_model = d_model\n","\n","    def forward(self, x):\n","        return self.lut(x) * math.sqrt(self.d_model)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"flsGFWNWAKso","colab_type":"code","colab":{}},"source":["class PositionalEncoding(nn.Module):\n","    \"Implement the PE function.\"\n","    def __init__(self, d_model, dropout, max_len=5000):\n","        super(PositionalEncoding, self).__init__()\n","        self.dropout = nn.Dropout(p=dropout)\n","        \n","        # Compute the positional encodings once in log space.\n","        pe = torch.zeros(max_len, d_model)\n","        position = torch.arange(0, max_len).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2) *\n","                             -(math.log(10000.0) / d_model))\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","        pe = pe.unsqueeze(0)\n","        self.register_buffer('pe', pe)\n","        \n","    def forward(self, x):\n","        x = x + Variable(self.pe[:, :x.size(1)], \n","                         requires_grad=False)\n","        return self.dropout(x)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"veyYVrUlANvh","colab_type":"code","colab":{}},"source":["def make_model(src_vocab, tgt_vocab, N=6, \n","               d_model=512, d_ff=2048, h=8, dropout=0.1):\n","    \"Helper: Construct a model from hyperparameters.\"\n","    c = copy.deepcopy\n","    attn = MultiHeadedAttention(h, d_model)\n","    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n","    position = PositionalEncoding(d_model, dropout)\n","    model = EncoderDecoder(\n","        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n","        Decoder(DecoderLayer(d_model, c(attn), c(attn), \n","                             c(ff), dropout), N),\n","        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),\n","        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),\n","        Generator(d_model, tgt_vocab))\n","    \n","    # This was important from their code. \n","    # Initialize parameters with Glorot / fan_avg.\n","    for p in model.parameters():\n","        if p.dim() > 1:\n","            nn.init.xavier_uniform_(p)\n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v5xV-42LAX6d","colab_type":"text"},"source":["# Model 2"]},{"cell_type":"code","metadata":{"id":"uHIbkFgbAhpa","colab_type":"code","colab":{}},"source":["class EncoderDecoder(nn.Module):\n","    \n","    def __init__(self,encoder,decoder,source_embed,target_embed,generator):\n","        super().__init__()\n","        \n","        self.encoder=encoder\n","        self.decoder=decoder\n","        \n","        self.source_embed=source_embed\n","        self.target_embed=target_embed\n","        \n","        self.generator=generator # Linear + Log_softmax\n","        \n","    def forward(self,source,target,source_mask,target_mask):\n","        return self.decode(self.encode(source,source_mask),source_mask,target,target_mask)\n","    \n","    def encode(self,source,source_mask):\n","        return self.encoder(self.source_embed(source),source_mask)\n","    \n","    def decode(self,memory, source_mask,target,target_mask):\n","        return self.decoder(self.target_embed(target),memory,source_mask,target_mask)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ewcrG0AeCHQc","colab_type":"code","colab":{}},"source":["class Generator(nn.Module):\n","    \n","    def __init__(self,d_model,vocab_size):\n","        super().__init__()\n","        self.projection=nn.Linear(d_model,vocab_size)\n","        \n","    def forward(self,decoder_output):\n","        return F.log_softmax(self.projection(decoder_output),dim=-1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jNIgAbiYCKg0","colab_type":"code","colab":{}},"source":["def clones(module,N):\n","    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qgHHuuzYCNL8","colab_type":"code","colab":{}},"source":["class Encoder(nn.Module):\n","    \n","    def __init__(self,layer,N):\n","        super().__init__()\n","        \n","        self.layers=clones(layer,N)\n","        self.norm=LayerNorm(layer.size)\n","    \n","    def forward(self,x,mask):\n","        \n","        for layer in self.layers:\n","            x=layer(x,mask)\n","        \n","        return self.norm(x)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dQRX2bAMCPlO","colab_type":"code","colab":{}},"source":["class LayerNorm(nn.Module):\n","    \n","    def __init__(self,features,eps=1e-6):\n","        super().__init__()\n","        self.a_2=nn.Parameter(torch.ones(features))\n","        self.b_2=nn.Parameter(torch.zeros(features))\n","        self.eps=eps\n","        \n","    def forward(self,x):\n","        mean=x.mean(-1,keepdim=True)\n","        std=x.std(-1,keepdim=True)\n","        return self.a_2*(x-mean)/(std+self.eps)+self.b_2"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-sIDH01lCSsv","colab_type":"code","colab":{}},"source":["class SublayerConnection(nn.Module):\n","    \n","    def __init__(self,size,dropout):\n","        super().__init__()\n","        \n","        self.dropout=nn.Dropout(dropout)\n","        self.norm=LayerNorm(size)\n","        \n","    def forward(self,x,sublayer):\n","        return x+self.dropout(sublayer(self.norm(x)))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1zQ_LsoCCVpO","colab_type":"code","colab":{}},"source":["class EncoderLayer(nn.Module):\n","    def __init__(self,size,self_attn,feed_forward,dropout):\n","        super().__init__()\n","        \n","        self.attn=self_attn\n","        self.feed_forward=feed_forward\n","        self.sublayer=clones(SublayerConnection(size,dropout),2)\n","        self.size=size\n","        \n","    def forward(self,x,mask):\n","        \n","        x=self.sublayer[0](x,lambda x: self.attn(x,x,x,mask))\n","        return self.sublayer[1](x,self.feed_forward)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cZjOa0vwCY5H","colab_type":"code","colab":{}},"source":["class Decoder(nn.Module):\n","    \n","    def __init__(self,layer,N):\n","        super().__init__()\n","        \n","        self.layers=clones(layer,N)\n","        self.norm=LayerNorm(layer.size)\n","    \n","    def forward(self,x,memory,curr_mask,tgt_mask):\n","        \n","        for layer in self.layers:\n","            x=layer(x,memory,curr_mask,tgt_mask)\n","            \n","        return self.norm(x)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FDHFdRvRCbgt","colab_type":"code","colab":{}},"source":["class DecoderLayer(nn.Module):\n","    \n","    def __init__(self,size,self_attn,src_attn,feed_forward,dropout):\n","        super().__init__()\n","        \n","        self.size=size\n","        self.self_attn=self_attn\n","        self.src_attn=src_attn\n","        self.feed_forward=feed_forward\n","        \n","        self.sublayer=clones(SublayerConnection(size,dropout),3)\n","        \n","    def forward(self,x,memory,src_mask,tgt_mask):\n","        \n","        m=memory\n","        x=self.sublayer[0](x,lambda x:self.self_attn(x,x,x,tgt_mask))\n","        x=self.sublayer[1](x,lambda x: self.src_attn(x,m,m,src_mask))\n","        return self.sublayer[2](x,self.feed_forward)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OWbh8piBCeC_","colab_type":"code","colab":{}},"source":["def attention(query,key,value,mask=None,dropout=None):\n","    \n","    d_k=query.size(-1)\n","\n","    scores=torch.matmul(query,key.transpose(-2,-1))/math.sqrt(d_k)\n","    \n","    if mask is not None:\n","        scores=scores.masked_fill(mask==0,-1e9)\n","        \n","    p_attn=F.softmax(scores,dim=-1)\n","    \n","    if dropout is not None:\n","        p_attn=dropout(p_attn)\n","        \n","    return torch.matmul(p_attn,value),p_attn"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZtHuOzFTCgie","colab_type":"code","colab":{}},"source":["class MultiHeadedAttention(nn.Module):\n","    \n","    def __init__(self,h,d_model,dropout=0.1):\n","        super().__init__()\n","        \n","        assert d_model%h==0\n","        \n","        self.d_k=d_model//h\n","        self.h=h\n","        self.linears=clones(nn.Linear(d_model,d_model),4)\n","        self.attn=None\n","        self.dropout=nn.Dropout(dropout)\n","        \n","    def forward(self,query,key,values,mask=None):\n","        \n","        if mask is not None:\n","            mask=mask.unsqueeze(1)\n","            \n","        nbatches=query.size(0)\n","        \n","        query,key,values=[l(x).view(nbatches,-1,self.h,self.d_k).transpose(1,2) for l, x in zip(self.linears,(query,key,values))]\n","        \n","        x,self.attn=attention(query,key,values,mask=mask,dropout=self.dropout)\n","        \n","        x=x.transpose(1,2).contiguous().view(nbatches,-1,self.h*self.d_k)\n","        \n","        return self.linears[-1](x)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GD9vxJrECkSp","colab_type":"code","colab":{}},"source":["class PositionwiseFeedForward(nn.Module):\n","    \n","    def __init__(self,d_model,d_ff,dropout=0.1):\n","        super().__init__()\n","        \n","        self.w_1=nn.Linear(d_model,d_ff)\n","        self.w_2=nn.Linear(d_ff,d_model)\n","        self.dropout=nn.Dropout(dropout)\n","        \n","    def forward(self,x):\n","        return self.w_2(self.dropout(F.relu(self.w_1(x))))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8Qna2tDeCmue","colab_type":"code","colab":{}},"source":["class Embeddings(nn.Module):\n","    \n","    def __init__(self,d_model,vocab):\n","        super().__init__()\n","        \n","        self.embed=nn.Embedding(vocab,d_model)\n","        self.d_model=d_model\n","    \n","    def forward(self,x):\n","#         print(x.device)\n","        return self.embed(x)*math.sqrt(self.d_model)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nCEQ9n0XCo7b","colab_type":"code","colab":{}},"source":["class PositionalEncoding(nn.Module):\n","    \n","    def __init__(self,d_model,dropout,max_len=5000):\n","        super().__init__()\n","        \n","        self.dropout=nn.Dropout(dropout)\n","        pe=torch.zeros(max_len,d_model,dtype=torch.float)\n","        position=torch.arange(0.,max_len).unsqueeze(1)\n","        div_term=torch.exp(torch.arange(0.,d_model,2)*-(math.log(10000.0)/d_model))\n","        \n","        pe[:,0::2]=torch.sin(position*div_term)\n","        pe[:,1::2]=torch.cos(position*div_term)\n","        \n","        pe=pe.unsqueeze(0)\n","        self.register_buffer('pe',pe)\n","        \n","    def forward(self,x):\n","        \n","        x=x+Variable(self.pe[:,:x.size(1)],requires_grad=False)\n","        return self.dropout(x)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CkLif2nLC5U6","colab_type":"code","colab":{}},"source":["\"\"\"\n","triu function generates a copy of matrix with elemens below kth diagonal zeroed.\n","The main diagonal is zeroeth diagonal above is first(k=1) and so on.\n","\n","Eg:\n","A=[[1,2,3],[4,5,6],[7,8,9]]\n","for above matrix:\n","triu(A,k=1)\n","will give [[0,2,3],[0,0,6],[0,0,0]]\n","\"\"\"\n","\n","def subsequent_mask(size):\n","    attn_shape=(1,size,size)\n","    mask=np.triu(np.ones(attn_shape),k=1).astype('uint8')\n","    \n","    return torch.from_numpy(mask)==0"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WPOL8rdMCruh","colab_type":"code","colab":{}},"source":["def make_model2(src_vocab,tgt_vocab,N=6,d_model=512,d_ff=2048,h=8,dropout=0.1):\n","    \n","    c=copy.deepcopy\n","    attn=MultiHeadedAttention(h,d_model)\n","    ff=PositionwiseFeedForward(d_model,d_ff,dropout)\n","    position=PositionalEncoding(d_model,dropout)\n","    model=EncoderDecoder(Encoder(EncoderLayer(d_model,c(attn),c(ff),dropout),N),\n","                        Decoder(DecoderLayer(d_model,c(attn),c(attn),c(ff),dropout),N),\n","                        nn.Sequential(Embeddings(d_model,src_vocab),c(position)),\n","                        nn.Sequential(Embeddings(d_model,tgt_vocab),c(position)),\n","                        Generator(d_model,tgt_vocab))\n","    \n","    for p in model.parameters():\n","        if p.dim()>1:\n","            nn.init.xavier_uniform_(p)\n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"K6TsrSL5CuNH","colab_type":"code","outputId":"34069e92-eab5-45ab-88c8-91d25c463e1b","executionInfo":{"status":"ok","timestamp":1586270368434,"user_tz":-330,"elapsed":10181,"user":{"displayName":"deepak goyal","photoUrl":"https://lh5.googleusercontent.com/-3gfEZruD9Sk/AAAAAAAAAAI/AAAAAAAACvg/54H4lMsdOgA/s64/photo.jpg","userId":"05164064759516400423"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["sample_model=make_model2(voc.num_words,voc.num_words,1,512,2048,8,0.1)\n","sample_model.to(device)\n","# print(sample_model)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["EncoderDecoder(\n","  (encoder): Encoder(\n","    (layers): ModuleList(\n","      (0): EncoderLayer(\n","        (attn): MultiHeadedAttention(\n","          (linears): ModuleList(\n","            (0): Linear(in_features=512, out_features=512, bias=True)\n","            (1): Linear(in_features=512, out_features=512, bias=True)\n","            (2): Linear(in_features=512, out_features=512, bias=True)\n","            (3): Linear(in_features=512, out_features=512, bias=True)\n","          )\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (sublayer): ModuleList(\n","          (0): SublayerConnection(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (norm): LayerNorm()\n","          )\n","          (1): SublayerConnection(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (norm): LayerNorm()\n","          )\n","        )\n","      )\n","    )\n","    (norm): LayerNorm()\n","  )\n","  (decoder): Decoder(\n","    (layers): ModuleList(\n","      (0): DecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linears): ModuleList(\n","            (0): Linear(in_features=512, out_features=512, bias=True)\n","            (1): Linear(in_features=512, out_features=512, bias=True)\n","            (2): Linear(in_features=512, out_features=512, bias=True)\n","            (3): Linear(in_features=512, out_features=512, bias=True)\n","          )\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (src_attn): MultiHeadedAttention(\n","          (linears): ModuleList(\n","            (0): Linear(in_features=512, out_features=512, bias=True)\n","            (1): Linear(in_features=512, out_features=512, bias=True)\n","            (2): Linear(in_features=512, out_features=512, bias=True)\n","            (3): Linear(in_features=512, out_features=512, bias=True)\n","          )\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (sublayer): ModuleList(\n","          (0): SublayerConnection(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (norm): LayerNorm()\n","          )\n","          (1): SublayerConnection(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (norm): LayerNorm()\n","          )\n","          (2): SublayerConnection(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (norm): LayerNorm()\n","          )\n","        )\n","      )\n","    )\n","    (norm): LayerNorm()\n","  )\n","  (source_embed): Sequential(\n","    (0): Embeddings(\n","      (embed): Embedding(7816, 512)\n","    )\n","    (1): PositionalEncoding(\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","  )\n","  (target_embed): Sequential(\n","    (0): Embeddings(\n","      (embed): Embedding(7816, 512)\n","    )\n","    (1): PositionalEncoding(\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","  )\n","  (generator): Generator(\n","    (projection): Linear(in_features=512, out_features=7816, bias=True)\n","  )\n",")"]},"metadata":{"tags":[]},"execution_count":32}]},{"cell_type":"code","metadata":{"id":"TZ0e9Y5cCwzw","colab_type":"code","outputId":"92dc2f78-b725-4b00-f1fe-c5d77264e95e","executionInfo":{"status":"ok","timestamp":1586270375254,"user_tz":-330,"elapsed":1026,"user":{"displayName":"deepak goyal","photoUrl":"https://lh5.googleusercontent.com/-3gfEZruD9Sk/AAAAAAAAAAI/AAAAAAAACvg/54H4lMsdOgA/s64/photo.jpg","userId":"05164064759516400423"}},"colab":{"base_uri":"https://localhost:8080/","height":69}},"source":["#Sample Run\n","source=torch.ones(5,12,dtype=torch.long,device=device)\n","target=torch.ones(5,12,dtype=torch.long,device=device)\n","source_mask=torch.ones(5,12,12,dtype=torch.long,device=device)\n","target_mask=torch.ones(5,12,12,dtype=torch.long,device=device)\n","out=sample_model(source,target,source_mask,target_mask)\n","print(\"-\"*80)\n","print(\"Output size: \"+str(out.shape))\n","print(\"-\"*80)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["--------------------------------------------------------------------------------\n","Output size: torch.Size([5, 12, 512])\n","--------------------------------------------------------------------------------\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Hb-TFfrCDn_q","colab_type":"text"},"source":["# Training\n"]},{"cell_type":"code","metadata":{"id":"EBjAhtyS16_t","colab_type":"code","colab":{}},"source":["def F1_score(x,y):\n","#     print(x)\n","#     print(y)\n","    inp=torch.argmax(x,dim=-1)\n","    score=(inp==y).sum()\n","    score=score/(x.size()[0]*x.size()[1])\n","    return score"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KBz2k8ywDqZ4","colab_type":"code","colab":{}},"source":["def data_generation(pairs,batch_size,n_batches):\n","    \n","    # sample_batches=[batch2TrainData(voc,[random.choice(pairs) for _ in range(batch_size)]) for _ in range(n_batches)]\n","    sample_batches=[]\n","    batches=[]\n","    for i in range(n_batches):\n","        curr_batch=[]\n","        for j in range(batch_size):\n","            curr_id=i*batch_size+j\n","            curr_batch.append(pairs[curr_id])\n","\n","        sample_batches.append(batch2TrainData(voc,curr_batch))\n","\n","    \n","    for i in range(n_batches):\n","        batches.append(Batch(sample_batches[i][1],sample_batches[i][-1]))\n","    \n","    return batches"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"upPkHvWXECA2","colab_type":"code","colab":{}},"source":["class Batch:\n","    \"Object for holding a batch of data with mask during training.\"\n","    def __init__(self, src, trg=None, pad=0):\n","        src=src.to(torch.int64)\n","        trg=trg.to(torch.int64)\n","        self.src = src\n","        self.src_mask = (src != pad).unsqueeze(-2)\n","        if trg is not None:\n","            self.trg = trg[:, :-1]\n","            self.trg_y = trg[:, 1:]\n","            self.trg_mask = \\\n","                self.make_std_mask(self.trg, pad)\n","            self.ntokens = (self.trg_y != pad).data.sum()\n","        self.src.to(device)\n","        self.trg.to(device)\n","        self.src_mask.to(device)\n","        self.trg_mask.to(device)\n","    \n","    @staticmethod\n","    def make_std_mask(tgt, pad):\n","        \"Create a mask to hide padding and future words.\"\n","        tgt_mask = (tgt != pad).unsqueeze(-2)\n","        tgt_mask = tgt_mask & Variable(\n","            subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data))\n","        return tgt_mask"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YRV3rCOGEMFn","colab_type":"code","colab":{}},"source":["def run_single_batch(data,model,loss_compute):\n","    \n","    start_time=time.time()\n","    \n","    total_tokens=0\n","    total_loss=0\n","    tokens=0\n","    \n","    source=data.src\n","    source_mask=data.src_mask\n","    target=data.trg\n","    target_mask=data.trg_mask\n","    target_y=data.trg_y\n","    \n","    source=source.to(device)\n","    target=target.to(device)\n","    source_mask=source_mask.to(device)\n","    target_mask=target_mask.to(device)\n","    target_y=target_y.to(device)\n","\n","    out=model(source,target,source_mask,target_mask)\n","\n","    loss,f1_score,ppl=loss_compute(out,target_y,data.ntokens)\n","    \n","    return loss.item(),f1_score.item(),data.ntokens.item(),ppl"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"a5Kfbj2TEXyp","colab_type":"code","colab":{}},"source":["\n","class SimpleLossCompute:\n","    \"A simple loss compute and train function.\"\n","    def __init__(self, generator, opt=None):\n","        self.generator = generator\n","        self.criterion = nn.CrossEntropyLoss()\n","        self.opt = opt\n","        \n","    def __call__(self, x, y, norm):\n","        x = self.generator(x)\n","        sentence_length=x.size()[1]\n","#         print(str(x.size())+\" \"+str(y.size()))\n","        f1_score=F1_score(x,y)\n","#         ppl=perplexity(x,y)\n","        \n","        loss = self.criterion(x.contiguous().view(-1, x.size(-1)), \n","                              y.contiguous().view(-1)) \n","        loss.backward()\n","        _=nn.utils.clip_grad_norm_(model.parameters(),1.0)\n","        # plot_grad_flow(model.named_parameters())\n","        if self.opt is not None:\n","            self.opt.step()\n","            self.opt.zero_grad()\n","            \n","        return loss,f1_score,math.exp((loss.item()*norm.item())/sentence_length)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FixxgorW2fkQ","colab_type":"code","colab":{}},"source":["def training(batches,model,n_epochs,n_batches,model_opt,loadFile,save_every):\n","    \n","    start_epoch=0\n","    total_time=0\n","    if loadFile:\n","        start_epoch=torch.load(loadFile)[\"epoch\"]\n","        total_time=torch.load(loadFile)[\"time\"]\n","        start_epoch=start_epoch+1\n","    \n","    for epoch in range(start_epoch,n_epochs):\n","        t1=time.time()\n","        loss=0\n","        f1_score=0\n","        ppl=0\n","        n_tokens=0\n","        for i in range(n_batches):\n","            current_batch=batches[i]\n","            loss_val,current_f1_score,current_tokens,curr_ppl=run_single_batch(current_batch,model,SimpleLossCompute(model.generator, model_opt))\n","            loss+=loss_val\n","            f1_score+=current_f1_score\n","            n_tokens+=current_tokens\n","            ppl+=curr_ppl\n","\n","        loss=loss/(n_batches)\n","        ppl=ppl/n_batches\n","        f1_score=f1_score/n_batches\n","        \n","        if epoch%save_every==0:\n","            directory=os.path.join(save_dir,'transformer','cornell-movie')\n","            if not os.path.exists(directory):\n","                os.makedirs(directory)\n","            torch.save({\n","                \"epoch\":epoch,\n","                \"model\":model.state_dict(),\n","                \"opt\":model_opt.state_dict(),\n","                \"loss\":loss,\n","                \"ppl\":ppl,\n","                \"f1\":f1_score,\n","                \"time\":total_time\n","            },os.path.join(directory,'{}_{}.tar'.format(epoch,'checkpoint')))\n","        \n","        print(\"=\"*100)\n","        print(\"| End of Epoch : \"+str(epoch)+\"| Loss Value: \"+str(loss)+\"| F1 Score: \"+str(f1_score/n_batches)+\"| PPL: \"+str(ppl)+\"| Time Took: \"+\n","              str(time.time()-t1)+\" |\")\n","        print(\"=\"*100)\n","        total_time+=time.time()-t1\n","\n","    print(\"| Training Finished | Total Training Time: \"+str(total_time)+\" |\")\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kwbJnrp1V-R8","colab_type":"code","colab":{}},"source":["def get_parameter_count(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8vivb-Oon46j","colab_type":"code","colab":{}},"source":["### Hyperparameters\n","\n","N=2\n","d_model=256\n","d_ff=512\n","num_head=8\n","dropout=0.1\n","\n","batch_size=10\n","n_epochs=1000"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ez4Swx52EfUT","colab_type":"code","outputId":"a737e1b6-eea6-47b8-c687-96d3cb9b0638","executionInfo":{"status":"ok","timestamp":1586534693198,"user_tz":-330,"elapsed":1838,"user":{"displayName":"Deepak Goyal","photoUrl":"","userId":"12155914074048018284"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["batches=data_generation(pairs,10,1000)\n","print(len(batches))\n","# for i in range(5):\n","#     print(str(batches[i].src)+\" \"+str(batches[i].trg))\n"],"execution_count":69,"outputs":[{"output_type":"stream","text":["1000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"b67TIlqdEj-O","colab_type":"code","outputId":"73ba810d-5058-4209-ecda-fd572c82223a","executionInfo":{"status":"ok","timestamp":1586538975058,"user_tz":-330,"elapsed":3200246,"user":{"displayName":"Deepak Goyal","photoUrl":"","userId":"12155914074048018284"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["print(\"Initialising and creating models....\")\n","V=voc.num_words\n","t1=time.time()\n","# criterion=LabelSmoothing()\n","\n","model=make_model(V,V,N,d_model,d_ff,num_head,dropout)\n","model.to(device)\n","\n","learnable_parameter_count=get_parameter_count(model)\n","\n","print(\"Number of learnable parameters for this model: \"+str(learnable_parameter_count))\n","\n","# model_opt=torch.optim.Adam(model.parameters(),lr=0.0001,betas=(0.9,0.988),eps=1e-9)\n","model_opt = torch.optim.Adam(model.parameters(), lr=0.00001, betas=(0.9, 0.98), eps=1e-9)\n","print(\"=\"*100)\n","print(\"Creating Models took: \"+str(time.time()-t1))\n","\n","save_dir='/content/drive/My Drive/Model Data'\n","# loadFile=os.path.join(save_dir,'transformer','cornell-movie','2_checkpoint')\n","# loadFile=\"C:\\\\Users\\\\deepa\\\\Conversational Agents\\\\transformer\\\\cornell-movie\\\\1_checkpoint.tar\"\n","loadFile='/content/drive/My Drive/Model Data/transformer/cornell-movie/20_checkpoint.tar'\n","# loadFile=None\n","\n","if(loadFile):\n","    checkpoint=torch.load(loadFile)\n","    model.load_state_dict(checkpoint['model'])\n","    model_opt.load_state_dict(checkpoint['opt'])\n","    \n","    \n","\n","model.train()\n","training(batches,model,101,1000,model_opt,loadFile,5)"],"execution_count":72,"outputs":[{"output_type":"stream","text":["Initialising and creating models....\n","Number of learnable parameters for this model: 8647304\n","====================================================================================================\n","Creating Models took: 0.14748167991638184\n","====================================================================================================\n","| End of Epoch : 21| Loss Value: 2.111442457675934| F1 Score: 0.0| PPL: 467048615.04927766| Time Took: 41.41088271141052 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 22| Loss Value: 2.1022402947545054| F1 Score: 0.0| PPL: 468507095.9746668| Time Took: 41.909711837768555 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 23| Loss Value: 2.0930760363936423| F1 Score: 0.0| PPL: 374971667.38614833| Time Took: 41.37411642074585 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 24| Loss Value: 2.083487618267536| F1 Score: 0.0| PPL: 386626376.4529959| Time Took: 40.22255563735962 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 25| Loss Value: 2.0745881590247155| F1 Score: 0.0| PPL: 326817930.92423713| Time Took: 40.85902523994446 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 26| Loss Value: 2.068549079298973| F1 Score: 0.0| PPL: 287996649.1611418| Time Took: 40.13608407974243 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 27| Loss Value: 2.0593805589079857| F1 Score: 0.0| PPL: 280307689.9379986| Time Took: 39.820067405700684 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 28| Loss Value: 2.053154782831669| F1 Score: 0.0| PPL: 255389364.61167482| Time Took: 39.46518564224243 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 29| Loss Value: 2.045654865384102| F1 Score: 0.0| PPL: 238820310.79332703| Time Took: 39.97571659088135 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 30| Loss Value: 2.0372073732614515| F1 Score: 0.0| PPL: 223293409.30829817| Time Took: 39.006651163101196 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 31| Loss Value: 2.0301353014707564| F1 Score: 0.0| PPL: 187466507.75111035| Time Took: 39.89063572883606 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 32| Loss Value: 2.0239891807436945| F1 Score: 0.0| PPL: 166603812.12483454| Time Took: 40.03754472732544 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 33| Loss Value: 2.0175964441299437| F1 Score: 0.0| PPL: 161961257.811106| Time Took: 39.40377426147461 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 34| Loss Value: 2.0098636053800583| F1 Score: 0.0| PPL: 144415651.94513488| Time Took: 39.32034230232239 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 35| Loss Value: 2.0039832743406296| F1 Score: 0.0| PPL: 159104861.03717446| Time Took: 39.54851746559143 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 36| Loss Value: 1.998963796377182| F1 Score: 0.0| PPL: 136242720.4802457| Time Took: 39.38814449310303 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 37| Loss Value: 1.9915392495393753| F1 Score: 0.0| PPL: 124617812.43593407| Time Took: 39.92632222175598 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 38| Loss Value: 1.9856160646677017| F1 Score: 0.0| PPL: 112844521.58347832| Time Took: 39.88622832298279 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 39| Loss Value: 1.9789293648600579| F1 Score: 0.0| PPL: 107832106.6623156| Time Took: 40.1127655506134 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 40| Loss Value: 1.9735730759501457| F1 Score: 0.0| PPL: 96985737.56616665| Time Took: 40.6770555973053 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 41| Loss Value: 1.9693709762096405| F1 Score: 0.0| PPL: 116349009.17382035| Time Took: 40.99012780189514 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 42| Loss Value: 1.9626334960460663| F1 Score: 0.0| PPL: 101675932.74163245| Time Took: 40.20260167121887 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 43| Loss Value: 1.9575215438008309| F1 Score: 0.0| PPL: 73381301.2870582| Time Took: 40.800945520401 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 44| Loss Value: 1.9517870346307755| F1 Score: 0.0| PPL: 65483522.560317144| Time Took: 40.15533709526062 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 45| Loss Value: 1.945961722791195| F1 Score: 0.0| PPL: 86630270.47723874| Time Took: 39.91487002372742 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 46| Loss Value: 1.941563535630703| F1 Score: 0.0| PPL: 71476698.16861033| Time Took: 39.71946334838867 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 47| Loss Value: 1.9366034827828407| F1 Score: 0.0| PPL: 63188235.47653596| Time Took: 38.93654704093933 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 48| Loss Value: 1.930783708870411| F1 Score: 0.0| PPL: 78726660.92287822| Time Took: 40.17599391937256 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 49| Loss Value: 1.9270329173803329| F1 Score: 0.0| PPL: 64640198.204158634| Time Took: 39.94796323776245 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 50| Loss Value: 1.9215547968745232| F1 Score: 0.0| PPL: 54206302.23958513| Time Took: 40.70023226737976 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 51| Loss Value: 1.917128144979477| F1 Score: 0.0| PPL: 49093958.18422056| Time Took: 39.96959447860718 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 52| Loss Value: 1.9112619988322257| F1 Score: 0.0| PPL: 41889161.548497155| Time Took: 40.24248385429382 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 53| Loss Value: 1.90479476583004| F1 Score: 0.0| PPL: 47671364.57179387| Time Took: 40.33196520805359 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 54| Loss Value: 1.9002746452093124| F1 Score: 0.0| PPL: 53031455.31242414| Time Took: 39.78139400482178 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 55| Loss Value: 1.895934776008129| F1 Score: 0.0| PPL: 44341886.96064791| Time Took: 40.044774770736694 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 56| Loss Value: 1.893332776606083| F1 Score: 0.0| PPL: 37287581.56837131| Time Took: 39.050098180770874 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 57| Loss Value: 1.888919199168682| F1 Score: 0.0| PPL: 46859729.12944601| Time Took: 39.99533295631409 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 58| Loss Value: 1.882949074268341| F1 Score: 0.0| PPL: 32196829.106608424| Time Took: 39.807870864868164 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 59| Loss Value: 1.8782527620196343| F1 Score: 0.0| PPL: 40602375.5016218| Time Took: 39.58738851547241 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 60| Loss Value: 1.8742768181562424| F1 Score: 0.0| PPL: 26937322.72613678| Time Took: 40.216992139816284 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 61| Loss Value: 1.8706746940016747| F1 Score: 0.0| PPL: 35885889.05408065| Time Took: 40.143898725509644 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 62| Loss Value: 1.8642608168125152| F1 Score: 0.0| PPL: 39833866.90427786| Time Took: 39.55555057525635 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 63| Loss Value: 1.8604339439868927| F1 Score: 0.0| PPL: 33123111.29513659| Time Took: 40.041335344314575 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 64| Loss Value: 1.8563845411539077| F1 Score: 0.0| PPL: 28986434.092527483| Time Took: 40.30697321891785 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 65| Loss Value: 1.8526825870871544| F1 Score: 0.0| PPL: 24677424.92015181| Time Took: 40.4689884185791 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 66| Loss Value: 1.8480674549937248| F1 Score: 0.0| PPL: 24477726.91420825| Time Took: 39.60297751426697 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 67| Loss Value: 1.8444585384130479| F1 Score: 0.0| PPL: 29380303.392519| Time Took: 39.44556999206543 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 68| Loss Value: 1.8391990737915038| F1 Score: 0.0| PPL: 24821249.781341325| Time Took: 40.972023010253906 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 69| Loss Value: 1.833521472275257| F1 Score: 0.0| PPL: 24377283.011881117| Time Took: 40.25504660606384 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 70| Loss Value: 1.8323672965764999| F1 Score: 0.0| PPL: 20167992.369587578| Time Took: 40.61816596984863 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 71| Loss Value: 1.8267570139169693| F1 Score: 0.0| PPL: 22338777.69214209| Time Took: 41.18693780899048 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 72| Loss Value: 1.824227689921856| F1 Score: 0.0| PPL: 21083845.333564233| Time Took: 40.33232545852661 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 73| Loss Value: 1.8177655137181281| F1 Score: 0.0| PPL: 17227899.860508006| Time Took: 40.80170011520386 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 74| Loss Value: 1.814940161526203| F1 Score: 0.0| PPL: 19230550.675531566| Time Took: 40.4481258392334 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 75| Loss Value: 1.8104269681572913| F1 Score: 0.0| PPL: 18080895.75055376| Time Took: 40.461331605911255 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 76| Loss Value: 1.8057248377799988| F1 Score: 0.0| PPL: 17255266.703494694| Time Took: 40.12374234199524 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 77| Loss Value: 1.8027514032721519| F1 Score: 0.0| PPL: 15664657.426534416| Time Took: 36.95486831665039 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 78| Loss Value: 1.7976728959083557| F1 Score: 0.0| PPL: 13713737.371708328| Time Took: 37.998621702194214 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 79| Loss Value: 1.7935326220393182| F1 Score: 0.0| PPL: 17038999.632904295| Time Took: 39.45976281166077 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 80| Loss Value: 1.7879272252321243| F1 Score: 0.0| PPL: 12584517.843880542| Time Took: 38.45353841781616 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 81| Loss Value: 1.7868620439171792| F1 Score: 0.0| PPL: 14677570.96398028| Time Took: 37.44835638999939 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 82| Loss Value: 1.7797088142037392| F1 Score: 0.0| PPL: 12315263.871700425| Time Took: 39.813023805618286 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 83| Loss Value: 1.7776475337147712| F1 Score: 0.0| PPL: 12855747.810700258| Time Took: 40.83404779434204 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 84| Loss Value: 1.7746681364774703| F1 Score: 0.0| PPL: 10047206.196730472| Time Took: 40.541560649871826 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 85| Loss Value: 1.7711602228283883| F1 Score: 0.0| PPL: 12060709.774557762| Time Took: 40.94476318359375 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 86| Loss Value: 1.7667678954005241| F1 Score: 0.0| PPL: 12386844.613162158| Time Took: 40.08657455444336 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 87| Loss Value: 1.7618737187981606| F1 Score: 0.0| PPL: 10817483.012198813| Time Took: 39.39873266220093 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 88| Loss Value: 1.7594927586317062| F1 Score: 0.0| PPL: 9639430.013962593| Time Took: 40.2395658493042 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 89| Loss Value: 1.7565642300248145| F1 Score: 0.0| PPL: 9043507.172461675| Time Took: 39.331443071365356 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 90| Loss Value: 1.7509420654177665| F1 Score: 0.0| PPL: 10521139.64891219| Time Took: 39.85838842391968 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 91| Loss Value: 1.7472236108779908| F1 Score: 0.0| PPL: 8828031.287951712| Time Took: 40.332642555236816 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 92| Loss Value: 1.7415383983850479| F1 Score: 0.0| PPL: 8664101.877442123| Time Took: 39.796221017837524 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 93| Loss Value: 1.7390939937233925| F1 Score: 0.0| PPL: 6326938.35974956| Time Took: 39.03425335884094 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 94| Loss Value: 1.736373791873455| F1 Score: 0.0| PPL: 7223728.998309792| Time Took: 39.9708468914032 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 95| Loss Value: 1.7308190662264824| F1 Score: 0.0| PPL: 6925432.144171721| Time Took: 40.068289041519165 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 96| Loss Value: 1.7273864005208015| F1 Score: 0.0| PPL: 7002252.549879619| Time Took: 40.10601615905762 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 97| Loss Value: 1.724647541463375| F1 Score: 0.0| PPL: 6154213.784551729| Time Took: 40.0375862121582 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 98| Loss Value: 1.7180804806947707| F1 Score: 0.0| PPL: 7264059.487282615| Time Took: 40.20252084732056 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 99| Loss Value: 1.716909934103489| F1 Score: 0.0| PPL: 6987779.015918439| Time Took: 39.939340114593506 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 100| Loss Value: 1.712376708805561| F1 Score: 0.0| PPL: 5436543.36984915| Time Took: 39.585286378860474 |\n","====================================================================================================\n","| Training Finished | Total Training Time: 4010.645087480545 |\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"eSNOkTzVEoLO","colab_type":"code","colab":{}},"source":["def greedy_decode(model, src, src_mask, max_len, start_symbol):\n","    memory = model.encode(src, src_mask)\n","    ys = torch.ones(1, 1).fill_(start_symbol).type_as(src.data)\n","    for i in range(max_len-1):\n","        out = model.decode(memory, src_mask, \n","                           Variable(ys), \n","                           Variable(subsequent_mask(ys.size(1))\n","                                    .type_as(src.data)))\n","        prob = model.generator(out[:, -1])\n","        _, next_word = torch.max(prob, dim = 1)\n","        next_word = next_word.data[0]\n","        ys = torch.cat([ys, \n","                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1)\n","    return ys"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Kq5eD4uOF65q","colab_type":"code","outputId":"6a1584ab-9faa-4a18-cab6-706c4bd42950","executionInfo":{"status":"ok","timestamp":1586540117217,"user_tz":-330,"elapsed":1288,"user":{"displayName":"Deepak Goyal","photoUrl":"","userId":"12155914074048018284"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["model.eval()"],"execution_count":74,"outputs":[{"output_type":"execute_result","data":{"text/plain":["EncoderDecoder(\n","  (encoder): Encoder(\n","    (layers): ModuleList(\n","      (0): EncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linears): ModuleList(\n","            (0): Linear(in_features=256, out_features=256, bias=True)\n","            (1): Linear(in_features=256, out_features=256, bias=True)\n","            (2): Linear(in_features=256, out_features=256, bias=True)\n","            (3): Linear(in_features=256, out_features=256, bias=True)\n","          )\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=256, out_features=512, bias=True)\n","          (w_2): Linear(in_features=512, out_features=256, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (sublayer): ModuleList(\n","          (0): SublayerConnection(\n","            (norm): LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): SublayerConnection(\n","            (norm): LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (1): EncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linears): ModuleList(\n","            (0): Linear(in_features=256, out_features=256, bias=True)\n","            (1): Linear(in_features=256, out_features=256, bias=True)\n","            (2): Linear(in_features=256, out_features=256, bias=True)\n","            (3): Linear(in_features=256, out_features=256, bias=True)\n","          )\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=256, out_features=512, bias=True)\n","          (w_2): Linear(in_features=512, out_features=256, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (sublayer): ModuleList(\n","          (0): SublayerConnection(\n","            (norm): LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): SublayerConnection(\n","            (norm): LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (norm): LayerNorm()\n","  )\n","  (decoder): Decoder(\n","    (layers): ModuleList(\n","      (0): DecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linears): ModuleList(\n","            (0): Linear(in_features=256, out_features=256, bias=True)\n","            (1): Linear(in_features=256, out_features=256, bias=True)\n","            (2): Linear(in_features=256, out_features=256, bias=True)\n","            (3): Linear(in_features=256, out_features=256, bias=True)\n","          )\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (src_attn): MultiHeadedAttention(\n","          (linears): ModuleList(\n","            (0): Linear(in_features=256, out_features=256, bias=True)\n","            (1): Linear(in_features=256, out_features=256, bias=True)\n","            (2): Linear(in_features=256, out_features=256, bias=True)\n","            (3): Linear(in_features=256, out_features=256, bias=True)\n","          )\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=256, out_features=512, bias=True)\n","          (w_2): Linear(in_features=512, out_features=256, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (sublayer): ModuleList(\n","          (0): SublayerConnection(\n","            (norm): LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): SublayerConnection(\n","            (norm): LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): SublayerConnection(\n","            (norm): LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (1): DecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linears): ModuleList(\n","            (0): Linear(in_features=256, out_features=256, bias=True)\n","            (1): Linear(in_features=256, out_features=256, bias=True)\n","            (2): Linear(in_features=256, out_features=256, bias=True)\n","            (3): Linear(in_features=256, out_features=256, bias=True)\n","          )\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (src_attn): MultiHeadedAttention(\n","          (linears): ModuleList(\n","            (0): Linear(in_features=256, out_features=256, bias=True)\n","            (1): Linear(in_features=256, out_features=256, bias=True)\n","            (2): Linear(in_features=256, out_features=256, bias=True)\n","            (3): Linear(in_features=256, out_features=256, bias=True)\n","          )\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=256, out_features=512, bias=True)\n","          (w_2): Linear(in_features=512, out_features=256, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (sublayer): ModuleList(\n","          (0): SublayerConnection(\n","            (norm): LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): SublayerConnection(\n","            (norm): LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): SublayerConnection(\n","            (norm): LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (norm): LayerNorm()\n","  )\n","  (src_embed): Sequential(\n","    (0): Embeddings(\n","      (lut): Embedding(7816, 256)\n","    )\n","    (1): PositionalEncoding(\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","  )\n","  (tgt_embed): Sequential(\n","    (0): Embeddings(\n","      (lut): Embedding(7816, 256)\n","    )\n","    (1): PositionalEncoding(\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","  )\n","  (generator): Generator(\n","    (proj): Linear(in_features=256, out_features=7816, bias=True)\n","  )\n",")"]},"metadata":{"tags":[]},"execution_count":74}]},{"cell_type":"code","metadata":{"id":"dAj4kZlOF96b","colab_type":"code","outputId":"bef12e04-d6a0-4bfe-98ce-e71ae40eb9c8","executionInfo":{"status":"ok","timestamp":1586540132052,"user_tz":-330,"elapsed":4457,"user":{"displayName":"Deepak Goyal","photoUrl":"","userId":"12155914074048018284"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["for i in range(10):\n","    batch=batches[i]\n","    for i in range(5):\n","        source=batch.src[i].view(-1,12).to(device)\n","        source_mask=batch.src_mask[i].view(1,-1,12).to(device)\n","        output=greedy_decode(model,source,source_mask,10,1)\n","        src=batch.src[i].view(-1)\n","        trg=batch.trg[i].view(-1)\n","        pred=output.view(-1)\n","        # print(src.size())\n","        # for id in src:\n","        #     print(id)\n","        src_sentence=[voc.index2word[id.item()] for id in src]\n","        trg_sentence=[voc.index2word[id.item()] for id in trg]\n","        pred_sentence=[voc.index2word[id.item()] for id in pred]\n","        print(src_sentence)\n","        print(trg_sentence)\n","        print(pred_sentence)\n","        print(\"-\"*80)\n","#         print(\"-\"*80)\n","#         print(str(output)+\" \"+str(batch.src[i])+\" \"+str(batch.trg[i]))\n","    "],"execution_count":75,"outputs":[{"output_type":"stream","text":["['SOS', 'there', '.', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'where', '?', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'what', '?', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","--------------------------------------------------------------------------------\n","['SOS', 'you', 'have', 'my', 'word', '.', 'as', 'a', 'gentleman', 'EOS', 'PAD', 'PAD']\n","['SOS', 'you', 're', 'sweet', '.', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'not', 'yet', '.', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","--------------------------------------------------------------------------------\n","['SOS', 'hi', '.', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'looks', 'like', 'things', 'worked', 'out', 'tonight', 'huh', '?', 'EOS', 'PAD']\n","['SOS', 'hi', '.', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","--------------------------------------------------------------------------------\n","['SOS', 'have', 'fun', 'tonight', '?', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'tons', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'what', '?', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","--------------------------------------------------------------------------------\n","['SOS', 'well', 'no', '.', '.', '.', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'then', 'that', 's', 'all', 'you', 'had', 'to', 'say', '.', 'EOS']\n","['SOS', 'i', 'm', 'sorry', '.', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD']\n","--------------------------------------------------------------------------------\n","['SOS', 'she', 'okay', '?', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'i', 'hope', 'so', '.', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'yeah', '.', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","--------------------------------------------------------------------------------\n","['SOS', 'they', 'do', 'to', '!', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'they', 'do', 'not', '!', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'what', 's', 'the', 'fuck', '?', 'EOS', 'PAD', 'PAD', 'PAD']\n","--------------------------------------------------------------------------------\n","['SOS', 'did', 'you', 'change', 'your', 'hair', '?', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'no', '.', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'yes', '.', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","--------------------------------------------------------------------------------\n","['SOS', 'no', '.', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'you', 'might', 'wanna', 'think', 'about', 'it', 'EOS', 'PAD', 'PAD', 'PAD']\n","['SOS', 'what', 's', 'the', 'fuck', '?', 'EOS', 'PAD', 'PAD', 'PAD']\n","--------------------------------------------------------------------------------\n","['SOS', 'who', '?', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'joey', '.', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'the', 'best', '.', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","--------------------------------------------------------------------------------\n","['SOS', 'why', '?', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'he', 'was', 'like', 'a', 'total', 'babe', 'EOS', 'PAD', 'PAD', 'PAD']\n","['SOS', 'i', 'don', 't', 'know', '.', 'EOS', 'PAD', 'PAD', 'PAD']\n","--------------------------------------------------------------------------------\n","['SOS', 'he', 'was', 'like', 'a', 'total', 'babe', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'but', 'you', 'hate', 'joey', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'you', 're', 'not', 'gonna', 'be', 'EOS', 'PAD', 'PAD', 'PAD']\n","--------------------------------------------------------------------------------\n","['SOS', 'you', 'looked', 'beautiful', 'last', 'night', 'you', 'know', '.', 'EOS', 'PAD', 'PAD']\n","['SOS', 'so', 'did', 'you', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'no', '.', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","--------------------------------------------------------------------------------\n","['SOS', 'let', 'go', '!', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'you', 'set', 'me', 'up', '.', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'you', 're', 'not', 'a', 'little', 'good', 'night', '.', 'EOS']\n","--------------------------------------------------------------------------------\n","['SOS', 'you', 'set', 'me', 'up', '.', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'i', 'just', 'wanted', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'i', 'm', 'not', '.', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD']\n","--------------------------------------------------------------------------------\n","['SOS', 'the', 'prom', '?', 'kat', 'has', 'a', 'date', '?', 'EOS', 'PAD', 'PAD']\n","['SOS', 'no', 'but', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'yes', '.', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","--------------------------------------------------------------------------------\n","['SOS', 'you', 'the', 'new', 'guy', '?', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'so', 'they', 'tell', 'me', '.', '.', '.', 'EOS', 'PAD', 'PAD']\n","['SOS', 'yes', '.', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","--------------------------------------------------------------------------------\n","['SOS', 'so', 'which', 'dakota', 'you', 'from', '?', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'north', 'actually', '.', 'how', 'd', 'you', '?', 'EOS', 'PAD', 'PAD']\n","['SOS', 'the', 'best', '.', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","--------------------------------------------------------------------------------\n","['SOS', 'north', 'actually', '.', 'how', 'd', 'you', '?', 'EOS', 'PAD', 'PAD', 'PAD']\n","['SOS', 'i', 'was', 'kidding', '.', 'people', 'actually', 'live', 'there', '?', 'EOS']\n","['SOS', 'what', '?', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","--------------------------------------------------------------------------------\n","['SOS', 'how', 'many', 'people', 'were', 'in', 'your', 'old', 'school', '?', 'EOS', 'PAD']\n","['SOS', 'thirty', 'two', '.', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'the', 'best', '.', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","--------------------------------------------------------------------------------\n","['SOS', 'her', 'favorite', 'uncle', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'dead', 'at', 'forty', 'one', '.', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'yeah', '.', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","--------------------------------------------------------------------------------\n","['SOS', 'he', 's', 'pretty', '!', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'okay', '!', 'i', 'wasn', 't', 'sure', 'EOS', 'PAD', 'PAD', 'PAD']\n","['SOS', 'i', 'm', 'sorry', '.', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD']\n","--------------------------------------------------------------------------------\n","['SOS', 'cameron', 'i', 'm', 'a', 'little', 'busy', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'it', 's', 'off', '.', 'the', 'whole', 'thing', '.', 'EOS', 'PAD']\n","['SOS', 'you', 're', 'a', 'little', 'a', 'little', 'you', '.', 'EOS']\n","--------------------------------------------------------------------------------\n","['SOS', 'cameron', 'do', 'you', 'like', 'the', 'girl', '?', 'EOS', 'PAD', 'PAD', 'PAD']\n","['SOS', 'sure', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'i', 'don', 't', 'know', '.', 'EOS', 'PAD', 'PAD', 'PAD']\n","--------------------------------------------------------------------------------\n","['SOS', 'sure', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'then', 'go', 'get', 'her', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'i', 'm', 'sorry', '.', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD']\n","--------------------------------------------------------------------------------\n","['SOS', 'leave', 'my', 'sister', 'alone', '.', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'and', 'why', 'would', 'i', 'do', 'that', '?', 'EOS', 'PAD', 'PAD']\n","['SOS', 'yes', '?', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","--------------------------------------------------------------------------------\n","['SOS', 'yeah', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'what', 'do', 'you', 'think', '?', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'i', 'm', 'sorry', '.', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD']\n","--------------------------------------------------------------------------------\n","['SOS', 'you', 'just', 'said', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'you', 'need', 'money', 'to', 'take', 'a', 'girl', 'out', 'EOS', 'PAD']\n","['SOS', 'i', 'm', 'sorry', '.', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD']\n","--------------------------------------------------------------------------------\n","['SOS', 'what', '?', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'a', 'hundred', 'bucks', 'a', 'date', '.', 'EOS', 'PAD', 'PAD', 'PAD']\n","['SOS', 'i', 'don', 't', 'know', '.', 'EOS', 'PAD', 'PAD', 'PAD']\n","--------------------------------------------------------------------------------\n","['SOS', 'a', 'hundred', 'bucks', 'a', 'date', '.', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'forget', 'it', '.', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'what', '?', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","--------------------------------------------------------------------------------\n","['SOS', 'are', 'you', 'lost', '?', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'nope', 'just', 'came', 'by', 'to', 'chat', 'EOS', 'PAD', 'PAD', 'PAD']\n","['SOS', 'i', 'm', 'not', 'here', '.', 'EOS', 'PAD', 'PAD', 'PAD']\n","--------------------------------------------------------------------------------\n","['SOS', 'nope', 'just', 'came', 'by', 'to', 'chat', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'we', 'don', 't', 'chat', '.', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'what', 's', 'that', '?', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD']\n","--------------------------------------------------------------------------------\n","['SOS', 'i', 'hear', 'you', 're', 'helpin', 'verona', '.', 'EOS', 'PAD', 'PAD', 'PAD']\n","['SOS', 'uh', 'yeah', '.', 'we', 're', 'old', 'friend', 'EOS', 'PAD', 'PAD']\n","['SOS', 'yeah', '.', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","--------------------------------------------------------------------------------\n","['SOS', 'uh', 'yeah', '.', 'we', 're', 'old', 'friend', 'EOS', 'PAD', 'PAD', 'PAD']\n","['SOS', 'you', 'and', 'verona', '?', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'you', 're', 'going', 'to', 'be', 'up', '?', 'EOS', 'PAD']\n","--------------------------------------------------------------------------------\n","['SOS', 'he', 'always', 'look', 'so', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'block', 'e', '?', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'i', 'm', 'sorry', '.', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD']\n","--------------------------------------------------------------------------------\n","['SOS', 'pick', 'you', 'up', 'friday', 'then', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'oh', 'right', '.', 'friday', '.', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'you', 're', 'not', 'yet', '.', 'EOS', 'PAD', 'PAD', 'PAD']\n","--------------------------------------------------------------------------------\n","['SOS', 'you', 'covered', 'in', 'my', 'vomit', '.', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'seven', 'thirty', '?', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', '?', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","--------------------------------------------------------------------------------\n","['SOS', 'excuse', 'me', '?', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'that', 's', 'what', 'you', 'want', 'isn', 't', 'it', '?', 'EOS']\n","['SOS', 'i', 'm', 'sorry', '.', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD']\n","--------------------------------------------------------------------------------\n","['SOS', 'i', 'say', 'do', 'what', 'you', 'wanna', 'do', '.', 'EOS', 'PAD', 'PAD']\n","['SOS', 'funny', 'you', 're', 'the', 'only', 'one', 'EOS', 'PAD', 'PAD', 'PAD']\n","['SOS', 'i', 'don', 't', 'know', '.', 'EOS', 'PAD', 'PAD', 'PAD']\n","--------------------------------------------------------------------------------\n","['SOS', 'okay', '?', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'i', 'm', 'fine', '.', 'i', 'm', 'EOS', 'PAD', 'PAD', 'PAD']\n","['SOS', 'i', 'm', 'sorry', '.', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD']\n","--------------------------------------------------------------------------------\n","['SOS', 'why', 'd', 'you', 'let', 'him', 'get', 'to', 'you', '?', 'EOS', 'PAD']\n","['SOS', 'who', '?', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'i', 'm', 'sorry', '.', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD']\n","--------------------------------------------------------------------------------\n","['SOS', 'i', 'thought', 'you', 'were', 'above', 'all', 'that', 'EOS', 'PAD', 'PAD', 'PAD']\n","['SOS', 'you', 'know', 'what', 'they', 'say', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'i', 'm', 'not', '.', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD']\n","--------------------------------------------------------------------------------\n","['SOS', 'kat', '!', 'wake', 'up', '!', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'what', '?', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'what', '?', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","--------------------------------------------------------------------------------\n","['SOS', 'busy', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'were', 'you', 'in', 'jail', '?', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'you', 're', 'not', 'a', 'little', 'time', '.', 'EOS', 'PAD']\n","--------------------------------------------------------------------------------\n","['SOS', 'were', 'you', 'in', 'jail', '?', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'maybe', '.', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'yes', '.', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","--------------------------------------------------------------------------------\n","['SOS', 'start', 'a', 'band', '?', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'my', 'father', 'wouldn', 't', 'approve', 'of', 'that', 'that', 'EOS', 'PAD']\n","['SOS', 'yes', '.', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","--------------------------------------------------------------------------------\n","['SOS', 'oh', 'so', 'now', 'you', 'think', 'you', 'know', 'me', '?', 'EOS', 'PAD']\n","['SOS', 'i', 'm', 'gettin', 'there', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'i', 'm', 'sorry', '.', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD']\n","--------------------------------------------------------------------------------\n","['SOS', 'who', '?', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'bianca', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'the', 'best', '.', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","--------------------------------------------------------------------------------\n","['SOS', 'what', 'are', 'you', 'doing', 'here', '?', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'i', 'heard', 'there', 'was', 'a', 'poetry', 'reading', '.', 'EOS', 'PAD']\n","['SOS', 'i', 'm', 'not', 'in', 'the', 'world', '.', 'EOS', 'PAD']\n","--------------------------------------------------------------------------------\n","['SOS', 'i', 'heard', 'there', 'was', 'a', 'poetry', 'reading', '.', 'EOS', 'PAD', 'PAD']\n","['SOS', 'you', 're', 'so', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","['SOS', 'you', 're', 'not', 'a', 'little', 'EOS', 'PAD', 'PAD', 'PAD']\n","--------------------------------------------------------------------------------\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"R2lay0ahGArk","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}