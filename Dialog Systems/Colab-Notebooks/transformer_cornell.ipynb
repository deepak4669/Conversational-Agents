{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"transformer_cornell.ipynb","provenance":[],"collapsed_sections":["v5xV-42LAX6d"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"8i8or0mWcKPL","colab_type":"code","outputId":"2d5a6346-d43c-4b3b-fb4f-a16f90403f0a","executionInfo":{"status":"ok","timestamp":1586594131518,"user_tz":-330,"elapsed":35015,"user":{"displayName":"Deepak Goyal","photoUrl":"","userId":"12155914074048018284"}},"colab":{"base_uri":"https://localhost:8080/","height":124}},"source":["from google.colab import drive\n","drive.mount(\"/content/drive\")"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"U-K4mgwHE8oj","colab_type":"code","outputId":"46d49619-5649-4cfd-da2b-8627e0e0a04a","executionInfo":{"status":"ok","timestamp":1586594147594,"user_tz":-330,"elapsed":11728,"user":{"displayName":"Deepak Goyal","photoUrl":"","userId":"12155914074048018284"}},"colab":{"base_uri":"https://localhost:8080/","height":312}},"source":["!nvidia-smi"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Sat Apr 11 08:35:40 2020       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 440.64.00    Driver Version: 418.67       CUDA Version: 10.1     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|===============================+======================+======================|\n","|   0  Tesla P4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   38C    P8     7W /  75W |      0MiB /  7611MiB |      0%      Default |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                       GPU Memory |\n","|  GPU       PID   Type   Process name                             Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"R-KwziUccUv9","colab_type":"code","colab":{}},"source":["#Pre-Processing\n","import os\n","import re\n","import torch\n","import random\n","import itertools\n","\n","#Model\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","\n","import numpy as np\n","\n","# For visualising metrics\n","# from visdom import Visdom\n","\n","# For visualising gradients plot\n","import matplotlib.pyplot as plt\n","from matplotlib.lines import Line2D\n","\n","import copy\n","import math\n","import time"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bzOQagtOdqyn","colab_type":"code","outputId":"3afcc29a-79ae-48f6-9fbb-8c1ecce01a76","executionInfo":{"status":"ok","timestamp":1586594152087,"user_tz":-330,"elapsed":3553,"user":{"displayName":"Deepak Goyal","photoUrl":"","userId":"12155914074048018284"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","# device=torch.device(\"cpu\")\n","print(\"The device found: \"+str(device))"],"execution_count":4,"outputs":[{"output_type":"stream","text":["The device found: cuda\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"19BVL5GNdtbi","colab_type":"code","colab":{}},"source":["def plot_grad_flow(named_parameters):\n","    \"\"\"\n","        Plotting gradient flow across various layers\n","        Thanks to: https://discuss.pytorch.org/t/check-gradient-flow-in-network/15063/2\n","    \"\"\"   \n","    ave_grads = []\n","    layers = []\n","    for n, p in named_parameters:\n","        if(p.requires_grad) and (\"bias\" not in n):\n","            layers.append(n)\n","            ave_grads.append(p.grad.abs().mean())\n","    plt.plot(ave_grads, alpha=0.3, color=\"b\")\n","    plt.hlines(0, 0, len(ave_grads)+1, linewidth=1, color=\"k\" )\n","    plt.xticks(range(0,len(ave_grads), 1), layers, rotation=\"vertical\")\n","    plt.xlim(xmin=0, xmax=len(ave_grads))\n","    plt.xlabel(\"Layers\")\n","    plt.ylabel(\"average gradient\")\n","    plt.title(\"Gradient flow\")\n","    plt.grid(True)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XyVbmpGw26_v","colab_type":"text"},"source":["# Preprocessing\n"]},{"cell_type":"code","metadata":{"id":"qz7CbxS4dwh3","colab_type":"code","outputId":"bb98c347-791c-49ef-8af4-4517ab635352","executionInfo":{"status":"ok","timestamp":1586594155000,"user_tz":-330,"elapsed":5659,"user":{"displayName":"Deepak Goyal","photoUrl":"","userId":"12155914074048018284"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["path='/content/drive/My Drive/Data'\n","dataset='cornell movie-dialogs corpus'\n","\n","data_folder=os.path.join(path,dataset)\n","\n","print(\"The final data corpus folder: \"+str(data_folder))"],"execution_count":6,"outputs":[{"output_type":"stream","text":["The final data corpus folder: /content/drive/My Drive/Data/cornell movie-dialogs corpus\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"dDKqcO2de7RM","colab_type":"code","colab":{}},"source":["def get_lines_conversations():\n","    \"\"\"\n","    Loads movie lines and conversations from the dataset.\n","    \n","    data_folder: Destination where conversations and lines are stored.\n","    \n","    movie_lines: Consist of movie lines as given by the dataset.\n","    movie_conversations: Consist of movie conversations as given by the dataset.\n","    \n","    \"\"\"\n","    movie_lines=[]\n","    movie_conversations=[]\n","\n","    with open(os.path.join(data_folder,'movie_lines.txt'),'r',encoding='iso-8859-1') as f:\n","        for line in f:\n","            movie_lines.append(line)\n","    \n","    with open(os.path.join(data_folder,'movie_conversations.txt'),'r', encoding='iso-8859-1') as f:\n","        for line in f:\n","            movie_conversations.append(line)\n","                                       \n","\n","    return movie_lines,movie_conversations"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-txSMOjQfEnx","colab_type":"code","outputId":"21fff9b5-40cd-4c39-af71-e638fa935e34","executionInfo":{"status":"ok","timestamp":1586594158235,"user_tz":-330,"elapsed":8461,"user":{"displayName":"Deepak Goyal","photoUrl":"","userId":"12155914074048018284"}},"colab":{"base_uri":"https://localhost:8080/","height":173}},"source":["t1=time.time()\n","print(\"Extracting movie lines and movie conversations...\")\n","movie_lines,movie_conversations=get_lines_conversations()\n","\n","print(\"Number of distinct lines: \"+str(len(movie_lines)))\n","print(\"Number of conversations: \"+str(len(movie_conversations)))\n","print(\"Average Number of lines per conversations: \"+str(len(movie_lines)/len(movie_conversations)))\n","\n","print(movie_lines[0])\n","print(movie_conversations[0])\n","\n","print(\"Extracting took place in: \"+str(time.time()-t1))"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Extracting movie lines and movie conversations...\n","Number of distinct lines: 304713\n","Number of conversations: 83097\n","Average Number of lines per conversations: 3.6669554857576085\n","L1045 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ They do not!\n","\n","u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L194', 'L195', 'L196', 'L197']\n","\n","Extracting took place in: 4.363175868988037\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3LXCFv_qfMsN","colab_type":"code","colab":{}},"source":["exceptions=[]\n","def loadLines(movie_lines,fields):\n","    lines={}\n","    for lineid in range(len(movie_lines)):\n","        \n","        line=movie_lines[lineid]\n","        values=line.split(\" +++$+++ \")\n","        \n","        \n","        lineVals={}\n","        \n","        # print(\"values\"+str(len(values)))\n","        # print(\"fields\"+str(len(fields)))\n","              \n","        for i,field in enumerate(fields):\n","            try:\n","                lineVals[field]=values[i]\n","            except:\n","                print(\"Exception: \"+str(len(values)))\n","                exceptions.append(lineid)\n","        \n","        lines[lineVals['lineID']]=lineVals\n","    \n","    return lines\n","\n","def loadConversations(movie_conversations,lines,fields):\n","    conversations=[]\n","    \n","    for convo in movie_conversations:\n","        values=convo.split(\" +++$+++ \")\n","        conVals={}\n","       \n","        for i,field in enumerate(fields):\n","            conVals[field]=values[i]\n","        \n","        lineIDs=eval(conVals[\"utteranceIDs\"])\n","        \n","        conVals[\"lines\"]=[]\n","        \n","        for lineID in lineIDs:\n","            conVals[\"lines\"].append(lines[lineID])\n","        conversations.append(conVals)\n","        \n","    return conversations\n","\n","def sentencePairs(conversations):\n","    qr_pairs=[]\n","    \n","    for conversation in conversations:\n","        for i in range(len(conversation[\"lines\"])-1):\n","            query=conversation[\"lines\"][i][\"text\"].strip()\n","            response=conversation[\"lines\"][i+1][\"text\"].strip()\n","            \n","            if query and response:\n","                qr_pairs.append([query,response])\n","        \n","    return qr_pairs"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"J2dngu2Xyvzt","colab_type":"code","outputId":"f20966e6-f4b5-40a5-d2ed-8ce451f2dbb5","executionInfo":{"status":"ok","timestamp":1586594160106,"user_tz":-330,"elapsed":9896,"user":{"displayName":"Deepak Goyal","photoUrl":"","userId":"12155914074048018284"}},"colab":{"base_uri":"https://localhost:8080/","height":69}},"source":["t1=time.time()\n","print(\"Separating meaningfull information for our model...\")\n","\n","lines={}\n","conversations=[]\n","qr_pairs=[]\n","\n","movie_lines_fields=[\"lineID\",\"characterID\",\"movieID\",\"character\",\"text\"]\n","movie_convo_fields=[\"charcaterID\",\"character2ID\",\"movieID\",\"utteranceIDs\"]\n","\n","lines=loadLines(movie_lines,movie_lines_fields)\n","conversations=loadConversations(movie_conversations,lines,movie_convo_fields)\n","qr_pairs=sentencePairs(conversations)\n","\n","print(\"The number of query-response pairs are: \"+str(len(qr_pairs)))\n","print(\"Separation took place in: \"+str(time.time()-t1))"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Separating meaningfull information for our model...\n","The number of query-response pairs are: 221282\n","Separation took place in: 1.9829459190368652\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"NFMCnpuO2jpr","colab_type":"code","colab":{}},"source":["PAD_Token=0\n","START_Token=1\n","END_Token=2\n","\n","class Vocabulary:\n","    def __init__(self):\n","        self.trimmed=False\n","        self.word2count={}\n","        self.index2word={PAD_Token:\"PAD\",START_Token:\"SOS\",END_Token:\"EOS\"}\n","        self.word2index={\"PAD\":PAD_Token,\"SOS\":START_Token,\"EOS\":END_Token}\n","        self.num_words=3\n","        \n","    def addSentence(self,sentence):\n","        for word in sentence.split(\" \"):\n","            self.addWord(word)\n","    def addWord(self,word):\n","        if word not in self.word2index:\n","            self.word2index[word]=self.num_words\n","            self.index2word[self.num_words]=word\n","            self.word2count[word]=1\n","            self.num_words=self.num_words+1\n","        else:\n","            self.word2count[word]+=1\n","            \n","    def trim(self,min_count):\n","        \n","        if self.trimmed:\n","            return\n","        self.trimmed=True\n","        \n","        keep_words=[]\n","        \n","        for word,freq in self.word2count.items():\n","            if freq>=min_count:\n","                keep_words.append(word)\n","        \n","        self.word2count={}\n","        self.index2word={PAD_Token:\"PAD\",START_Token:\"SOS\",END_Token:\"EOS\"}\n","        self.word2index={\"PAD\":PAD_Token,\"SOS\":START_Token,\"EOS\":END_Token}\n","        self.num_words=3\n","        \n","        for word in keep_words:\n","            self.addWord(word)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GeScbB7iy0AC","colab_type":"code","outputId":"0ddcc2e3-5cef-4e13-e7df-bdf5d2344e37","executionInfo":{"status":"ok","timestamp":1586594166482,"user_tz":-330,"elapsed":15637,"user":{"displayName":"Deepak Goyal","photoUrl":"","userId":"12155914074048018284"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["Max_Length=10\n","\n","def normalizeString(s):\n","    s=s.lower().strip()\n","    s=re.sub(r\"([.!?])\", r\" \\1\", s)\n","    s=re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n","    s=re.sub(r\"\\s+\", r\" \", s).strip()\n","    return s\n","\n","def readVocs(qr_pairs):\n","    \n","    for qr_pair in qr_pairs:\n","        qr_pair[0]=normalizeString(qr_pair[0])\n","        qr_pair[1]=normalizeString(qr_pair[1])\n","    \n","    voc=Vocabulary()\n","    return voc,qr_pairs\n","\n","def filterPair(pair):\n","    return len(pair[0].split(\" \"))<Max_Length and len(pair[1].split(\" \"))<Max_Length\n","\n","def filterPairs(qr_pairs):\n","    return [pair for pair in qr_pairs if filterPair(pair)]\n","\n","def prepareDataset(qr_pairs):\n","    voc, qr_pairs=readVocs(qr_pairs)\n","    qr_pairs=filterPairs(qr_pairs)\n","       \n","    for pair in qr_pairs:\n","        voc.addSentence(pair[0])\n","        voc.addSentence(pair[1])\n","#     print(\"Number\"+str(voc.num_words))\n","    return voc,qr_pairs\n","\n","t1=time.time()\n","print(\"Preparing dataset and corresponding vocabulary...\")\n","voc, pairs=prepareDataset(qr_pairs)\n","print(\"Preparation took place in: \"+str(time.time()-t1))"],"execution_count":12,"outputs":[{"output_type":"stream","text":["Preparing dataset and corresponding vocabulary...\n","Preparation took place in: 5.793337345123291\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"UUBpnFjQ2SCS","colab_type":"code","outputId":"dcff2985-0728-4de8-81b5-d1d140a32679","executionInfo":{"status":"ok","timestamp":1586594166485,"user_tz":-330,"elapsed":15236,"user":{"displayName":"Deepak Goyal","photoUrl":"","userId":"12155914074048018284"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["Min_Count=3\n","\n","def trimRareWords(voc,qr_pairs):\n","    \n","    voc.trim(Min_Count)\n","    keep_pairs=[]\n","    \n","    for pair in qr_pairs:\n","        input_sentence=pair[0]\n","        output_sentence=pair[1]\n","        \n","        keep_input=True\n","        keep_output=True\n","        \n","        for word in input_sentence.split(\" \"):\n","            if word not in voc.word2index:\n","                keep_input=False\n","                break\n","        \n","        for word in output_sentence.split(\" \"):\n","            if word not in voc.word2index:\n","                keep_output=False\n","                break\n","                \n","        if keep_input and keep_output:\n","            keep_pairs.append(pair)\n","            \n","    return keep_pairs\n","\n","t1=time.time()\n","print(\"Trimming rare words from vocabulary and dataset..\")\n","\n","pairs=trimRareWords(voc,pairs)\n","\n","print(\"Trimming took place in: \"+str(time.time()-t1))"],"execution_count":13,"outputs":[{"output_type":"stream","text":["Trimming rare words from vocabulary and dataset..\n","Trimming took place in: 0.11775898933410645\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"VjOEe2nx2sbZ","colab_type":"code","colab":{}},"source":["def indexesFromSentence(voc,sentence):\n","    tokenised_sentence=[]\n","    tokenised_sentence.append(START_Token)\n","    \n","    for word in sentence.split(\" \"):\n","        tokenised_sentence.append(voc.word2index[word])\n","        \n","    tokenised_sentence.append(END_Token)\n","    \n","    assert len(tokenised_sentence)<=Max_Length+2\n","    for _ in range(Max_Length+2-len(tokenised_sentence)):\n","        tokenised_sentence.append(PAD_Token)\n","        \n","    return tokenised_sentence\n","\n","def binaryMatrix(l,value=PAD_Token):\n","    m=[]\n","    for i,seq in enumerate(l):\n","        m.append([])\n","        for token in seq:\n","            if token==value:\n","                m[i].append(0)\n","            else:\n","                m[i].append(1)\n","        \n","    return m\n","\n","def inputVar(voc,l):\n","    \n","    indexes_batch=[indexesFromSentence(voc,sentence) for sentence in l]\n","    input_lengths=torch.tensor([len(index) for index in indexes_batch])\n","    padVar=torch.LongTensor(indexes_batch)\n","    return input_lengths,padVar\n","\n","def outputVar(voc,l):\n","    indexes_batch=[indexesFromSentence(voc,sentence) for sentence in l]\n","    max_target_len=torch.tensor([len(index) for index in indexes_batch])\n","    mask=binaryMatrix(indexes_batch)\n","    mask=torch.ByteTensor(mask)\n","    padVar=torch.LongTensor(indexes_batch)\n","    return max_target_len, mask, padVar\n","\n","def batch2TrainData(voc,pair_batch):\n","    #sort function see \n","    input_batch=[]\n","    output_batch=[]\n","\n","    for pair in pair_batch:\n","        input_batch.append(pair[0])\n","        output_batch.append(pair[1])\n","                                  \n","    \n","    input_lengths,tokenised_input=inputVar(voc,input_batch)\n","    max_out_length,mask,tokenised_output=outputVar(voc,output_batch)\n","    return input_lengths,tokenised_input,max_out_length,mask,tokenised_output"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"f-wd2hB-2y4b","colab_type":"code","outputId":"8c743a02-9732-4aef-a0e8-c51bdb4f6fc5","executionInfo":{"status":"ok","timestamp":1586594166490,"user_tz":-330,"elapsed":14675,"user":{"displayName":"Deepak Goyal","photoUrl":"","userId":"12155914074048018284"}},"colab":{"base_uri":"https://localhost:8080/","height":416}},"source":["print(\"Number of query-response pairs after all the preprocessing: \"+str(len(pairs)))\n","\n","#Sample batch\n","batch=[random.choice(pairs) for _ in range(5)]\n","input_lengths,tokenised_input,max_out_length,mask,tokenised_output=batch2TrainData(voc,batch)\n","\n","print(\"Input length: \"+str(input_lengths)+\" Size: \"+str(input_lengths.shape))\n","print(\"-\"*80)\n","print(\"Tokenised Input: \"+str(tokenised_input)+\" Size: \"+str(tokenised_input.shape))\n","print(\"-\"*80)\n","print(\"Max out length: \"+str(max_out_length)+\" Size: \"+str(max_out_length.shape))\n","print(\"-\"*80)\n","print(\"Mask: \"+str(mask)+\" Size: \"+str(mask.shape))\n","print(\"-\"*80)\n","print(\"Tokenised Output: \"+str(tokenised_output)+\" Size: \"+str(tokenised_output.shape))\n","print(\"-\"*80)"],"execution_count":15,"outputs":[{"output_type":"stream","text":["Number of query-response pairs after all the preprocessing: 53113\n","Input length: tensor([12, 12, 12, 12, 12]) Size: torch.Size([5])\n","--------------------------------------------------------------------------------\n","Tokenised Input: tensor([[   1, 5178,  141,   83,   75, 1623, 5350,    4,    2,    0,    0,    0],\n","        [   1,  869,   27,  132,   60,   98,  510,    4,    2,    0,    0,    0],\n","        [   1,   25,  755,   98,   34,  199,    4,    2,    0,    0,    0,    0],\n","        [   1,   40,   53,  356,   56,    9,  376,  356, 3271,    6,    2,    0],\n","        [   1, 1625,   66,    2,    0,    0,    0,    0,    0,    0,    0,    0]]) Size: torch.Size([5, 12])\n","--------------------------------------------------------------------------------\n","Max out length: tensor([12, 12, 12, 12, 12]) Size: torch.Size([5])\n","--------------------------------------------------------------------------------\n","Mask: tensor([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]], dtype=torch.uint8) Size: torch.Size([5, 12])\n","--------------------------------------------------------------------------------\n","Tokenised Output: tensor([[   1,  393, 3716,    6,    2,    0,    0,    0,    0,    0,    0,    0],\n","        [   1,   47,    7,    8,  350,    6,    2,    0,    0,    0,    0,    0],\n","        [   1,   64,  380,   76,   66,    2,    0,    0,    0,    0,    0,    0],\n","        [   1,   53,  301,  314,    4,    2,    0,    0,    0,    0,    0,    0],\n","        [   1,   12, 1626,    4,    2,    0,    0,    0,    0,    0,    0,    0]]) Size: torch.Size([5, 12])\n","--------------------------------------------------------------------------------\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"DpR-1oMt3HA4","colab_type":"text"},"source":["# Model 1"]},{"cell_type":"code","metadata":{"id":"RJnnpKgt3Nyg","colab_type":"code","colab":{}},"source":["class EncoderDecoder(nn.Module):\n","    \"\"\"\n","    A standard Encoder-Decoder architecture. Base for this and many \n","    other models.\n","    \"\"\"\n","    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n","        super(EncoderDecoder, self).__init__()\n","        self.encoder = encoder\n","        self.decoder = decoder\n","        self.src_embed = src_embed\n","        self.tgt_embed = tgt_embed\n","        self.generator = generator\n","        \n","    def forward(self, src, tgt, src_mask, tgt_mask):\n","        \"Take in and process masked src and target sequences.\"\n","        return self.decode(self.encode(src, src_mask), src_mask,\n","                            tgt, tgt_mask)\n","    \n","    def encode(self, src, src_mask):\n","        return self.encoder(self.src_embed(src), src_mask)\n","    \n","    def decode(self, memory, src_mask, tgt, tgt_mask):\n","        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cRE6Ls4k_lvE","colab_type":"code","colab":{}},"source":["class Generator(nn.Module):\n","    \"Define standard linear + softmax generation step.\"\n","    def __init__(self, d_model, vocab):\n","        super(Generator, self).__init__()\n","        self.proj = nn.Linear(d_model, vocab)\n","\n","    def forward(self, x):\n","        return self.proj(x)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aPiLhBPP_oQL","colab_type":"code","colab":{}},"source":["def clones(module, N):\n","    \"Produce N identical layers.\"\n","    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3ApjC3XD_quP","colab_type":"code","colab":{}},"source":["class Encoder(nn.Module):\n","    \"Core encoder is a stack of N layers\"\n","    def __init__(self, layer, N):\n","        super(Encoder, self).__init__()\n","        self.layers = clones(layer, N)\n","        self.norm = LayerNorm(layer.size)\n","        \n","    def forward(self, x, mask):\n","        \"Pass the input (and mask) through each layer in turn.\"\n","        for layer in self.layers:\n","            x = layer(x, mask)\n","        return self.norm(x)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5wUahzPb_s88","colab_type":"code","colab":{}},"source":["class LayerNorm(nn.Module):\n","    \"Construct a layernorm module (See citation for details).\"\n","    def __init__(self, features, eps=1e-6):\n","        super(LayerNorm, self).__init__()\n","        self.a_2 = nn.Parameter(torch.ones(features))\n","        self.b_2 = nn.Parameter(torch.zeros(features))\n","        self.eps = eps\n","\n","    def forward(self, x):\n","        mean = x.mean(-1, keepdim=True)\n","        std = x.std(-1, keepdim=True)\n","        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zowO7FHp_v5s","colab_type":"code","colab":{}},"source":["class SublayerConnection(nn.Module):\n","    \"\"\"\n","    A residual connection followed by a layer norm.\n","    Note for code simplicity the norm is first as opposed to last.\n","    \"\"\"\n","    def __init__(self, size, dropout):\n","        super(SublayerConnection, self).__init__()\n","        self.norm = LayerNorm(size)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x, sublayer):\n","        \"Apply residual connection to any sublayer with the same size.\"\n","        return x + self.dropout(sublayer(self.norm(x)))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_nkoHVj4_yiG","colab_type":"code","colab":{}},"source":["class EncoderLayer(nn.Module):\n","    \"Encoder is made up of self-attn and feed forward (defined below)\"\n","    def __init__(self, size, self_attn, feed_forward, dropout):\n","        super(EncoderLayer, self).__init__()\n","        self.self_attn = self_attn\n","        self.feed_forward = feed_forward\n","        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n","        self.size = size\n","\n","    def forward(self, x, mask):\n","        \"Follow Figure 1 (left) for connections.\"\n","        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n","        return self.sublayer[1](x, self.feed_forward)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BIJVFhXm_1FF","colab_type":"code","colab":{}},"source":["class Decoder(nn.Module):\n","    \"Generic N layer decoder with masking.\"\n","    def __init__(self, layer, N):\n","        super(Decoder, self).__init__()\n","        self.layers = clones(layer, N)\n","        self.norm = LayerNorm(layer.size)\n","        \n","    def forward(self, x, memory, src_mask, tgt_mask):\n","        for layer in self.layers:\n","            x = layer(x, memory, src_mask, tgt_mask)\n","        return self.norm(x)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZKn03mRp_3V2","colab_type":"code","colab":{}},"source":["class DecoderLayer(nn.Module):\n","    \"Decoder is made of self-attn, src-attn, and feed forward (defined below)\"\n","    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n","        super(DecoderLayer, self).__init__()\n","        self.size = size\n","        self.self_attn = self_attn\n","        self.src_attn = src_attn\n","        self.feed_forward = feed_forward\n","        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n"," \n","    def forward(self, x, memory, src_mask, tgt_mask):\n","        \"Follow Figure 1 (right) for connections.\"\n","        m = memory\n","        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n","        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n","        return self.sublayer[2](x, self.feed_forward)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KddgfVJx_6DC","colab_type":"code","colab":{}},"source":["def subsequent_mask(size):\n","    \"Mask out subsequent positions.\"\n","    attn_shape = (1, size, size)\n","    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n","    return torch.from_numpy(subsequent_mask) == 0"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BYWVMJoh_8-k","colab_type":"code","colab":{}},"source":["def attention(query, key, value, mask=None, dropout=None):\n","    \"Compute 'Scaled Dot Product Attention'\"\n","    d_k = query.size(-1)\n","    scores = torch.matmul(query, key.transpose(-2, -1)) \\\n","             / math.sqrt(d_k)\n","    if mask is not None:\n","        scores = scores.masked_fill(mask == 0, -1e9)\n","    p_attn = F.softmax(scores, dim = -1)\n","    if dropout is not None:\n","        p_attn = dropout(p_attn)\n","    return torch.matmul(p_attn, value), p_attn"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"veKkKGgb__q9","colab_type":"code","colab":{}},"source":["class MultiHeadedAttention(nn.Module):\n","    def __init__(self, h, d_model, dropout=0.1):\n","        \"Take in model size and number of heads.\"\n","        super(MultiHeadedAttention, self).__init__()\n","        assert d_model % h == 0\n","        # We assume d_v always equals d_k\n","        self.d_k = d_model // h\n","        self.h = h\n","        self.linears = clones(nn.Linear(d_model, d_model), 4)\n","        self.attn = None\n","        self.dropout = nn.Dropout(p=dropout)\n","        \n","    def forward(self, query, key, value, mask=None):\n","        \"Implements Figure 2\"\n","        if mask is not None:\n","            # Same mask applied to all h heads.\n","            mask = mask.unsqueeze(1)\n","        nbatches = query.size(0)\n","        \n","        # 1) Do all the linear projections in batch from d_model => h x d_k \n","        query, key, value = \\\n","            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n","             for l, x in zip(self.linears, (query, key, value))]\n","        \n","        # 2) Apply attention on all the projected vectors in batch. \n","        x, self.attn = attention(query, key, value, mask=mask, \n","                                 dropout=self.dropout)\n","        \n","        # 3) \"Concat\" using a view and apply a final linear. \n","        x = x.transpose(1, 2).contiguous() \\\n","             .view(nbatches, -1, self.h * self.d_k)\n","        return self.linears[-1](x)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aSqvuoJVAE_i","colab_type":"code","colab":{}},"source":["class PositionwiseFeedForward(nn.Module):\n","    \"Implements FFN equation.\"\n","    def __init__(self, d_model, d_ff, dropout=0.1):\n","        super(PositionwiseFeedForward, self).__init__()\n","        self.w_1 = nn.Linear(d_model, d_ff)\n","        self.w_2 = nn.Linear(d_ff, d_model)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        return self.w_2(self.dropout(F.relu(self.w_1(x))))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ft2ByV62AHlf","colab_type":"code","colab":{}},"source":["class Embeddings(nn.Module):\n","    def __init__(self, d_model, vocab):\n","        super(Embeddings, self).__init__()\n","        self.lut = nn.Embedding(vocab, d_model)\n","        self.d_model = d_model\n","\n","    def forward(self, x):\n","        return self.lut(x) * math.sqrt(self.d_model)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"flsGFWNWAKso","colab_type":"code","colab":{}},"source":["class PositionalEncoding(nn.Module):\n","    \"Implement the PE function.\"\n","    def __init__(self, d_model, dropout, max_len=5000):\n","        super(PositionalEncoding, self).__init__()\n","        self.dropout = nn.Dropout(p=dropout)\n","        \n","        # Compute the positional encodings once in log space.\n","        pe = torch.zeros(max_len, d_model)\n","        position = torch.arange(0, max_len).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2) *\n","                             -(math.log(10000.0) / d_model))\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","        pe = pe.unsqueeze(0)\n","        self.register_buffer('pe', pe)\n","        \n","    def forward(self, x):\n","        x = x + Variable(self.pe[:, :x.size(1)], \n","                         requires_grad=False)\n","        return self.dropout(x)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"veyYVrUlANvh","colab_type":"code","colab":{}},"source":["def make_model(src_vocab, tgt_vocab, N=6, \n","               d_model=512, d_ff=2048, h=8, dropout=0.1):\n","    \"Helper: Construct a model from hyperparameters.\"\n","    c = copy.deepcopy\n","    attn = MultiHeadedAttention(h, d_model)\n","    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n","    position = PositionalEncoding(d_model, dropout)\n","    model = EncoderDecoder(\n","        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n","        Decoder(DecoderLayer(d_model, c(attn), c(attn), \n","                             c(ff), dropout), N),\n","        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),\n","        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),\n","        Generator(d_model, tgt_vocab))\n","    \n","    # This was important from their code. \n","    # Initialize parameters with Glorot / fan_avg.\n","    for p in model.parameters():\n","        if p.dim() > 1:\n","            nn.init.xavier_uniform_(p)\n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v5xV-42LAX6d","colab_type":"text"},"source":["# Model 2"]},{"cell_type":"code","metadata":{"id":"uHIbkFgbAhpa","colab_type":"code","colab":{}},"source":["class EncoderDecoder(nn.Module):\n","    \n","    def __init__(self,encoder,decoder,source_embed,target_embed,generator):\n","        super().__init__()\n","        \n","        self.encoder=encoder\n","        self.decoder=decoder\n","        \n","        self.source_embed=source_embed\n","        self.target_embed=target_embed\n","        \n","        self.generator=generator # Linear + Log_softmax\n","        \n","    def forward(self,source,target,source_mask,target_mask):\n","        return self.decode(self.encode(source,source_mask),source_mask,target,target_mask)\n","    \n","    def encode(self,source,source_mask):\n","        return self.encoder(self.source_embed(source),source_mask)\n","    \n","    def decode(self,memory, source_mask,target,target_mask):\n","        return self.decoder(self.target_embed(target),memory,source_mask,target_mask)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ewcrG0AeCHQc","colab_type":"code","colab":{}},"source":["class Generator(nn.Module):\n","    \n","    def __init__(self,d_model,vocab_size):\n","        super().__init__()\n","        self.projection=nn.Linear(d_model,vocab_size)\n","        \n","    def forward(self,decoder_output):\n","        return F.log_softmax(self.projection(decoder_output),dim=-1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jNIgAbiYCKg0","colab_type":"code","colab":{}},"source":["def clones(module,N):\n","    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qgHHuuzYCNL8","colab_type":"code","colab":{}},"source":["class Encoder(nn.Module):\n","    \n","    def __init__(self,layer,N):\n","        super().__init__()\n","        \n","        self.layers=clones(layer,N)\n","        self.norm=LayerNorm(layer.size)\n","    \n","    def forward(self,x,mask):\n","        \n","        for layer in self.layers:\n","            x=layer(x,mask)\n","        \n","        return self.norm(x)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dQRX2bAMCPlO","colab_type":"code","colab":{}},"source":["class LayerNorm(nn.Module):\n","    \n","    def __init__(self,features,eps=1e-6):\n","        super().__init__()\n","        self.a_2=nn.Parameter(torch.ones(features))\n","        self.b_2=nn.Parameter(torch.zeros(features))\n","        self.eps=eps\n","        \n","    def forward(self,x):\n","        mean=x.mean(-1,keepdim=True)\n","        std=x.std(-1,keepdim=True)\n","        return self.a_2*(x-mean)/(std+self.eps)+self.b_2"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-sIDH01lCSsv","colab_type":"code","colab":{}},"source":["class SublayerConnection(nn.Module):\n","    \n","    def __init__(self,size,dropout):\n","        super().__init__()\n","        \n","        self.dropout=nn.Dropout(dropout)\n","        self.norm=LayerNorm(size)\n","        \n","    def forward(self,x,sublayer):\n","        return x+self.dropout(sublayer(self.norm(x)))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1zQ_LsoCCVpO","colab_type":"code","colab":{}},"source":["class EncoderLayer(nn.Module):\n","    def __init__(self,size,self_attn,feed_forward,dropout):\n","        super().__init__()\n","        \n","        self.attn=self_attn\n","        self.feed_forward=feed_forward\n","        self.sublayer=clones(SublayerConnection(size,dropout),2)\n","        self.size=size\n","        \n","    def forward(self,x,mask):\n","        \n","        x=self.sublayer[0](x,lambda x: self.attn(x,x,x,mask))\n","        return self.sublayer[1](x,self.feed_forward)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cZjOa0vwCY5H","colab_type":"code","colab":{}},"source":["class Decoder(nn.Module):\n","    \n","    def __init__(self,layer,N):\n","        super().__init__()\n","        \n","        self.layers=clones(layer,N)\n","        self.norm=LayerNorm(layer.size)\n","    \n","    def forward(self,x,memory,curr_mask,tgt_mask):\n","        \n","        for layer in self.layers:\n","            x=layer(x,memory,curr_mask,tgt_mask)\n","            \n","        return self.norm(x)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FDHFdRvRCbgt","colab_type":"code","colab":{}},"source":["class DecoderLayer(nn.Module):\n","    \n","    def __init__(self,size,self_attn,src_attn,feed_forward,dropout):\n","        super().__init__()\n","        \n","        self.size=size\n","        self.self_attn=self_attn\n","        self.src_attn=src_attn\n","        self.feed_forward=feed_forward\n","        \n","        self.sublayer=clones(SublayerConnection(size,dropout),3)\n","        \n","    def forward(self,x,memory,src_mask,tgt_mask):\n","        \n","        m=memory\n","        x=self.sublayer[0](x,lambda x:self.self_attn(x,x,x,tgt_mask))\n","        x=self.sublayer[1](x,lambda x: self.src_attn(x,m,m,src_mask))\n","        return self.sublayer[2](x,self.feed_forward)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OWbh8piBCeC_","colab_type":"code","colab":{}},"source":["def attention(query,key,value,mask=None,dropout=None):\n","    \n","    d_k=query.size(-1)\n","\n","    scores=torch.matmul(query,key.transpose(-2,-1))/math.sqrt(d_k)\n","    \n","    if mask is not None:\n","        scores=scores.masked_fill(mask==0,-1e9)\n","        \n","    p_attn=F.softmax(scores,dim=-1)\n","    \n","    if dropout is not None:\n","        p_attn=dropout(p_attn)\n","        \n","    return torch.matmul(p_attn,value),p_attn"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZtHuOzFTCgie","colab_type":"code","colab":{}},"source":["class MultiHeadedAttention(nn.Module):\n","    \n","    def __init__(self,h,d_model,dropout=0.1):\n","        super().__init__()\n","        \n","        assert d_model%h==0\n","        \n","        self.d_k=d_model//h\n","        self.h=h\n","        self.linears=clones(nn.Linear(d_model,d_model),4)\n","        self.attn=None\n","        self.dropout=nn.Dropout(dropout)\n","        \n","    def forward(self,query,key,values,mask=None):\n","        \n","        if mask is not None:\n","            mask=mask.unsqueeze(1)\n","            \n","        nbatches=query.size(0)\n","        \n","        query,key,values=[l(x).view(nbatches,-1,self.h,self.d_k).transpose(1,2) for l, x in zip(self.linears,(query,key,values))]\n","        \n","        x,self.attn=attention(query,key,values,mask=mask,dropout=self.dropout)\n","        \n","        x=x.transpose(1,2).contiguous().view(nbatches,-1,self.h*self.d_k)\n","        \n","        return self.linears[-1](x)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GD9vxJrECkSp","colab_type":"code","colab":{}},"source":["class PositionwiseFeedForward(nn.Module):\n","    \n","    def __init__(self,d_model,d_ff,dropout=0.1):\n","        super().__init__()\n","        \n","        self.w_1=nn.Linear(d_model,d_ff)\n","        self.w_2=nn.Linear(d_ff,d_model)\n","        self.dropout=nn.Dropout(dropout)\n","        \n","    def forward(self,x):\n","        return self.w_2(self.dropout(F.relu(self.w_1(x))))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8Qna2tDeCmue","colab_type":"code","colab":{}},"source":["class Embeddings(nn.Module):\n","    \n","    def __init__(self,d_model,vocab):\n","        super().__init__()\n","        \n","        self.embed=nn.Embedding(vocab,d_model)\n","        self.d_model=d_model\n","    \n","    def forward(self,x):\n","#         print(x.device)\n","        return self.embed(x)*math.sqrt(self.d_model)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nCEQ9n0XCo7b","colab_type":"code","colab":{}},"source":["class PositionalEncoding(nn.Module):\n","    \n","    def __init__(self,d_model,dropout,max_len=5000):\n","        super().__init__()\n","        \n","        self.dropout=nn.Dropout(dropout)\n","        pe=torch.zeros(max_len,d_model,dtype=torch.float)\n","        position=torch.arange(0.,max_len).unsqueeze(1)\n","        div_term=torch.exp(torch.arange(0.,d_model,2)*-(math.log(10000.0)/d_model))\n","        \n","        pe[:,0::2]=torch.sin(position*div_term)\n","        pe[:,1::2]=torch.cos(position*div_term)\n","        \n","        pe=pe.unsqueeze(0)\n","        self.register_buffer('pe',pe)\n","        \n","    def forward(self,x):\n","        \n","        x=x+Variable(self.pe[:,:x.size(1)],requires_grad=False)\n","        return self.dropout(x)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CkLif2nLC5U6","colab_type":"code","colab":{}},"source":["\"\"\"\n","triu function generates a copy of matrix with elemens below kth diagonal zeroed.\n","The main diagonal is zeroeth diagonal above is first(k=1) and so on.\n","\n","Eg:\n","A=[[1,2,3],[4,5,6],[7,8,9]]\n","for above matrix:\n","triu(A,k=1)\n","will give [[0,2,3],[0,0,6],[0,0,0]]\n","\"\"\"\n","\n","def subsequent_mask(size):\n","    attn_shape=(1,size,size)\n","    mask=np.triu(np.ones(attn_shape),k=1).astype('uint8')\n","    \n","    return torch.from_numpy(mask)==0"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WPOL8rdMCruh","colab_type":"code","colab":{}},"source":["def make_model2(src_vocab,tgt_vocab,N=6,d_model=512,d_ff=2048,h=8,dropout=0.1):\n","    \n","    c=copy.deepcopy\n","    attn=MultiHeadedAttention(h,d_model)\n","    ff=PositionwiseFeedForward(d_model,d_ff,dropout)\n","    position=PositionalEncoding(d_model,dropout)\n","    model=EncoderDecoder(Encoder(EncoderLayer(d_model,c(attn),c(ff),dropout),N),\n","                        Decoder(DecoderLayer(d_model,c(attn),c(attn),c(ff),dropout),N),\n","                        nn.Sequential(Embeddings(d_model,src_vocab),c(position)),\n","                        nn.Sequential(Embeddings(d_model,tgt_vocab),c(position)),\n","                        Generator(d_model,tgt_vocab))\n","    \n","    for p in model.parameters():\n","        if p.dim()>1:\n","            nn.init.xavier_uniform_(p)\n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"K6TsrSL5CuNH","colab_type":"code","outputId":"34069e92-eab5-45ab-88c8-91d25c463e1b","executionInfo":{"status":"ok","timestamp":1586270368434,"user_tz":-330,"elapsed":10181,"user":{"displayName":"deepak goyal","photoUrl":"https://lh5.googleusercontent.com/-3gfEZruD9Sk/AAAAAAAAAAI/AAAAAAAACvg/54H4lMsdOgA/s64/photo.jpg","userId":"05164064759516400423"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["sample_model=make_model2(voc.num_words,voc.num_words,1,512,2048,8,0.1)\n","sample_model.to(device)\n","# print(sample_model)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["EncoderDecoder(\n","  (encoder): Encoder(\n","    (layers): ModuleList(\n","      (0): EncoderLayer(\n","        (attn): MultiHeadedAttention(\n","          (linears): ModuleList(\n","            (0): Linear(in_features=512, out_features=512, bias=True)\n","            (1): Linear(in_features=512, out_features=512, bias=True)\n","            (2): Linear(in_features=512, out_features=512, bias=True)\n","            (3): Linear(in_features=512, out_features=512, bias=True)\n","          )\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (sublayer): ModuleList(\n","          (0): SublayerConnection(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (norm): LayerNorm()\n","          )\n","          (1): SublayerConnection(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (norm): LayerNorm()\n","          )\n","        )\n","      )\n","    )\n","    (norm): LayerNorm()\n","  )\n","  (decoder): Decoder(\n","    (layers): ModuleList(\n","      (0): DecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linears): ModuleList(\n","            (0): Linear(in_features=512, out_features=512, bias=True)\n","            (1): Linear(in_features=512, out_features=512, bias=True)\n","            (2): Linear(in_features=512, out_features=512, bias=True)\n","            (3): Linear(in_features=512, out_features=512, bias=True)\n","          )\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (src_attn): MultiHeadedAttention(\n","          (linears): ModuleList(\n","            (0): Linear(in_features=512, out_features=512, bias=True)\n","            (1): Linear(in_features=512, out_features=512, bias=True)\n","            (2): Linear(in_features=512, out_features=512, bias=True)\n","            (3): Linear(in_features=512, out_features=512, bias=True)\n","          )\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (sublayer): ModuleList(\n","          (0): SublayerConnection(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (norm): LayerNorm()\n","          )\n","          (1): SublayerConnection(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (norm): LayerNorm()\n","          )\n","          (2): SublayerConnection(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (norm): LayerNorm()\n","          )\n","        )\n","      )\n","    )\n","    (norm): LayerNorm()\n","  )\n","  (source_embed): Sequential(\n","    (0): Embeddings(\n","      (embed): Embedding(7816, 512)\n","    )\n","    (1): PositionalEncoding(\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","  )\n","  (target_embed): Sequential(\n","    (0): Embeddings(\n","      (embed): Embedding(7816, 512)\n","    )\n","    (1): PositionalEncoding(\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","  )\n","  (generator): Generator(\n","    (projection): Linear(in_features=512, out_features=7816, bias=True)\n","  )\n",")"]},"metadata":{"tags":[]},"execution_count":32}]},{"cell_type":"code","metadata":{"id":"TZ0e9Y5cCwzw","colab_type":"code","outputId":"92dc2f78-b725-4b00-f1fe-c5d77264e95e","executionInfo":{"status":"ok","timestamp":1586270375254,"user_tz":-330,"elapsed":1026,"user":{"displayName":"deepak goyal","photoUrl":"https://lh5.googleusercontent.com/-3gfEZruD9Sk/AAAAAAAAAAI/AAAAAAAACvg/54H4lMsdOgA/s64/photo.jpg","userId":"05164064759516400423"}},"colab":{"base_uri":"https://localhost:8080/","height":69}},"source":["#Sample Run\n","source=torch.ones(5,12,dtype=torch.long,device=device)\n","target=torch.ones(5,12,dtype=torch.long,device=device)\n","source_mask=torch.ones(5,12,12,dtype=torch.long,device=device)\n","target_mask=torch.ones(5,12,12,dtype=torch.long,device=device)\n","out=sample_model(source,target,source_mask,target_mask)\n","print(\"-\"*80)\n","print(\"Output size: \"+str(out.shape))\n","print(\"-\"*80)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["--------------------------------------------------------------------------------\n","Output size: torch.Size([5, 12, 512])\n","--------------------------------------------------------------------------------\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Hb-TFfrCDn_q","colab_type":"text"},"source":["# Training\n"]},{"cell_type":"code","metadata":{"id":"EBjAhtyS16_t","colab_type":"code","colab":{}},"source":["def F1_score(x,y):\n","#     print(x)\n","#     print(y)\n","    inp=torch.argmax(x,dim=-1)\n","    score=(inp==y).sum()\n","    score=score/(x.size()[0]*x.size()[1])\n","    return score"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KBz2k8ywDqZ4","colab_type":"code","colab":{}},"source":["def data_generation(pairs,batch_size,n_batches):\n","    \n","    # sample_batches=[batch2TrainData(voc,[random.choice(pairs) for _ in range(batch_size)]) for _ in range(n_batches)]\n","    sample_batches=[]\n","    batches=[]\n","    for i in range(n_batches):\n","        curr_batch=[]\n","        for j in range(batch_size):\n","            curr_id=i*batch_size+j\n","            curr_batch.append(pairs[curr_id])\n","\n","        sample_batches.append(batch2TrainData(voc,curr_batch))\n","\n","    \n","    for i in range(n_batches):\n","        batches.append(Batch(sample_batches[i][1],sample_batches[i][-1]))\n","    \n","    return batches"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"upPkHvWXECA2","colab_type":"code","colab":{}},"source":["class Batch:\n","    \"Object for holding a batch of data with mask during training.\"\n","    def __init__(self, src, trg=None, pad=0):\n","        src=src.to(torch.int64)\n","        trg=trg.to(torch.int64)\n","        self.src = src\n","        self.src_mask = (src != pad).unsqueeze(-2)\n","        if trg is not None:\n","            self.trg = trg[:, :-1]\n","            self.trg_y = trg[:, 1:]\n","            self.trg_mask = \\\n","                self.make_std_mask(self.trg, pad)\n","            self.ntokens = (self.trg_y != pad).data.sum()\n","        self.src.to(device)\n","        self.trg.to(device)\n","        self.src_mask.to(device)\n","        self.trg_mask.to(device)\n","    \n","    @staticmethod\n","    def make_std_mask(tgt, pad):\n","        \"Create a mask to hide padding and future words.\"\n","        tgt_mask = (tgt != pad).unsqueeze(-2)\n","        tgt_mask = tgt_mask & Variable(\n","            subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data))\n","        return tgt_mask"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YRV3rCOGEMFn","colab_type":"code","colab":{}},"source":["def run_single_batch(data,model,loss_compute):\n","    \n","    start_time=time.time()\n","    \n","    total_tokens=0\n","    total_loss=0\n","    tokens=0\n","    \n","    source=data.src\n","    source_mask=data.src_mask\n","    target=data.trg\n","    target_mask=data.trg_mask\n","    target_y=data.trg_y\n","    \n","    source=source.to(device)\n","    target=target.to(device)\n","    source_mask=source_mask.to(device)\n","    target_mask=target_mask.to(device)\n","    target_y=target_y.to(device)\n","\n","    out=model(source,target,source_mask,target_mask)\n","\n","    loss,f1_score,ppl=loss_compute(out,target_y,data.ntokens)\n","    \n","    return loss.item(),f1_score.item(),data.ntokens.item(),ppl"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"a5Kfbj2TEXyp","colab_type":"code","colab":{}},"source":["\n","class SimpleLossCompute:\n","    \"A simple loss compute and train function.\"\n","    def __init__(self, generator, opt=None):\n","        self.generator = generator\n","        self.criterion = nn.CrossEntropyLoss()\n","        self.opt = opt\n","        \n","    def __call__(self, x, y, norm):\n","        x = self.generator(x)\n","        sentence_length=x.size()[1]\n","#         print(str(x.size())+\" \"+str(y.size()))\n","        f1_score=F1_score(x,y)\n","#         ppl=perplexity(x,y)\n","        \n","        loss = self.criterion(x.contiguous().view(-1, x.size(-1)), \n","                              y.contiguous().view(-1)) \n","        loss.backward()\n","        _=nn.utils.clip_grad_norm_(model.parameters(),1.0)\n","        # plot_grad_flow(model.named_parameters())\n","        if self.opt is not None:\n","            self.opt.step()\n","            self.opt.zero_grad()\n","            \n","        return loss,f1_score,math.exp((loss.item()*norm.item())/sentence_length)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FixxgorW2fkQ","colab_type":"code","colab":{}},"source":["def training(batches,model,n_epochs,n_batches,model_opt,loadFile,save_every):\n","    \n","    start_epoch=0\n","    total_time=0\n","    if loadFile:\n","        start_epoch=torch.load(loadFile)[\"epoch\"]\n","        total_time=torch.load(loadFile)[\"time\"]\n","        start_epoch=start_epoch+1\n","    \n","    for epoch in range(start_epoch,n_epochs):\n","        t1=time.time()\n","        loss=0\n","        f1_score=0\n","        ppl=0\n","        n_tokens=0\n","        for i in range(n_batches):\n","            current_batch=batches[i]\n","            loss_val,current_f1_score,current_tokens,curr_ppl=run_single_batch(current_batch,model,SimpleLossCompute(model.generator, model_opt))\n","            loss+=loss_val\n","            f1_score+=current_f1_score\n","            n_tokens+=current_tokens\n","            ppl+=curr_ppl\n","\n","        loss=loss/(n_batches)\n","        ppl=ppl/n_batches\n","        f1_score=f1_score/n_batches\n","        \n","        if epoch%save_every==0:\n","            directory=os.path.join(save_dir,'transformer','cornell-movie')\n","            if not os.path.exists(directory):\n","                os.makedirs(directory)\n","            torch.save({\n","                \"epoch\":epoch,\n","                \"model\":model.state_dict(),\n","                \"opt\":model_opt.state_dict(),\n","                \"loss\":loss,\n","                \"ppl\":ppl,\n","                \"f1\":f1_score,\n","                \"time\":total_time\n","            },os.path.join(directory,'{}_{}.tar'.format(epoch,'checkpoint')))\n","        \n","        print(\"=\"*100)\n","        print(\"| End of Epoch : \"+str(epoch)+\"| Loss Value: \"+str(loss)+\"| F1 Score: \"+str(f1_score/n_batches)+\"| PPL: \"+str(ppl)+\"| Time Took: \"+\n","              str(time.time()-t1)+\" |\")\n","        print(\"=\"*100)\n","        total_time+=time.time()-t1\n","\n","    print(\"| Training Finished | Total Training Time: \"+str(total_time)+\" |\")\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kwbJnrp1V-R8","colab_type":"code","colab":{}},"source":["def get_parameter_count(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8vivb-Oon46j","colab_type":"code","colab":{}},"source":["### Hyperparameters\n","\n","N=2\n","d_model=256\n","d_ff=512\n","num_head=8\n","dropout=0.1\n","\n","batch_size=10\n","n_epochs=1000"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ez4Swx52EfUT","colab_type":"code","outputId":"1f1ad7d1-65d5-449b-e141-9ff0a4fe560a","executionInfo":{"status":"ok","timestamp":1586594198445,"user_tz":-330,"elapsed":11329,"user":{"displayName":"Deepak Goyal","photoUrl":"","userId":"12155914074048018284"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["batches=data_generation(pairs,10,1000)\n","print(len(batches))\n","# for i in range(5):\n","#     print(str(batches[i].src)+\" \"+str(batches[i].trg))\n"],"execution_count":40,"outputs":[{"output_type":"stream","text":["1000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"b67TIlqdEj-O","colab_type":"code","outputId":"3d299d8e-9ddc-4dd3-df47-c0f243e72128","executionInfo":{"status":"ok","timestamp":1586606320835,"user_tz":-330,"elapsed":4041614,"user":{"displayName":"Deepak Goyal","photoUrl":"","userId":"12155914074048018284"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["print(\"Initialising and creating models....\")\n","V=voc.num_words\n","t1=time.time()\n","# criterion=LabelSmoothing()\n","\n","model=make_model(V,V,N,d_model,d_ff,num_head,dropout)\n","model.to(device)\n","\n","learnable_parameter_count=get_parameter_count(model)\n","\n","print(\"Number of learnable parameters for this model: \"+str(learnable_parameter_count))\n","\n","# model_opt=torch.optim.Adam(model.parameters(),lr=0.0001,betas=(0.9,0.988),eps=1e-9)\n","model_opt = torch.optim.Adam(model.parameters(), lr=0.00001, betas=(0.9, 0.98), eps=1e-9)\n","print(\"=\"*100)\n","print(\"Creating Models took: \"+str(time.time()-t1))\n","\n","save_dir='/content/drive/My Drive/Model Data'\n","# loadFile=os.path.join(save_dir,'transformer','cornell-movie','2_checkpoint')\n","# loadFile=\"C:\\\\Users\\\\deepa\\\\Conversational Agents\\\\transformer\\\\cornell-movie\\\\1_checkpoint.tar\"\n","loadFile='/content/drive/My Drive/Model Data/transformer/cornell-movie/500_checkpoint.tar'\n","# loadFile=None\n","\n","if(loadFile):\n","    checkpoint=torch.load(loadFile)\n","    model.load_state_dict(checkpoint['model'])\n","    model_opt.load_state_dict(checkpoint['opt'])\n","    \n","    \n","\n","model.train()\n","training(batches,model,601,1000,model_opt,loadFile,5)"],"execution_count":42,"outputs":[{"output_type":"stream","text":["Initialising and creating models....\n","Number of learnable parameters for this model: 8647304\n","====================================================================================================\n","Creating Models took: 0.16564512252807617\n","====================================================================================================\n","| End of Epoch : 501| Loss Value: 0.6386213672757148| F1 Score: 0.0| PPL: 116.42198186739455| Time Took: 40.456676721572876 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 502| Loss Value: 0.6366421922296286| F1 Score: 0.0| PPL: 109.6402558188792| Time Took: 39.55906057357788 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 503| Loss Value: 0.6354856252521276| F1 Score: 0.0| PPL: 100.23327455194874| Time Took: 39.609050273895264 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 504| Loss Value: 0.6361039760857821| F1 Score: 0.0| PPL: 121.83285443509297| Time Took: 39.566983461380005 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 505| Loss Value: 0.6332014010697603| F1 Score: 0.0| PPL: 107.16506854834992| Time Took: 40.01509714126587 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 506| Loss Value: 0.6308814351856709| F1 Score: 0.0| PPL: 108.01194413563188| Time Took: 41.41896033287048 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 507| Loss Value: 0.6275224740505219| F1 Score: 0.0| PPL: 108.53666152165572| Time Took: 39.276827812194824 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 508| Loss Value: 0.6311605100035668| F1 Score: 0.0| PPL: 104.08119262788252| Time Took: 40.04330611228943 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 509| Loss Value: 0.6282434673905373| F1 Score: 0.0| PPL: 99.4007646779994| Time Took: 40.26719427108765 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 510| Loss Value: 0.6261762188673019| F1 Score: 0.0| PPL: 102.79588539910579| Time Took: 40.57305026054382 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 511| Loss Value: 0.6257503722310066| F1 Score: 0.0| PPL: 100.19098632419379| Time Took: 39.99255180358887 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 512| Loss Value: 0.6229951232224703| F1 Score: 0.0| PPL: 98.33386725956635| Time Took: 40.109023571014404 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 513| Loss Value: 0.6227430989593268| F1 Score: 0.0| PPL: 100.5894541731883| Time Took: 39.973761558532715 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 514| Loss Value: 0.6204593805968761| F1 Score: 0.0| PPL: 97.66682300718793| Time Took: 39.957677364349365 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 515| Loss Value: 0.6184372026622296| F1 Score: 0.0| PPL: 94.30801921643852| Time Took: 40.2745897769928 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 516| Loss Value: 0.618504270605743| F1 Score: 0.0| PPL: 90.25371355776421| Time Took: 40.75331473350525 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 517| Loss Value: 0.6168405698388815| F1 Score: 0.0| PPL: 93.68665394069886| Time Took: 39.896296977996826 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 518| Loss Value: 0.6163650812804699| F1 Score: 0.0| PPL: 97.11357408137879| Time Took: 39.574185371398926 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 519| Loss Value: 0.6164825209677219| F1 Score: 0.0| PPL: 98.74052846896448| Time Took: 39.920912981033325 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 520| Loss Value: 0.6113639461249113| F1 Score: 0.0| PPL: 83.46220626464604| Time Took: 40.24726152420044 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 521| Loss Value: 0.611765526548028| F1 Score: 0.0| PPL: 91.10586349698889| Time Took: 40.35033297538757 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 522| Loss Value: 0.6111640184521675| F1 Score: 0.0| PPL: 95.72738561051248| Time Took: 40.62567639350891 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 523| Loss Value: 0.6115157015770674| F1 Score: 0.0| PPL: 86.5387466579959| Time Took: 40.56659531593323 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 524| Loss Value: 0.6080984975546598| F1 Score: 0.0| PPL: 84.97507526612657| Time Took: 40.688551902770996 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 525| Loss Value: 0.605805113106966| F1 Score: 0.0| PPL: 86.19871368485303| Time Took: 40.97412919998169 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 526| Loss Value: 0.605453268006444| F1 Score: 0.0| PPL: 82.80654191311075| Time Took: 40.06361389160156 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 527| Loss Value: 0.604725090071559| F1 Score: 0.0| PPL: 86.05699436962959| Time Took: 40.26983976364136 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 528| Loss Value: 0.6055292520597577| F1 Score: 0.0| PPL: 83.26501561847321| Time Took: 40.38421630859375 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 529| Loss Value: 0.6044028168171645| F1 Score: 0.0| PPL: 90.61792323484141| Time Took: 40.66233468055725 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 530| Loss Value: 0.5999398062974215| F1 Score: 0.0| PPL: 82.77313321838763| Time Took: 39.97122931480408 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 531| Loss Value: 0.5986064169406891| F1 Score: 0.0| PPL: 76.18461844427642| Time Took: 40.98992347717285 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 532| Loss Value: 0.5993060404956341| F1 Score: 0.0| PPL: 82.33776986206296| Time Took: 40.1897976398468 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 533| Loss Value: 0.5953844881802798| F1 Score: 0.0| PPL: 76.70056201266621| Time Took: 40.12533617019653 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 534| Loss Value: 0.5952698553204536| F1 Score: 0.0| PPL: 80.13830185017079| Time Took: 40.0301775932312 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 535| Loss Value: 0.5933495313674212| F1 Score: 0.0| PPL: 74.91037607022731| Time Took: 40.05004954338074 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 536| Loss Value: 0.5933894664496183| F1 Score: 0.0| PPL: 81.55859097970823| Time Took: 40.6427047252655 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 537| Loss Value: 0.5957192489132285| F1 Score: 0.0| PPL: 76.47969104458679| Time Took: 40.529815912246704 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 538| Loss Value: 0.59015617595613| F1 Score: 0.0| PPL: 78.92061052858195| Time Took: 40.6789333820343 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 539| Loss Value: 0.5913000514954329| F1 Score: 0.0| PPL: 72.97980750389819| Time Took: 40.51485872268677 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 540| Loss Value: 0.58696819165349| F1 Score: 0.0| PPL: 72.59519617649026| Time Took: 40.576215505599976 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 541| Loss Value: 0.5884683479964733| F1 Score: 0.0| PPL: 74.1504117499101| Time Took: 40.05999255180359 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 542| Loss Value: 0.5852552473172545| F1 Score: 0.0| PPL: 71.90059534938673| Time Took: 41.188751220703125 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 543| Loss Value: 0.5848524365723133| F1 Score: 0.0| PPL: 70.21418500014971| Time Took: 41.0901141166687 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 544| Loss Value: 0.5862155680432916| F1 Score: 0.0| PPL: 71.64904740310952| Time Took: 40.48419499397278 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 545| Loss Value: 0.5847249910756945| F1 Score: 0.0| PPL: 70.42140026150302| Time Took: 41.10239386558533 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 546| Loss Value: 0.5839016495794058| F1 Score: 0.0| PPL: 71.47761876784696| Time Took: 41.86883306503296 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 547| Loss Value: 0.5827142718359828| F1 Score: 0.0| PPL: 72.10279810516433| Time Took: 40.8994824886322 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 548| Loss Value: 0.5802300836294889| F1 Score: 1e-06| PPL: 69.84708152422175| Time Took: 41.39664888381958 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 549| Loss Value: 0.5793392696157098| F1 Score: 1e-06| PPL: 69.39318242373787| Time Took: 41.38462710380554 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 550| Loss Value: 0.5822391653954982| F1 Score: 0.0| PPL: 69.33197764936963| Time Took: 41.44328761100769 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 551| Loss Value: 0.5788905118703842| F1 Score: 0.0| PPL: 66.38347669477105| Time Took: 41.003522634506226 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 552| Loss Value: 0.5758844366967678| F1 Score: 0.0| PPL: 68.01375343378976| Time Took: 41.25377154350281 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 553| Loss Value: 0.5772843555808067| F1 Score: 0.0| PPL: 68.43825265562583| Time Took: 40.44194459915161 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 554| Loss Value: 0.5766324441432953| F1 Score: 0.0| PPL: 65.42346911296188| Time Took: 40.87817335128784 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 555| Loss Value: 0.5748740593492985| F1 Score: 0.0| PPL: 64.74531655698705| Time Took: 41.56988763809204 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 556| Loss Value: 0.5723748189806939| F1 Score: 0.0| PPL: 62.84989917109547| Time Took: 40.989564418792725 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 557| Loss Value: 0.5738041134476661| F1 Score: 0.0| PPL: 63.06075270055356| Time Took: 41.13970756530762 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 558| Loss Value: 0.5724506802931428| F1 Score: 0.0| PPL: 65.6968271576777| Time Took: 40.687800884246826 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 559| Loss Value: 0.5723512853160501| F1 Score: 0.0| PPL: 62.10078789649618| Time Took: 40.730567932128906 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 560| Loss Value: 0.5700785796120763| F1 Score: 0.0| PPL: 64.21466221569045| Time Took: 41.55165648460388 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 561| Loss Value: 0.5702467681542039| F1 Score: 0.0| PPL: 61.46800403761111| Time Took: 40.977115631103516 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 562| Loss Value: 0.5684569771066308| F1 Score: 0.0| PPL: 63.36733799190296| Time Took: 40.49035406112671 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 563| Loss Value: 0.567852175578475| F1 Score: 0.0| PPL: 58.935194042216416| Time Took: 40.92420816421509 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 564| Loss Value: 0.5695785562247038| F1 Score: 0.0| PPL: 62.471517862915704| Time Took: 39.88699007034302 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 565| Loss Value: 0.5661327758431435| F1 Score: 0.0| PPL: 58.08558539318659| Time Took: 40.0302152633667 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 566| Loss Value: 0.5668780316933989| F1 Score: 0.0| PPL: 59.154675384552334| Time Took: 41.32900810241699 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 567| Loss Value: 0.5626003436818718| F1 Score: 1e-06| PPL: 58.789863622280244| Time Took: 40.96549654006958 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 568| Loss Value: 0.5653063658177853| F1 Score: 1e-06| PPL: 59.79797434421227| Time Took: 40.277350187301636 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 569| Loss Value: 0.5602886981070042| F1 Score: 0.0| PPL: 58.19782502859599| Time Took: 40.481865882873535 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 570| Loss Value: 0.5595822165161372| F1 Score: 1e-06| PPL: 55.442194527387194| Time Took: 40.8961181640625 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 571| Loss Value: 0.5594244272857904| F1 Score: 0.0| PPL: 60.136566181420655| Time Took: 40.17570996284485 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 572| Loss Value: 0.5568693471625448| F1 Score: 0.0| PPL: 55.74950193320426| Time Took: 40.79883861541748 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 573| Loss Value: 0.559781770542264| F1 Score: 0.0| PPL: 55.66122897178272| Time Took: 39.958086252212524 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 574| Loss Value: 0.5560639937371016| F1 Score: 0.0| PPL: 52.242332388569906| Time Took: 39.385969400405884 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 575| Loss Value: 0.5588065349534154| F1 Score: 0.0| PPL: 55.56389092213925| Time Took: 40.303011417388916 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 576| Loss Value: 0.5565320647805929| F1 Score: 0.0| PPL: 56.3733046213686| Time Took: 39.93450355529785 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 577| Loss Value: 0.5576134389713406| F1 Score: 0.0| PPL: 56.06821300144215| Time Took: 39.90961456298828 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 578| Loss Value: 0.5541674232557416| F1 Score: 0.0| PPL: 57.13998380414085| Time Took: 40.067760705947876 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 579| Loss Value: 0.5529599477872252| F1 Score: 0.0| PPL: 51.70716170977451| Time Took: 39.80968761444092 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 580| Loss Value: 0.5514762361943721| F1 Score: 0.0| PPL: 53.77483409942819| Time Took: 40.95147728919983 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 581| Loss Value: 0.5535308263674379| F1 Score: 1e-06| PPL: 53.49058779223003| Time Took: 40.528847217559814 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 582| Loss Value: 0.5486998858973384| F1 Score: 0.0| PPL: 53.8940939970051| Time Took: 39.63538718223572 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 583| Loss Value: 0.5495926517248154| F1 Score: 0.0| PPL: 53.11794496848696| Time Took: 39.6320013999939 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 584| Loss Value: 0.5476653413176537| F1 Score: 0.0| PPL: 52.654325605519524| Time Took: 40.042723655700684 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 585| Loss Value: 0.5499746554642916| F1 Score: 0.0| PPL: 55.62800090843224| Time Took: 39.97559976577759 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 586| Loss Value: 0.5489046172201634| F1 Score: 0.0| PPL: 52.172612811671456| Time Took: 40.1202118396759 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 587| Loss Value: 0.5487592008039355| F1 Score: 1e-06| PPL: 51.5231276253391| Time Took: 39.73704195022583 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 588| Loss Value: 0.5462564260736108| F1 Score: 0.0| PPL: 51.22534997281477| Time Took: 40.22960805892944 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 589| Loss Value: 0.5450567823201418| F1 Score: 0.0| PPL: 50.11455910627161| Time Took: 39.70297336578369 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 590| Loss Value: 0.5432315692156553| F1 Score: 0.0| PPL: 50.96871393807443| Time Took: 40.333696365356445 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 591| Loss Value: 0.5434073012471199| F1 Score: 0.0| PPL: 50.087905969615825| Time Took: 40.086660623550415 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 592| Loss Value: 0.5441794624701143| F1 Score: 0.0| PPL: 48.952887611258596| Time Took: 40.00641655921936 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 593| Loss Value: 0.5396356333196163| F1 Score: 1e-06| PPL: 48.82060088325874| Time Took: 38.88407349586487 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 594| Loss Value: 0.5418262414559722| F1 Score: 0.0| PPL: 49.77916674020243| Time Took: 39.76022386550903 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 595| Loss Value: 0.537981284275651| F1 Score: 0.0| PPL: 48.705595387850266| Time Took: 40.01428699493408 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 596| Loss Value: 0.5405987102910876| F1 Score: 0.0| PPL: 47.84751278175285| Time Took: 40.46387600898743 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 597| Loss Value: 0.5382485403269529| F1 Score: 0.0| PPL: 46.55581125822882| Time Took: 40.000744342803955 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 598| Loss Value: 0.5373059145957232| F1 Score: 0.0| PPL: 48.075529341397186| Time Took: 40.610905170440674 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 599| Loss Value: 0.5338474792316557| F1 Score: 0.0| PPL: 43.486315179162844| Time Took: 40.32816410064697 |\n","====================================================================================================\n","====================================================================================================\n","| End of Epoch : 600| Loss Value: 0.5364424723833799| F1 Score: 1e-06| PPL: 48.20141542533309| Time Took: 39.77521777153015 |\n","====================================================================================================\n","| Training Finished | Total Training Time: 22500.17640542984 |\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"eSNOkTzVEoLO","colab_type":"code","colab":{}},"source":["def greedy_decode(model, src, src_mask, max_len, start_symbol):\n","    memory = model.encode(src, src_mask)\n","    ys = torch.ones(1, 1).fill_(start_symbol).type_as(src.data)\n","    for i in range(max_len-1):\n","        out = model.decode(memory, src_mask, \n","                           Variable(ys), \n","                           Variable(subsequent_mask(ys.size(1))\n","                                    .type_as(src.data)))\n","        prob = model.generator(out[:, -1])\n","        _, next_word = torch.max(prob, dim = 1)\n","        next_word = next_word.data[0]\n","        ys = torch.cat([ys, \n","                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1)\n","    return ys"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Kq5eD4uOF65q","colab_type":"code","colab":{}},"source":["model.eval()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dAj4kZlOF96b","colab_type":"code","colab":{}},"source":["for i in range(10):\n","    batch=batches[i]\n","    for i in range(5):\n","        source=batch.src[i].view(-1,12).to(device)\n","        source_mask=batch.src_mask[i].view(1,-1,12).to(device)\n","        output=greedy_decode(model,source,source_mask,10,1)\n","        src=batch.src[i].view(-1)\n","        trg=batch.trg[i].view(-1)\n","        pred=output.view(-1)\n","        # print(src.size())\n","        # for id in src:\n","        #     print(id)\n","        src_sentence=[voc.index2word[id.item()] for id in src]\n","        trg_sentence=[voc.index2word[id.item()] for id in trg]\n","        pred_sentence=[voc.index2word[id.item()] for id in pred]\n","        print(src_sentence)\n","        print(trg_sentence)\n","        print(pred_sentence)\n","        print(\"-\"*80)\n","#         print(\"-\"*80)\n","#         print(str(output)+\" \"+str(batch.src[i])+\" \"+str(batch.trg[i]))\n","    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"R2lay0ahGArk","colab_type":"code","colab":{}},"source":["def evaluate(model,sentence,voc,max_length):\n","\n","    tokenised_sentence=[indexesFromSentence(voc,sentence)]\n","    input_sentence=torch.LongTensor(tokenised_sentence)\n","    input_sentence_mask=(input_sentence!=0)\n","\n","    input_sentence=input_sentence.view(-1,max_length+2)\n","    input_sentence_mask=input_sentence_mask.view(1,-1,max_length+2)\n","\n","    output_sentence=greedy_decode(model,input_sentence.to(device),input_sentence_mask.to(device),max_length,1)\n","    output_sentence=output_sentence.view(-1)\n","\n","    decoded_words=[voc.index2word[id.item()] for id in output_sentence]\n","\n","    return decoded_words\n","\n","def evaluateInput(model,voc,max_length):\n","    input_sentence=''\n","\n","    while(1):\n","        try:\n","            input_sentence=input(\"Human: \")\n","            if input_sentence=='q' or input_sentence=='quit':\n","                break\n","            input_sentence=normalizeString(input_sentence)\n","\n","            output_words=evaluate(model,input_sentence,voc,max_length)\n","            output_words[:]=[x for x in output_words if not(x==\"SOS\" or x==\"EOS\" or x==\"PAD\")]\n","\n","            print(\"Bot:\",\" \".join(output_words))\n","\n","        except KeyError:\n","            print(\"Unkown Word..!!\")\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ho67LwPrReam","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":451},"outputId":"bcbe6beb-3df4-49b3-f44b-ca436ac78705","executionInfo":{"status":"ok","timestamp":1586543049353,"user_tz":-330,"elapsed":198946,"user":{"displayName":"Deepak Goyal","photoUrl":"","userId":"12155914074048018284"}}},"source":["evaluateInput(model,voc,10)"],"execution_count":62,"outputs":[{"output_type":"stream","text":["Human: hi\n","Bot: hi .\n","Human: how are you\n","Bot: i m sorry .\n","Human: where do you live\n","Bot: i m not in the car .\n","Human: which car do you drive\n","Bot: the best .\n","Human: what do you like to eat\n","Bot: i don t know .\n","Human: where do you live\n","Bot: i m not in the car .\n","Human: do you have any friends\n","Bot: i m sorry i m not .\n","Human: do you love me\n","Bot: i m not .\n","Human: what do you do\n","Bot: i don t know .\n","Human: any movie\n","Bot: what ?\n","Human: okay\n","Bot: i m sorry .\n","Human: do you smoke\n","Bot: i don t know .\n","Human: q\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4dpHu4jLR2n-","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}