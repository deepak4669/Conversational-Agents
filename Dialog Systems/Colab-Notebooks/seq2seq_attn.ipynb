{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"seq2seq_withattn.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNBzXSIvTT71mg9OHoBwEo0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"3JqhjAnVSaCY","colab_type":"code","outputId":"8c3db952-fd01-4487-bcf1-c6d99330ba13","executionInfo":{"status":"ok","timestamp":1587494905995,"user_tz":-330,"elapsed":43399,"user":{"displayName":"deepak goyal","photoUrl":"https://lh5.googleusercontent.com/-3gfEZruD9Sk/AAAAAAAAAAI/AAAAAAAACvg/54H4lMsdOgA/s64/photo.jpg","userId":"05164064759516400423"}},"colab":{"base_uri":"https://localhost:8080/","height":124}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8LceekfUUWxL","colab_type":"code","outputId":"36cc8bda-378d-458f-ec22-a14542f73f1c","executionInfo":{"status":"ok","timestamp":1587494918045,"user_tz":-330,"elapsed":6432,"user":{"displayName":"deepak goyal","photoUrl":"https://lh5.googleusercontent.com/-3gfEZruD9Sk/AAAAAAAAAAI/AAAAAAAACvg/54H4lMsdOgA/s64/photo.jpg","userId":"05164064759516400423"}},"colab":{"base_uri":"https://localhost:8080/","height":312}},"source":["!nvidia-smi \n"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Tue Apr 21 18:48:35 2020       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 440.64.00    Driver Version: 418.67       CUDA Version: 10.1     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   40C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                       GPU Memory |\n","|  GPU       PID   Type   Process name                             Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"CRbeP6r1VmTD","colab_type":"code","colab":{}},"source":["#Pre-Processing\n","import os\n","import re\n","import torch\n","import random\n","import itertools\n","\n","#Model\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","\n","import numpy as np\n","\n","# For visualising metrics\n","# from visdom import Visdom\n","\n","# For visualising gradients plot\n","import matplotlib.pyplot as plt\n","from matplotlib.lines import Line2D\n","\n","import copy\n","import math\n","import time\n","from collections import namedtuple, Counter\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5i5yWtmBH7LU","colab_type":"code","outputId":"96cfaaf2-6db7-4e7d-9ba7-d9d54059f87e","executionInfo":{"status":"ok","timestamp":1587494926988,"user_tz":-330,"elapsed":1469,"user":{"displayName":"deepak goyal","photoUrl":"https://lh5.googleusercontent.com/-3gfEZruD9Sk/AAAAAAAAAAI/AAAAAAAACvg/54H4lMsdOgA/s64/photo.jpg","userId":"05164064759516400423"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","# device=torch.device(\"cpu\")\n","print(\"The device found: \"+str(device))"],"execution_count":4,"outputs":[{"output_type":"stream","text":["The device found: cuda\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9D390QjbICa9","colab_type":"code","colab":{}},"source":["def plot_grad_flow(named_parameters):\n","    \"\"\"\n","        Plotting gradient flow across various layers\n","        Thanks to: https://discuss.pytorch.org/t/check-gradient-flow-in-network/15063/2\n","    \"\"\"   \n","    ave_grads = []\n","    layers = []\n","    for n, p in named_parameters:\n","        if(p.requires_grad) and (\"bias\" not in n):\n","            layers.append(n)\n","            ave_grads.append(p.grad.abs().mean())\n","    plt.plot(ave_grads, alpha=0.3, color=\"b\")\n","    plt.hlines(0, 0, len(ave_grads)+1, linewidth=1, color=\"k\" )\n","    plt.xticks(range(0,len(ave_grads), 1), layers, rotation=\"vertical\")\n","    plt.xlim(xmin=0, xmax=len(ave_grads))\n","    plt.xlabel(\"Layers\")\n","    plt.ylabel(\"average gradient\")\n","    plt.title(\"Gradient flow\")\n","    plt.grid(True)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z565czS1IFdU","colab_type":"text"},"source":["# Preprocessing\n"]},{"cell_type":"code","metadata":{"id":"c9AkyMgfWetG","colab_type":"code","outputId":"15e77819-b574-4e79-99d5-13dbdb6c4682","executionInfo":{"status":"ok","timestamp":1587494954043,"user_tz":-330,"elapsed":1268,"user":{"displayName":"deepak goyal","photoUrl":"https://lh5.googleusercontent.com/-3gfEZruD9Sk/AAAAAAAAAAI/AAAAAAAACvg/54H4lMsdOgA/s64/photo.jpg","userId":"05164064759516400423"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["path='/content/drive/My Drive/Data'\n","dataset='cornell movie-dialogs corpus'\n","\n","data_folder=os.path.join(path,dataset)\n","\n","print(\"The final data corpus folder: \"+str(data_folder))"],"execution_count":6,"outputs":[{"output_type":"stream","text":["The final data corpus folder: /content/drive/My Drive/Data/cornell movie-dialogs corpus\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"sNFBnhpYWhB4","colab_type":"code","colab":{}},"source":["def get_lines_conversations():\n","    \"\"\"\n","    Loads movie lines and conversations from the dataset.\n","    \n","    data_folder: Destination where conversations and lines are stored.\n","    \n","    movie_lines: Consist of movie lines as given by the dataset.\n","    movie_conversations: Consist of movie conversations as given by the dataset.\n","    \n","    \"\"\"\n","    movie_lines=[]\n","    movie_conversations=[]\n","\n","    with open(os.path.join(data_folder,'movie_lines.txt'),'r',encoding='iso-8859-1') as f:\n","        for line in f:\n","            movie_lines.append(line)\n","    \n","    with open(os.path.join(data_folder,'movie_conversations.txt'),'r', encoding='iso-8859-1') as f:\n","        for line in f:\n","            movie_conversations.append(line)\n","                                       \n","\n","    return movie_lines,movie_conversations"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_j92fLQrW1pq","colab_type":"code","outputId":"eb211de4-f170-4894-94f9-cba7f7873ca3","executionInfo":{"status":"ok","timestamp":1587494963927,"user_tz":-330,"elapsed":4176,"user":{"displayName":"deepak goyal","photoUrl":"https://lh5.googleusercontent.com/-3gfEZruD9Sk/AAAAAAAAAAI/AAAAAAAACvg/54H4lMsdOgA/s64/photo.jpg","userId":"05164064759516400423"}},"colab":{"base_uri":"https://localhost:8080/","height":173}},"source":["t1=time.time()\n","print(\"Extracting movie lines and movie conversations...\")\n","movie_lines,movie_conversations=get_lines_conversations()\n","\n","print(\"Number of distinct lines: \"+str(len(movie_lines)))\n","print(\"Number of conversations: \"+str(len(movie_conversations)))\n","print(\"Average Number of lines per conversations: \"+str(len(movie_lines)/len(movie_conversations)))\n","\n","print(movie_lines[0])\n","print(movie_conversations[0])\n","\n","print(\"Extracting took place in: \"+str(time.time()-t1))"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Extracting movie lines and movie conversations...\n","Number of distinct lines: 304713\n","Number of conversations: 83097\n","Average Number of lines per conversations: 3.6669554857576085\n","L1045 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ They do not!\n","\n","u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L194', 'L195', 'L196', 'L197']\n","\n","Extracting took place in: 2.8168225288391113\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"eG0A5czeXz9H","colab_type":"code","colab":{}},"source":["exceptions=[]\n","def loadLines(movie_lines,fields):\n","    lines={}\n","    for lineid in range(len(movie_lines)):\n","        \n","        line=movie_lines[lineid]\n","        values=line.split(\" +++$+++ \")\n","        \n","        \n","        lineVals={}\n","        \n","        # print(\"values\"+str(len(values)))\n","        # print(\"fields\"+str(len(fields)))\n","              \n","        for i,field in enumerate(fields):\n","            try:\n","                lineVals[field]=values[i]\n","            except:\n","                print(\"Exception: \"+str(len(values)))\n","                exceptions.append(lineid)\n","        \n","        lines[lineVals['lineID']]=lineVals\n","    \n","    return lines\n","\n","def loadConversations(movie_conversations,lines,fields):\n","    conversations=[]\n","    \n","    for convo in movie_conversations:\n","        values=convo.split(\" +++$+++ \")\n","        conVals={}\n","       \n","        for i,field in enumerate(fields):\n","            conVals[field]=values[i]\n","        \n","        lineIDs=eval(conVals[\"utteranceIDs\"])\n","        \n","        conVals[\"lines\"]=[]\n","        \n","        for lineID in lineIDs:\n","            conVals[\"lines\"].append(lines[lineID])\n","        conversations.append(conVals)\n","        \n","    return conversations\n","\n","def sentencePairs(conversations):\n","    qr_pairs=[]\n","    \n","    for conversation in conversations:\n","        for i in range(len(conversation[\"lines\"])-1):\n","            query=conversation[\"lines\"][i][\"text\"].strip()\n","            response=conversation[\"lines\"][i+1][\"text\"].strip()\n","            \n","            if query and response:\n","                qr_pairs.append([query,response])\n","        \n","    return qr_pairs"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Bh37EXhxYQAJ","colab_type":"code","outputId":"fcf02a14-eb48-4655-bc14-36623159ceb9","executionInfo":{"status":"ok","timestamp":1587494975570,"user_tz":-330,"elapsed":4149,"user":{"displayName":"deepak goyal","photoUrl":"https://lh5.googleusercontent.com/-3gfEZruD9Sk/AAAAAAAAAAI/AAAAAAAACvg/54H4lMsdOgA/s64/photo.jpg","userId":"05164064759516400423"}},"colab":{"base_uri":"https://localhost:8080/","height":69}},"source":["t1=time.time()\n","print(\"Separating meaningfull information for our model...\")\n","\n","lines={}\n","conversations=[]\n","qr_pairs=[]\n","\n","movie_lines_fields=[\"lineID\",\"characterID\",\"movieID\",\"character\",\"text\"]\n","movie_convo_fields=[\"charcaterID\",\"character2ID\",\"movieID\",\"utteranceIDs\"]\n","\n","lines=loadLines(movie_lines,movie_lines_fields)\n","conversations=loadConversations(movie_conversations,lines,movie_convo_fields)\n","qr_pairs=sentencePairs(conversations)\n","\n","print(\"The number of query-response pairs are: \"+str(len(qr_pairs)))\n","print(\"Separation took place in: \"+str(time.time()-t1))"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Separating meaningfull information for our model...\n","The number of query-response pairs are: 221282\n","Separation took place in: 2.4958572387695312\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"p5vYOofuzsaP","colab_type":"code","colab":{}},"source":["PAD_Token=0\n","START_Token=1\n","END_Token=2\n","\n","class Vocabulary:\n","    def __init__(self):\n","        self.trimmed=False\n","        self.word2count={}\n","        self.index2word={PAD_Token:\"PAD\",START_Token:\"SOS\",END_Token:\"EOS\"}\n","        self.word2index={\"PAD\":PAD_Token,\"SOS\":START_Token,\"EOS\":END_Token}\n","        self.num_words=3\n","        \n","    def addSentence(self,sentence):\n","        for word in sentence.split(\" \"):\n","            self.addWord(word)\n","    def addWord(self,word):\n","        if word not in self.word2index:\n","            self.word2index[word]=self.num_words\n","            self.index2word[self.num_words]=word\n","            self.word2count[word]=1\n","            self.num_words=self.num_words+1\n","        else:\n","            self.word2count[word]+=1\n","            \n","    def trim(self,min_count):\n","        \n","        if self.trimmed:\n","            return\n","        self.trimmed=True\n","        \n","        keep_words=[]\n","        \n","        for word,freq in self.word2count.items():\n","            if freq>=min_count:\n","                keep_words.append(word)\n","        \n","        self.word2count={}\n","        self.index2word={PAD_Token:\"PAD\",START_Token:\"SOS\",END_Token:\"EOS\"}\n","        self.word2index={\"PAD\":PAD_Token,\"SOS\":START_Token,\"EOS\":END_Token}\n","        self.num_words=3\n","        \n","        for word in keep_words:\n","            self.addWord(word)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MEPrFn1h4sFp","colab_type":"code","outputId":"d2499b4e-b436-479d-832b-c0ff0e6eac65","executionInfo":{"status":"ok","timestamp":1587494987742,"user_tz":-330,"elapsed":8264,"user":{"displayName":"deepak goyal","photoUrl":"https://lh5.googleusercontent.com/-3gfEZruD9Sk/AAAAAAAAAAI/AAAAAAAACvg/54H4lMsdOgA/s64/photo.jpg","userId":"05164064759516400423"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["Max_Length=10\n","\n","def normalizeString(s):\n","    s=s.lower().strip()\n","    s=re.sub(r\"([.!?])\", r\" \\1\", s)\n","    s=re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n","    s=re.sub(r\"\\s+\", r\" \", s).strip()\n","    return s\n","\n","def readVocs(qr_pairs):\n","    \n","    for qr_pair in qr_pairs:\n","        qr_pair[0]=normalizeString(qr_pair[0])\n","        qr_pair[1]=normalizeString(qr_pair[1])\n","    \n","    voc=Vocabulary()\n","    return voc,qr_pairs\n","\n","def filterPair(pair):\n","    return len(pair[0].split(\" \"))<Max_Length and len(pair[1].split(\" \"))<Max_Length\n","\n","def filterPairs(qr_pairs):\n","    return [pair for pair in qr_pairs if filterPair(pair)]\n","\n","def prepareDataset(qr_pairs):\n","    voc, qr_pairs=readVocs(qr_pairs)\n","    qr_pairs=filterPairs(qr_pairs)\n","       \n","    for pair in qr_pairs:\n","        voc.addSentence(pair[0])\n","        voc.addSentence(pair[1])\n","#     print(\"Number\"+str(voc.num_words))\n","    return voc,qr_pairs\n","\n","t1=time.time()\n","print(\"Preparing dataset and corresponding vocabulary...\")\n","voc, pairs=prepareDataset(qr_pairs)\n","print(\"Preparation took place in: \"+str(time.time()-t1))"],"execution_count":12,"outputs":[{"output_type":"stream","text":["Preparing dataset and corresponding vocabulary...\n","Preparation took place in: 7.02295184135437\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wv1ruAcB4wxK","colab_type":"code","outputId":"6410048a-2eae-42ff-8780-9a8132e4494d","executionInfo":{"status":"ok","timestamp":1587494992391,"user_tz":-330,"elapsed":1284,"user":{"displayName":"deepak goyal","photoUrl":"https://lh5.googleusercontent.com/-3gfEZruD9Sk/AAAAAAAAAAI/AAAAAAAACvg/54H4lMsdOgA/s64/photo.jpg","userId":"05164064759516400423"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["Min_Count=3\n","\n","def trimRareWords(voc,qr_pairs):\n","    \n","    voc.trim(Min_Count)\n","    keep_pairs=[]\n","    \n","    for pair in qr_pairs:\n","        input_sentence=pair[0]\n","        output_sentence=pair[1]\n","        \n","        keep_input=True\n","        keep_output=True\n","        \n","        for word in input_sentence.split(\" \"):\n","            if word not in voc.word2index:\n","                keep_input=False\n","                break\n","        \n","        for word in output_sentence.split(\" \"):\n","            if word not in voc.word2index:\n","                keep_output=False\n","                break\n","                \n","        if keep_input and keep_output:\n","            keep_pairs.append(pair)\n","            \n","    return keep_pairs\n","\n","t1=time.time()\n","print(\"Trimming rare words from vocabulary and dataset..\")\n","\n","pairs=trimRareWords(voc,pairs)\n","\n","print(\"Trimming took place in: \"+str(time.time()-t1))"],"execution_count":13,"outputs":[{"output_type":"stream","text":["Trimming rare words from vocabulary and dataset..\n","Trimming took place in: 0.14720582962036133\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YVoU5avw4zay","colab_type":"code","colab":{}},"source":["# def indexesFromSentence(voc,sentence):\n","#     tokenised_sentence=[]\n","#     tokenised_sentence.append(START_Token)\n","    \n","#     for word in sentence.split(\" \"):\n","#         tokenised_sentence.append(voc.word2index[word])\n","        \n","#     tokenised_sentence.append(END_Token)\n","    \n","#     assert len(tokenised_sentence)<=Max_Length+2\n","#     for _ in range(Max_Length+2-len(tokenised_sentence)):\n","#         tokenised_sentence.append(PAD_Token)\n","        \n","#     return tokenised_sentence\n","\n","# def binaryMatrix(l,value=PAD_Token):\n","#     m=[]\n","#     for i,seq in enumerate(l):\n","#         m.append([])\n","#         for token in seq:\n","#             if token==value:\n","#                 m[i].append(0)\n","#             else:\n","#                 m[i].append(1)\n","        \n","#     return m\n","\n","# def inputVar(voc,l):\n","    \n","#     indexes_batch=[indexesFromSentence(voc,sentence) for sentence in l]\n","#     input_lengths=torch.tensor([len(index) for index in indexes_batch])\n","#     padVar=torch.LongTensor(indexes_batch)\n","#     return input_lengths,padVar\n","\n","# def outputVar(voc,l):\n","#     indexes_batch=[indexesFromSentence(voc,sentence) for sentence in l]\n","#     max_target_len=torch.tensor([len(index) for index in indexes_batch])\n","#     mask=binaryMatrix(indexes_batch)\n","#     mask=torch.ByteTensor(mask)\n","#     padVar=torch.LongTensor(indexes_batch)\n","#     return max_target_len, mask, padVar\n","\n","# def batch2TrainData(voc,pair_batch):\n","#     #sort function see \n","#     input_batch=[]\n","#     output_batch=[]\n","\n","#     for pair in pair_batch:\n","#         input_batch.append(pair[0])\n","#         output_batch.append(pair[1])\n","                                  \n","    \n","#     input_lengths,tokenised_input=inputVar(voc,input_batch)\n","#     max_out_length,mask,tokenised_output=outputVar(voc,output_batch)\n","#     return input_lengths,tokenised_input,max_out_length,mask,tokenised_output\n","\n","def indexesFromSentence(voc, sentence):\n","    return [voc.word2index[word] for word in sentence.split(' ')] + [END_Token]\n","\n","\n","def zeroPadding(l, fillvalue=PAD_Token):\n","    return list(itertools.zip_longest(*l, fillvalue=fillvalue))\n","\n","def binaryMatrix(l, value=PAD_Token):\n","    m = []\n","    for i, seq in enumerate(l):\n","        m.append([])\n","        for token in seq:\n","            if token == PAD_Token:\n","                m[i].append(0)\n","            else:\n","                m[i].append(1)\n","    return m\n","\n","# Returns padded input sequence tensor and lengths\n","def inputVar(l, voc):\n","    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n","    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n","    padList = zeroPadding(indexes_batch)\n","    padVar = torch.LongTensor(padList)\n","    return padVar, lengths\n","\n","# Returns padded target sequence tensor, padding mask, and max target length\n","def outputVar(l, voc):\n","    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n","    max_target_len = max([len(indexes) for indexes in indexes_batch])\n","    padList = zeroPadding(indexes_batch)\n","    mask = binaryMatrix(padList)\n","    mask = torch.BoolTensor(mask)\n","    padVar = torch.LongTensor(padList)\n","    return padVar, mask, max_target_len\n","\n","# Returns all items for a given batch of pairs\n","def batch2TrainData(voc, pair_batch):\n","    pair_batch.sort(key=lambda x: len(x[0].split(\" \")), reverse=True)\n","    input_batch, output_batch = [], []\n","    for pair in pair_batch:\n","        input_batch.append(pair[0])\n","        output_batch.append(pair[1])\n","    inp, lengths = inputVar(input_batch, voc)\n","    output, mask, max_target_len = outputVar(output_batch, voc)\n","    return inp, lengths, output, mask, max_target_len\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"d8-18tBuMZMx","colab_type":"code","outputId":"db12c912-9ed0-4c10-cf77-1c9726240875","executionInfo":{"status":"ok","timestamp":1587495048222,"user_tz":-330,"elapsed":1517,"user":{"displayName":"deepak goyal","photoUrl":"https://lh5.googleusercontent.com/-3gfEZruD9Sk/AAAAAAAAAAI/AAAAAAAACvg/54H4lMsdOgA/s64/photo.jpg","userId":"05164064759516400423"}},"colab":{"base_uri":"https://localhost:8080/","height":607}},"source":["print(\"Number of query-response pairs after all the preprocessing: \"+str(len(pairs)))\n","print(\"Number of Unique Words in our vocabulary: \"+str(voc.num_words))\n","\n","#Sample batch\n","batch=[random.choice(pairs) for _ in range(5)]\n","tokenised_input,input_lengths,tokenised_output,mask,max_out_length=batch2TrainData(voc,batch)\n","\n","print(\"Input length: \"+str(input_lengths)+\" Size: \"+str(input_lengths.shape))\n","print(\"-\"*80)\n","print(\"Tokenised Input: \"+str(tokenised_input)+\" Size: \"+str(tokenised_input.shape))\n","print(\"-\"*80)\n","print(\"Max out length: \"+str(max_out_length)+\" Size: \")\n","print(\"-\"*80)\n","print(\"Mask: \"+str(mask)+\" Size: \"+str(mask.shape))\n","print(\"-\"*80)\n","print(\"Tokenised Output: \"+str(tokenised_output)+\" Size: \"+str(tokenised_output.shape))\n","print(\"-\"*80)"],"execution_count":15,"outputs":[{"output_type":"stream","text":["Number of query-response pairs after all the preprocessing: 53113\n","Number of Unique Words in our vocabulary: 7816\n","Input length: tensor([9, 6, 6, 5, 4]) Size: torch.Size([5])\n","--------------------------------------------------------------------------------\n","Tokenised Input: tensor([[  25,    7,  124,   36,  167],\n","        [ 192, 1588,   34,   37, 1607],\n","        [  24, 1799,   34,  266,    6],\n","        [  50,  317,   34,    4,    2],\n","        [  36,  223,    4,    2,    0],\n","        [1850,    2,    2,    0,    0],\n","        [  18,    0,    0,    0,    0],\n","        [   4,    0,    0,    0,    0],\n","        [   2,    0,    0,    0,    0]]) Size: torch.Size([9, 5])\n","--------------------------------------------------------------------------------\n","Max out length: 8 Size: \n","--------------------------------------------------------------------------------\n","Mask: tensor([[ True,  True,  True,  True,  True],\n","        [ True,  True,  True,  True,  True],\n","        [ True,  True,  True,  True,  True],\n","        [ True,  True,  True,  True,  True],\n","        [ True,  True,  True,  True,  True],\n","        [ True, False, False,  True,  True],\n","        [ True, False, False,  True,  True],\n","        [ True, False, False, False,  True]]) Size: torch.Size([8, 5])\n","--------------------------------------------------------------------------------\n","Tokenised Output: tensor([[ 122,   48,    7,   65,   94],\n","        [  38,   40,   14,  132,    7],\n","        [  53,   53, 2279, 1622, 1404],\n","        [1749,  199,    4,    7,   83],\n","        [  27,    2,    2,   21,   12],\n","        [   8,    0,    0,    4, 3367],\n","        [   4,    0,    0,    2,    6],\n","        [   2,    0,    0,    0,    2]]) Size: torch.Size([8, 5])\n","--------------------------------------------------------------------------------\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Oppo7HKtKMxW","colab_type":"text"},"source":["# Model"]},{"cell_type":"code","metadata":{"id":"ra7_TfQv42Ra","colab_type":"code","colab":{}},"source":["class EncoderRNN(nn.Module):\n","    \n","    def __init__(self,hidden_size,embedding,n_layers=1,dropout=0):\n","        \"\"\"\n","        Encoder module for seq2seq architechture.\n","        \"\"\"\n","    \n","        super().__init__()\n","        \n","        self.n_layers=n_layers\n","        self.hidden_size=hidden_size\n","        \n","        self.embedding=embedding\n","        self.gru=nn.GRU(hidden_size,hidden_size,n_layers,dropout=(0 if n_layers==1 else dropout),bidirectional=True)\n","        \n","    def forward(self,input_seq,input_lengths,hidden=None):\n","        \n","        embedded_input=self.embedding(input_seq)\n","        packed=nn.utils.rnn.pack_padded_sequence(embedded_input,input_lengths)\n","        outputs,hidden=self.gru(packed,hidden)\n","        \n","        outputs,_=nn.utils.rnn.pad_packed_sequence(outputs)\n","        \n","        outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:]\n","        \n","        return outputs,hidden\n","\n","    def hidden_init(self,batch_size):\n","        return torch.zeros(self.n_layers*2,batch_size,self.hidden_size,device=device)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2ZsVL85g49Er","colab_type":"code","colab":{}},"source":["# Luong attention layer\n","class Attn(torch.nn.Module):\n","    def __init__(self, method, hidden_size):\n","        super(Attn, self).__init__()\n","        self.method = method\n","        if self.method not in ['dot', 'general', 'concat']:\n","            raise ValueError(self.method, \"is not an appropriate attention method.\")\n","        self.hidden_size = hidden_size\n","        if self.method == 'general':\n","            self.attn = torch.nn.Linear(self.hidden_size, hidden_size)\n","        elif self.method == 'concat':\n","            self.attn = torch.nn.Linear(self.hidden_size * 2, hidden_size)\n","            self.v = torch.nn.Parameter(torch.FloatTensor(hidden_size))\n","\n","    def dot_score(self, hidden, encoder_output):\n","        return torch.sum(hidden * encoder_output, dim=2)\n","\n","    def general_score(self, hidden, encoder_output):\n","        energy = self.attn(encoder_output)\n","        return torch.sum(hidden * energy, dim=2)\n","\n","    def concat_score(self, hidden, encoder_output):\n","        energy = self.attn(torch.cat((hidden.expand(encoder_output.size(0), -1, -1), encoder_output), 2)).tanh()\n","        return torch.sum(self.v * energy, dim=2)\n","\n","    def forward(self, hidden, encoder_outputs):\n","        # Calculate the attention weights (energies) based on the given method\n","        if self.method == 'general':\n","            attn_energies = self.general_score(hidden, encoder_outputs)\n","        elif self.method == 'concat':\n","            attn_energies = self.concat_score(hidden, encoder_outputs)\n","        elif self.method == 'dot':\n","            attn_energies = self.dot_score(hidden, encoder_outputs)\n","\n","        # Transpose max_length and batch_size dimensions\n","        attn_energies = attn_energies.t()\n","\n","        # Return the softmax normalized probability scores (with added dimension)\n","        return F.softmax(attn_energies, dim=1).unsqueeze(1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"S6Vv95PR5TBu","colab_type":"code","colab":{}},"source":["class LuongAttnDecoderRNN(nn.Module):\n","    def __init__(self, attn_model, embedding, hidden_size, output_size, n_layers=1, dropout=0.1):\n","        super(LuongAttnDecoderRNN, self).__init__()\n","\n","        # Keep for reference\n","        self.attn_model = attn_model\n","        self.hidden_size = hidden_size\n","        self.output_size = output_size\n","        self.n_layers = n_layers\n","        self.dropout = dropout\n","\n","        # Define layers\n","        self.embedding = embedding\n","        self.embedding_dropout = nn.Dropout(dropout)\n","        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers == 1 else dropout))\n","        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n","        self.out = nn.Linear(hidden_size, output_size)\n","\n","        self.attn = Attn(attn_model, hidden_size)\n","\n","    def forward(self, input_step, last_hidden, encoder_outputs):\n","        # Note: we run this one step (word) at a time\n","        # Get embedding of current input word\n","        embedded = self.embedding(input_step)\n","        embedded = self.embedding_dropout(embedded)\n","        # Forward through unidirectional GRU\n","        \n","        rnn_output, hidden = self.gru(embedded, last_hidden)\n","        \n","        # Calculate attention weights from the current GRU output\n","        attn_weights = self.attn(rnn_output, encoder_outputs)\n","        # Multiply attention weights to encoder outputs to get new \"weighted sum\" context vector\n","        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))\n","        # Concatenate weighted context vector and GRU output using Luong eq. 5\n","        rnn_output = rnn_output.squeeze(0)\n","        context = context.squeeze(1)\n","        concat_input = torch.cat((rnn_output, context), 1)\n","        concat_output = torch.tanh(self.concat(concat_input))\n","        # Predict next word using Luong eq. 6\n","        output = self.out(concat_output)\n","        # output = F.softmax(output, dim=1)\n","        # Return output and final hidden state\n","        return output, hidden"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PybGzLUrPRGD","colab_type":"code","colab":{}},"source":["def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Bqc_v6v09aN3","colab_type":"code","colab":{}},"source":["def make_model(vocabulary_size,d_model=500,num_encoders=1,num_decoders=1,dropout_encoder=0.1,dropout_decoder=0.1,attn_model='general'):\n","\n","    embedding=nn.Embedding(vocabulary_size,d_model)\n","    embedding.weight.requires_grad=False\n","\n","    encoder=EncoderRNN(d_model,embedding,num_encoders,dropout_encoder)\n","    decoder=LuongAttnDecoderRNN(attn_model,embedding,d_model,vocabulary_size,num_decoders,dropout_decoder)\n","\n","    for p in encoder.parameters():\n","        if p.dim()>1:\n","            nn.init.xavier_uniform_(p)\n","    \n","    for p in decoder.parameters():\n","        if p.dim()>1:\n","            nn.init.xavier_uniform_(p)\n","    \n","    num_parameters=0\n","\n","    num_parameters+=count_parameters(encoder)\n","    num_parameters+=count_parameters(decoder)\n","\n","    return encoder,decoder,num_parameters\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nxlGp-uOKbLD","colab_type":"text"},"source":["# Training"]},{"cell_type":"code","metadata":{"id":"RIMnSo1Q5WKP","colab_type":"code","colab":{}},"source":["def maskNLLLoss(inp, target, mask):\n","    nTotal = mask.sum()\n","    crossEntropy = -torch.log(torch.gather(inp, 1, target.view(-1, 1)).squeeze(1))\n","    loss = crossEntropy.masked_select(mask).mean()\n","    loss = loss.to(device)\n","    return loss, nTotal.item()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"G5RqXDqbpp5g","colab_type":"code","colab":{}},"source":["def f1_score(predictions, targets, average=True):\n","    def f1_score_items(pred_items, gold_items):\n","        common = Counter(gold_items) & Counter(pred_items)\n","        num_same = sum(common.values())\n","\n","        if num_same == 0:\n","            return 0\n","\n","        precision = num_same / len(pred_items)\n","        recall = num_same / len(gold_items)\n","        f1 = (2 * precision * recall) / (precision + recall)\n","\n","        return f1\n","    \n","    scores = [f1_score_items(p, t) for p, t in zip(predictions, targets)]\n","\n","    if average:\n","        return sum(scores) / len(scores)    \n","\n","    return scores"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1xk1Eu8y5cYn","colab_type":"code","colab":{}},"source":["\n","MAX_LENGTH=10\n","def train(input_variable,lengths,target_variable,mask,max_target_len,encoder,decoder,\n","          encoder_optimizer,decoder_optimizer,batch_size,clip,max_length=MAX_LENGTH):\n","    \n","    encoder_optimizer.zero_grad()\n","    decoder_optimizer.zero_grad()\n","\n","    loss_fn=nn.CrossEntropyLoss()\n","    \n","    input_variable=torch.tensor(input_variable).to(device)\n","    lengths=lengths.to(device)\n","\n","    target_variable=torch.tensor(target_variable).to(device)\n","    mask=mask.to(device)\n","    \n","    loss=0\n","    print_losses=[]\n","    n_totals=0\n","    encoder_hidden=encoder.hidden_init(input_variable.size()[1])\n","    encoder_outputs, encoder_hidden=encoder(input_variable,lengths,encoder_hidden)\n","    \n","    decoder_input=torch.LongTensor([[START_Token for _ in range(batch_size)]])\n","    decoder_input=decoder_input.to(device)\n","    use_teacher_forcing=True #if random.random()<teacher_forcing_ratio else False\n","    \n","    decoder_hidden=encoder_hidden[:decoder.n_layers]\n","\n","    predicted=torch.ones(max_target_len,input_variable.size()[1])\n","    \n","    if use_teacher_forcing:\n","        \n","        for t in range(max_target_len):\n","            decoder_output,decoder_hidden=decoder(decoder_input,decoder_hidden,encoder_outputs)\n","            \n","            decoder_input=target_variable[t].view(1,-1)\n","            \n","            \n","            # mask_loss=F.cross_entropy(decoder_output,target_variable[t])\n","            # mask_loss,nTotal=maskNLLLoss(decoder_output,target_variable[t],mask)\n","            mask_loss=loss_fn(decoder_output,target_variable[t])\n","            loss+=mask_loss\n","            nTotal=(mask.sum()).item()\n","            print_losses.append(mask_loss.item()*nTotal)\n","            n_totals+=nTotal\n","            predicted[t]=torch.argmax(decoder_output,dim=-1)\n","            \n","\n","            \n","    # else:\n","        \n","    #     for t in range(max_target_len):\n","    #         decoder_output,decoder_hidden=decoder(decoder_input,decoder_hidden,encoder_outputs)\n","            \n","    #         _,topi=decoder_output.topk(1)\n","    #         decoder_input=torch.LongTensor([[topi[i][0] for i in range(batch_size)]])\n","    #         decoder_input=decoder_input.to(device)\n","            \n","    #         mask_loss,nTotal=maskNLLLoss(decoder_output,target_variable[t],mask[t])\n","    #         loss+=mask_loss\n","    #         print_losses.append(mask_loss.item()*nTotal)\n","    #         n_totals+=nTotal\n","            \n","    F1=f1_score(predicted,target_variable.transpose(0,1))\n","    loss.backward()\n","    \n","    _=nn.utils.clip_grad_norm_(encoder.parameters(),clip)\n","    _=nn.utils.clip_grad_norm_(decoder.parameters(),clip)\n","    \n","    \n","    # plot_grad_flow(encoder.named_parameters())\n","    # plot_grad_flow(decoder.named_parameters())\n","    \n","    encoder_optimizer.step()\n","    decoder_optimizer.step()\n","    \n","    return sum(print_losses)/n_totals,F1\n","    \n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6-GQ9XmH5fLl","colab_type":"code","colab":{}},"source":["def trainIters(model_name,voc,pairs,encoder,decoder,encoder_optimizer,decoder_optimizer,\n","               encoder_n_layers,decoder_n_layers,save_dir,n_batches,batch_size,\n","               save_every,clip,corpus_name,loadFileName,n_epochs,training_batches):\n","    \n","    \n","    \n","    start_epoch=0\n","    loss=0\n","    perplexity=0\n","    time_taken=0\n","    f1_score=0\n","    \n","    if loadFileName:\n","        start_epoch=checkpoint['epoch']+1\n","        time_taken=checkpoint['time']\n","        \n","        \n","    for epoch in range(start_epoch,n_epochs):\n","\n","        t1=time.time()\n","        loss=0\n","        perplexity=0\n","        f1_score=0\n","\n","        for i in range(n_batches):\n","            \n","            training_batch=training_batches[i]\n","            \n","            input_variable,lengths,target_variable,mask,max_target_len=training_batch\n","\n","            curr_loss,F1=train(input_variable,lengths,target_variable,mask,max_target_len,encoder,decoder,\n","                    encoder_optimizer,decoder_optimizer,batch_size,clip)\n","            \n","            loss+=curr_loss\n","            perplexity+=math.exp(curr_loss)\n","            f1_score+=F1\n","            \n","        \n","        \n","        loss=loss/n_batches\n","        perplexity=perplexity/n_batches\n","        f1_score=f1_score/n_batches\n","\n","        \n","        \n","        \n","        \n","        \n","        if epoch%save_every==0:\n","\n","            directory=os.path.join(save_dir,model_name,corpus_name)\n","            if not os.path.exists(directory):\n","                os.makedirs(directory)\n","            \n","            torch.save({\n","                \"epoch\":epoch,\n","                \"encoder\":encoder.state_dict(),\n","                \"decoder\":decoder.state_dict(),\n","                \"loss\":loss,\n","                \"encoder_opt\":encoder_optimizer.state_dict(),\n","                \"decoder_opt\":decoder_optimizer.state_dict(),\n","                \"ppl\":perplexity,\n","                \"time\":time_taken,\n","                \"F1\":f1_score\n","\n","            },os.path.join(directory,'{}_{}.tar'.format(epoch,\"checkpoint\")))\n","        \n","        print(\"=\"*100)\n","        print(\"| End of epoch : \"+str(epoch)+\"| Loss Value: \"+str(loss)+\"| PPL: \"+str(perplexity)+\"| F1: \"+str(f1_score)+\"| Time Took: \"+\n","            str(time.time()-t1)+\" |\")\n","        print(\"=\"*100)\n","        time_taken+=(time.time()-t1)\n","\n","    print(\"| Training Finished | Took:\"+str(time_taken))          \n","        \n","\n","        \n","\n","        "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z3yEoRzsIsPN","colab_type":"code","colab":{}},"source":["def data_generation(pairs,batch_size,n_batches,start=0):\n","    \n","    # sample_batches=[batch2TrainData(voc,[random.choice(pairs) for _ in range(batch_size)]) for _ in range(n_batches)]\n","    sample_batches=[]\n","    \n","    for i in range(n_batches):\n","        curr_batch=[]\n","        for j in range(batch_size):\n","            curr_id=i*batch_size+j+start\n","            curr_batch.append(pairs[curr_id])\n","\n","        sample_batches.append(batch2TrainData(voc,curr_batch))\n","\n","    return sample_batches"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9D6B5hoG5mfA","colab_type":"code","colab":{}},"source":["model_name='seq2seq_attn'\n","corpus_name='cornell-movie'\n","\n","attn_model='dot'\n","num_encoder=2\n","num_decoder=2\n","d_model=500\n","dropout_encoder=0.1\n","dropout_decoder=0.1\n","\n","batch_size=10\n","n_batches=4000\n","\n","clip=50.0\n","teacher_forcing_ratio=1.0\n","learning_rate=0.0001\n","decoder_learning_ratio=5.0\n","\n","\n","save_every=10\n","n_epochs=101\n","\n","loadFile=None\n","\n","save_dir=\"/content/drive/My Drive/Model Data\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LkB9JkW15omn","colab_type":"code","outputId":"e66f48b3-dc8a-4997-ddc4-894b0492a99f","executionInfo":{"status":"ok","timestamp":1587511862430,"user_tz":-330,"elapsed":15652539,"user":{"displayName":"deepak goyal","photoUrl":"https://lh5.googleusercontent.com/-3gfEZruD9Sk/AAAAAAAAAAI/AAAAAAAACvg/54H4lMsdOgA/s64/photo.jpg","userId":"05164064759516400423"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["training_batches=data_generation(pairs,10,4000,0)\n","encoder,decoder,num_parameters=make_model(voc.num_words,d_model,num_encoder,num_decoder,dropout_encoder,dropout_decoder,attn_model)\n","\n","encoder_optimizer=torch.optim.Adam(encoder.parameters(),lr=learning_rate)\n","decoder_optimizer=torch.optim.Adam(decoder.parameters(),lr=learning_rate*decoder_learning_ratio)\n","\n","if loadFile:\n","    checkpoint=torch.load(loadFile)\n","\n","    encoder.load_state_dict(checkpoint['encoder'])\n","    decoder.load_state_dict(checkpoint['decoder'])\n","    encoder_optimizer.load_state_dict(checkpoint['encoder_opt'])\n","    decoder_optimizer.load_state_dict(checkpoint['decoder_opt'])\n","\n","\n","\n","encoder.to(device)\n","decoder.to(device)\n","\n","encoder.train()\n","decoder.train()\n","\n","trainIters(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer,\n","        num_encoder, num_decoder, save_dir, n_batches, batch_size, save_every, clip, corpus_name, loadFile,n_epochs,training_batches)\n","\n"],"execution_count":31,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  # This is added back by InteractiveShellApp.init_path()\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  \n"],"name":"stderr"},{"output_type":"stream","text":["====================================================================================================\n","| End of epoch : 0| Loss Value: 2.7594754202368845| PPL: 28.51271485434107| F1: 0.0| Time Took: 164.4622220993042 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 1| Loss Value: 2.3989257950537772| PPL: 12.113668101777783| F1: 0.0| Time Took: 152.22215294837952 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 2| Loss Value: 2.2691479103248615| PPL: 10.53936390762008| F1: 0.0| Time Took: 153.88207173347473 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 3| Loss Value: 2.1647400298048196| PPL: 9.413985329356978| F1: 0.0| Time Took: 155.8213279247284 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 4| Loss Value: 2.061794113466897| PPL: 8.421224298631529| F1: 0.0| Time Took: 156.86538648605347 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 5| Loss Value: 1.9565968067145834| PPL: 7.5115461308244775| F1: 0.0| Time Took: 155.36472034454346 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 6| Loss Value: 1.849086481740242| PPL: 6.690208054643429| F1: 0.0| Time Took: 153.74687314033508 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 7| Loss Value: 1.7450272885036295| PPL: 5.984414066498976| F1: 0.0| Time Took: 153.1725013256073 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 8| Loss Value: 1.6451689075201013| PPL: 5.381063588283356| F1: 0.0| Time Took: 153.3822202682495 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 9| Loss Value: 1.550817456206639| PPL: 4.872360016666641| F1: 0.0| Time Took: 153.1281726360321 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 10| Loss Value: 1.46308548064302| PPL: 4.444520992800815| F1: 0.0| Time Took: 168.54915642738342 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 11| Loss Value: 1.3810326734821696| PPL: 4.078345101095973| F1: 0.0| Time Took: 154.34245705604553 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 12| Loss Value: 1.3053146035162784| PPL: 3.770062035645973| F1: 0.0| Time Took: 153.7658772468567 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 13| Loss Value: 1.2345797765975952| PPL: 3.503720547598809| F1: 0.0| Time Took: 153.64867877960205 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 14| Loss Value: 1.1661432140854948| PPL: 3.2651551223771573| F1: 0.0| Time Took: 153.59850478172302 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 15| Loss Value: 1.1033738455950661| PPL: 3.06078210555375| F1: 0.0| Time Took: 153.17491269111633 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 16| Loss Value: 1.0466679293118655| PPL: 2.8879715583567487| F1: 0.0| Time Took: 152.88089990615845 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 17| Loss Value: 0.9901259695535111| PPL: 2.725293275109464| F1: 0.0| Time Took: 153.24584817886353 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 18| Loss Value: 0.9398933996062439| PPL: 2.588852719269909| F1: 0.0| Time Took: 153.19317817687988 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 19| Loss Value: 0.891045089381174| PPL: 2.4630902304837896| F1: 0.0| Time Took: 153.80823826789856 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 20| Loss Value: 0.8456040845349608| PPL: 2.3524059009387517| F1: 0.0| Time Took: 164.79435348510742 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 21| Loss Value: 0.8025134983722523| PPL: 2.251487379487674| F1: 0.0| Time Took: 156.1061351299286 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 22| Loss Value: 0.7639353928863555| PPL: 2.1649975500973277| F1: 0.0| Time Took: 155.29590487480164 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 23| Loss Value: 0.725851335488695| PPL: 2.083603465760731| F1: 0.0| Time Took: 156.15185642242432 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 24| Loss Value: 0.6874342601088406| PPL: 2.0037314247329086| F1: 0.0| Time Took: 155.5116527080536 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 25| Loss Value: 0.6535311589995798| PPL: 1.9362230244800946| F1: 0.0| Time Took: 154.9913511276245 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 26| Loss Value: 0.6222939330167075| PPL: 1.8764491892187252| F1: 0.0| Time Took: 156.58742594718933 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 27| Loss Value: 0.5909914511336665| PPL: 1.8173672537442345| F1: 0.0| Time Took: 156.1402451992035 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 28| Loss Value: 0.5615009266432069| PPL: 1.7641402160958057| F1: 0.0| Time Took: 155.78851079940796 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 29| Loss Value: 0.5369466763830968| PPL: 1.7212294880318988| F1: 0.0| Time Took: 154.7978174686432 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 30| Loss Value: 0.510813743368543| PPL: 1.6761597520195386| F1: 0.0| Time Took: 169.12082195281982 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 31| Loss Value: 0.48607827305088896| PPL: 1.6354034306650875| F1: 0.0| Time Took: 155.53812074661255 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 32| Loss Value: 0.46563038823543956| PPL: 1.6016273474437661| F1: 0.0| Time Took: 155.04515957832336 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 33| Loss Value: 0.44595915136092656| PPL: 1.5703686119686333| F1: 0.0| Time Took: 154.37807846069336 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 34| Loss Value: 0.4262965474222777| PPL: 1.5395653931390194| F1: 0.0| Time Took: 152.8244595527649 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 35| Loss Value: 0.4115108245104415| PPL: 1.516545999491542| F1: 0.0| Time Took: 150.82633447647095 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 36| Loss Value: 0.3943967574200037| PPL: 1.4906639490972666| F1: 0.0| Time Took: 151.13299322128296 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 37| Loss Value: 0.3814014437870605| PPL: 1.4712556007743691| F1: 0.0| Time Took: 150.8377456665039 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 38| Loss Value: 0.37089846218639| PPL: 1.4556933191297177| F1: 0.0| Time Took: 151.2034456729889 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 39| Loss Value: 0.356667084799632| PPL: 1.4350147511801328| F1: 0.0| Time Took: 151.44829678535461 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 40| Loss Value: 0.34476853827174275| PPL: 1.4181398987021925| F1: 0.0| Time Took: 163.52086758613586 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 41| Loss Value: 0.3382570785745504| PPL: 1.4087173181464854| F1: 0.0| Time Took: 152.69197916984558 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 42| Loss Value: 0.32798084742252687| PPL: 1.394173917084294| F1: 0.0| Time Took: 152.23759007453918 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 43| Loss Value: 0.3192773998737798| PPL: 1.3820771726656502| F1: 0.0| Time Took: 152.21166563034058 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 44| Loss Value: 0.31286197273672023| PPL: 1.3731301751999276| F1: 0.0| Time Took: 152.38544607162476 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 45| Loss Value: 0.3043130734377984| PPL: 1.3612888141616764| F1: 0.0| Time Took: 152.44312286376953 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 46| Loss Value: 0.29751387699339665| PPL: 1.3521992018131368| F1: 0.0| Time Took: 151.98123145103455 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 47| Loss Value: 0.2915866218728553| PPL: 1.3440081353566269| F1: 0.0| Time Took: 151.4292814731598 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 48| Loss Value: 0.285376538179966| PPL: 1.3355291951843005| F1: 0.0| Time Took: 152.10529136657715 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 49| Loss Value: 0.2815618192699312| PPL: 1.3306646068855015| F1: 0.0| Time Took: 152.63731694221497 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 50| Loss Value: 0.27709068041478185| PPL: 1.3245539136135152| F1: 0.0| Time Took: 163.68664264678955 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 51| Loss Value: 0.27386025029417665| PPL: 1.3202137703575703| F1: 0.0| Time Took: 154.2654013633728 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 52| Loss Value: 0.2682967001166762| PPL: 1.3129853886012357| F1: 0.0| Time Took: 153.10912775993347 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 53| Loss Value: 0.26625968165044606| PPL: 1.3102848965574467| F1: 0.0| Time Took: 153.0421221256256 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 54| Loss Value: 0.2604356382505833| PPL: 1.3026102364830754| F1: 0.0| Time Took: 152.82110691070557 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 55| Loss Value: 0.2568480264039376| PPL: 1.2979606285819543| F1: 0.0| Time Took: 151.84859585762024 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 56| Loss Value: 0.2541959413006195| PPL: 1.2943952908699594| F1: 0.0| Time Took: 152.2009415626526 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 57| Loss Value: 0.25338421759749163| PPL: 1.2935434715664635| F1: 0.0| Time Took: 152.13338422775269 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 58| Loss Value: 0.2490346676995057| PPL: 1.287686828011918| F1: 0.0| Time Took: 152.2109751701355 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 59| Loss Value: 0.2472306252758167| PPL: 1.2854240548561906| F1: 0.0| Time Took: 152.10912251472473 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 60| Loss Value: 0.2426773855797415| PPL: 1.279601267704703| F1: 0.0| Time Took: 163.1963152885437 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 61| Loss Value: 0.2422904304538969| PPL: 1.2790871874588756| F1: 0.0| Time Took: 152.1855366230011 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 62| Loss Value: 0.24030978769363734| PPL: 1.2766381787276084| F1: 0.0| Time Took: 151.15449929237366 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 63| Loss Value: 0.2378337104849753| PPL: 1.2733701533898731| F1: 0.0| Time Took: 151.32198238372803 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 64| Loss Value: 0.2351156857675287| PPL: 1.2700595161634545| F1: 0.0| Time Took: 152.0992293357849 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 65| Loss Value: 0.23261837432045276| PPL: 1.2666841647427944| F1: 0.0| Time Took: 151.7852132320404 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 66| Loss Value: 0.23265156713641472| PPL: 1.2669535130607303| F1: 0.0| Time Took: 152.9553518295288 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 67| Loss Value: 0.229570433843153| PPL: 1.2629150782187535| F1: 0.0| Time Took: 151.98382759094238 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 68| Loss Value: 0.22963430315354194| PPL: 1.2631383322178593| F1: 0.0| Time Took: 152.17652893066406 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 69| Loss Value: 0.22681756336482917| PPL: 1.2595135488516056| F1: 0.0| Time Took: 152.998850107193 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 70| Loss Value: 0.2264325265982695| PPL: 1.2588691530343572| F1: 0.0| Time Took: 172.60771918296814 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 71| Loss Value: 0.2240211638275557| PPL: 1.2557428765897318| F1: 0.0| Time Took: 153.4145097732544 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 72| Loss Value: 0.22474602640417735| PPL: 1.2569148333507136| F1: 0.0| Time Took: 153.38735222816467 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 73| Loss Value: 0.2226652781564104| PPL: 1.254166630436138| F1: 0.0| Time Took: 155.93523049354553 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 74| Loss Value: 0.21919953487274527| PPL: 1.2498730945596033| F1: 0.0| Time Took: 155.5874855518341 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 75| Loss Value: 0.21709708192626212| PPL: 1.2471618622635703| F1: 0.0| Time Took: 153.28219532966614 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 76| Loss Value: 0.21829572268446412| PPL: 1.2486558539440227| F1: 0.0| Time Took: 153.86025166511536 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 77| Loss Value: 0.2128581356696789| PPL: 1.2415301543678117| F1: 0.0| Time Took: 153.82524847984314 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 78| Loss Value: 0.2164025825992044| PPL: 1.2463824031450612| F1: 0.0| Time Took: 154.060485124588 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 79| Loss Value: 0.21133302693583947| PPL: 1.2398910544715636| F1: 0.0| Time Took: 152.6502661705017 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 80| Loss Value: 0.20991784975838287| PPL: 1.2381882739413226| F1: 0.0| Time Took: 164.32681345939636 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 81| Loss Value: 0.21287173427766143| PPL: 1.242071740823391| F1: 0.0| Time Took: 154.5463728904724 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 82| Loss Value: 0.21014479920055834| PPL: 1.2385593494458862| F1: 0.0| Time Took: 154.3695170879364 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 83| Loss Value: 0.20665866000420874| PPL: 1.2340754734292456| F1: 0.0| Time Took: 154.1561722755432 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 84| Loss Value: 0.20736323181423894| PPL: 1.2350375306172294| F1: 0.0| Time Took: 154.30088686943054 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 85| Loss Value: 0.20497626268529384| PPL: 1.2318503342728624| F1: 0.0| Time Took: 153.55069208145142 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 86| Loss Value: 0.20399337007440504| PPL: 1.2307467019509215| F1: 0.0| Time Took: 153.7001314163208 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 87| Loss Value: 0.2035977986689909| PPL: 1.2301070719517033| F1: 0.0| Time Took: 152.7288727760315 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 88| Loss Value: 0.2033824677696494| PPL: 1.2299862420480114| F1: 0.0| Time Took: 154.30783677101135 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 89| Loss Value: 0.20077173345917482| PPL: 1.2266500409888705| F1: 0.0| Time Took: 154.387042760849 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 90| Loss Value: 0.201732722417156| PPL: 1.2281021075601652| F1: 0.0| Time Took: 163.43452501296997 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 91| Loss Value: 0.20081728082263994| PPL: 1.2267704206793881| F1: 0.0| Time Took: 155.13768100738525 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 92| Loss Value: 0.19826504308966977| PPL: 1.2235518793164633| F1: 0.0| Time Took: 154.6272485256195 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 93| Loss Value: 0.20225864268752228| PPL: 1.2286198070297645| F1: 0.0| Time Took: 154.9129855632782 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 94| Loss Value: 0.20037651310684745| PPL: 1.2264239165223254| F1: 0.0| Time Took: 155.06092357635498 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 95| Loss Value: 0.19852147268696005| PPL: 1.223992287986414| F1: 0.0| Time Took: 155.5293984413147 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 96| Loss Value: 0.19523618225047543| PPL: 1.2198493798526482| F1: 0.0| Time Took: 157.19619417190552 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 97| Loss Value: 0.19837155266647136| PPL: 1.2238713743016911| F1: 0.0| Time Took: 156.2923367023468 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 98| Loss Value: 0.19680952186243614| PPL: 1.2219412459002343| F1: 0.0| Time Took: 155.51024079322815 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 99| Loss Value: 0.19530324187431225| PPL: 1.220115209232198| F1: 0.0| Time Took: 154.62067341804504 |\n","====================================================================================================\n","====================================================================================================\n","| End of epoch : 100| Loss Value: 0.1960026377882831| PPL: 1.2209716876943173| F1: 0.0| Time Took: 155.66900849342346 |\n","====================================================================================================\n","| Training Finished | Took:15640.702718019485\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OA5TjKYt5hhA","colab_type":"code","colab":{}},"source":["class GreedySearchDecoder(nn.Module):\n","    \n","    def __init__(self,encoder,decoder):\n","        super().__init__()\n","        \n","        self.encoder=encoder\n","        self.decoder=decoder\n","        \n","    def forward(self,input_seq,input_length,max_length):\n","        \n","        encoder_outputs,encoder_hidden=self.encoder(input_seq,input_length)\n","        \n","        decoder_hidden=encoder_hidden[:decoder.n_layers]\n","#         print(\"Decoder hidden state: \"+str(decoder_hidden))\n","        decoder_input=torch.ones(1,1,device=device,dtype=torch.long)*START_Token\n","        \n","        \n","#         print(\"Decoder's Input: \"+str(decoder_input))\n","        all_tokens=torch.zeros([0],device=device,dtype=torch.long)\n","        all_scores=torch.zeros([0],device=device)\n","        \n","        for _ in range(max_length):\n","            \n","            decoder_output,decoder_hidden=self.decoder(decoder_input,decoder_hidden,encoder_outputs)\n","#             print(\"Decoder Output: \"+str(decoder_output))\n","            decoder_scores,decoder_input=torch.max(decoder_output,dim=1)\n","            all_scores=torch.cat((all_scores,decoder_scores),dim=0)\n","            all_tokens=torch.cat((all_tokens,decoder_input),dim=0)\n","            \n","            decoder_input=torch.unsqueeze(decoder_input,0)\n","            \n","        return all_tokens, all_scores\n","        \n","        "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EhpBX3QT5kCf","colab_type":"code","colab":{}},"source":["def evaluate(encoder, decoder, searcher,voc,sentence,max_length=MAX_LENGTH):\n","    \n","    index_batch=[indexesFromSentence(voc,sentence)]\n","#     print(\"Indexed sentence: \"+str(index_batch))\n","    lengths=torch.tensor([len(index) for index in index_batch])\n","#     print(\"The Lengths tensor: \"+str(lengths))\n","    input_batch=torch.LongTensor(index_batch).transpose(0,1)\n","    \n","    input_batch=input_batch.to(device)\n","    lengths=lengths.to(device)\n","    \n","    tokens, scores=searcher(input_batch,lengths,max_length)\n","#     print(\"The tokens: \"+str(tokens))\n","    decoded_words=[voc.index2word[token.item()] for token in tokens]\n","    return decoded_words\n","\n","def evaluateInput(encoder,decoder,searcher,voc):\n","    input_sentence=''\n","    while True:\n","        try:\n","            input_sentence=input('Human> ')\n","            \n","            if input_sentence=='q' or input_sentence=='quit':\n","                break\n","            input_sentence=normalizeString(input_sentence)\n","#             print(\"The Normalized Input Sentence: \"+str(input_sentence))\n","            output_words=evaluate(encoder,decoder,searcher,voc,input_sentence)\n","            \n","            output_words[:]=[x for x in output_words if not(x==\"PAD\" or x==\"EOS\")]\n","            print(\"Bot:\",\" \".join(output_words))\n","            \n","        except KeyError:\n","            print(\"Unknown Word\")\n","            \n","            \n","    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Bk193gyI54Iv","colab_type":"code","outputId":"3a9febd4-9e22-4593-f9fd-cc53a2328035","executionInfo":{"status":"ok","timestamp":1587512536132,"user_tz":-330,"elapsed":1361,"user":{"displayName":"deepak goyal","photoUrl":"https://lh5.googleusercontent.com/-3gfEZruD9Sk/AAAAAAAAAAI/AAAAAAAACvg/54H4lMsdOgA/s64/photo.jpg","userId":"05164064759516400423"}},"colab":{"base_uri":"https://localhost:8080/","height":156}},"source":["encoder.eval()\n","decoder.eval()"],"execution_count":34,"outputs":[{"output_type":"execute_result","data":{"text/plain":["LuongAttnDecoderRNN(\n","  (embedding): Embedding(7816, 500)\n","  (embedding_dropout): Dropout(p=0.1, inplace=False)\n","  (gru): GRU(500, 500, num_layers=2, dropout=0.1)\n","  (concat): Linear(in_features=1000, out_features=500, bias=True)\n","  (out): Linear(in_features=500, out_features=7816, bias=True)\n","  (attn): Attn()\n",")"]},"metadata":{"tags":[]},"execution_count":34}]},{"cell_type":"code","metadata":{"id":"VoxgXE87-169","colab_type":"code","outputId":"745c01f4-082a-4075-89e3-e1994dff8591","executionInfo":{"status":"ok","timestamp":1587512620803,"user_tz":-330,"elapsed":82715,"user":{"displayName":"deepak goyal","photoUrl":"https://lh5.googleusercontent.com/-3gfEZruD9Sk/AAAAAAAAAAI/AAAAAAAACvg/54H4lMsdOgA/s64/photo.jpg","userId":"05164064759516400423"}},"colab":{"base_uri":"https://localhost:8080/","height":277}},"source":["searcher=GreedySearchDecoder(encoder,decoder)\n","evaluateInput(encoder,decoder,searcher,voc)"],"execution_count":35,"outputs":[{"output_type":"stream","text":["Human> hi\n","Bot: hi .\n","Human> how are you\n","Bot: what are you doing ?\n","Human> I doing just fine\n","Bot: i m interested you see .\n","Human> why do you think so\n","Bot: i don t have to\n","Human> why\n","Bot: because you re coming for the goddamn !\n","Human> why\n","Bot: because you re coming for the goddamn !\n","Human> but\n","Bot: you always read this selfish ?\n","Human> q\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"pAvPOTMXZ-Ic","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"3e72c3fa-1358-4725-c8d1-458fb21a9e08","executionInfo":{"status":"ok","timestamp":1587512644085,"user_tz":-330,"elapsed":1369,"user":{"displayName":"deepak goyal","photoUrl":"https://lh5.googleusercontent.com/-3gfEZruD9Sk/AAAAAAAAAAI/AAAAAAAACvg/54H4lMsdOgA/s64/photo.jpg","userId":"05164064759516400423"}}},"source":["print(num_parameters)"],"execution_count":36,"outputs":[{"output_type":"stream","text":["14934316\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"eSOFd0pjIIuv","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}