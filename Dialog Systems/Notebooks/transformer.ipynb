{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre-Processing\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "#Model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# For visualising metrics\n",
    "from visdom import Visdom\n",
    "\n",
    "# For visualising gradients plot\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "import copy\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The device found: cpu\n"
     ]
    }
   ],
   "source": [
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"The device found: \"+str(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisdomLinePlotter(object):\n",
    "    \"\"\"Plots to Visdom\"\"\"\n",
    "    \n",
    "    def __init__(self, env_name='main'):\n",
    "        self.viz = Visdom()\n",
    "        self.env = env_name\n",
    "        self.plots = {}\n",
    "    def plot(self, var_name, split_name, title_name, x, y):\n",
    "        if var_name not in self.plots:\n",
    "            self.plots[var_name] = self.viz.line(X=np.array([x,x]), Y=np.array([y,y]), env=self.env, opts=dict(\n",
    "                legend=[split_name],\n",
    "                title=title_name,\n",
    "                xlabel='Epochs',\n",
    "                ylabel=var_name\n",
    "            ))\n",
    "        else:\n",
    "            self.viz.line(X=np.array([x]), Y=np.array([y]), env=self.env, win=self.plots[var_name], name=split_name, update = 'append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_grad_flow(named_parameters):\n",
    "    \"\"\"\n",
    "        Plotting gradient flow across various layers\n",
    "        Thanks to: https://discuss.pytorch.org/t/check-gradient-flow-in-network/15063/2\n",
    "    \"\"\"   \n",
    "    ave_grads = []\n",
    "    layers = []\n",
    "    for n, p in named_parameters:\n",
    "        if(p.requires_grad) and (\"bias\" not in n):\n",
    "            layers.append(n)\n",
    "            ave_grads.append(p.grad.abs().mean())\n",
    "    plt.plot(ave_grads, alpha=0.3, color=\"b\")\n",
    "    plt.hlines(0, 0, len(ave_grads)+1, linewidth=1, color=\"k\" )\n",
    "    plt.xticks(range(0,len(ave_grads), 1), layers, rotation=\"vertical\")\n",
    "    plt.xlim(xmin=0, xmax=len(ave_grads))\n",
    "    plt.xlabel(\"Layers\")\n",
    "    plt.ylabel(\"average gradient\")\n",
    "    plt.title(\"Gradient flow\")\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The final data corpus folder: C:\\Users\\deepa\\Conversational Agents\\Datasets\\cornell movie-dialogs corpus\n"
     ]
    }
   ],
   "source": [
    "path='C:\\\\Users\\\\deepa\\\\Conversational Agents\\\\Datasets'\n",
    "dataset='cornell movie-dialogs corpus'\n",
    "\n",
    "data_folder=os.path.join(path,dataset)\n",
    "\n",
    "print(\"The final data corpus folder: \"+str(data_folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lines_conversations():\n",
    "    \"\"\"\n",
    "    Loads movie lines and conversations from the dataset.\n",
    "    \n",
    "    data_folder: Destination where conversations and lines are stored.\n",
    "    \n",
    "    movie_lines: Consist of movie lines as given by the dataset.\n",
    "    movie_conversations: Consist of movie conversations as given by the dataset.\n",
    "    \n",
    "    \"\"\"\n",
    "    f=open(os.path.join(data_folder,'movie_lines.txt'),'r')\n",
    "    movie_lines=f.read().splitlines()\n",
    "    f.close()\n",
    "    \n",
    "    f=open(os.path.join(data_folder,'movie_conversations.txt'),'r')\n",
    "    movie_conversations=f.read().splitlines()\n",
    "    f.close()\n",
    "    \n",
    "    return movie_lines,movie_conversations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting movie lines and movie conversations...\n",
      "Number of distinct lines: 304713\n",
      "Number of conversations: 83097\n",
      "Average Number of lines per conversations: 3.6669554857576085\n",
      "L1045 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ They do not!\n",
      "u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L194', 'L195', 'L196', 'L197']\n",
      "Extracting took place in: 0.47766828536987305\n"
     ]
    }
   ],
   "source": [
    "t1=time.time()\n",
    "print(\"Extracting movie lines and movie conversations...\")\n",
    "movie_lines,movie_conversations=get_lines_conversations()\n",
    "\n",
    "print(\"Number of distinct lines: \"+str(len(movie_lines)))\n",
    "print(\"Number of conversations: \"+str(len(movie_conversations)))\n",
    "print(\"Average Number of lines per conversations: \"+str(len(movie_lines)/len(movie_conversations)))\n",
    "\n",
    "print(movie_lines[0])\n",
    "print(movie_conversations[0])\n",
    "\n",
    "print(\"Extracting took place in: \"+str(time.time()-t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadLines(movie_lines,fields):\n",
    "    lines={}\n",
    "    for line in movie_lines:\n",
    "        values=line.split(\" +++$+++ \")\n",
    "        \n",
    "        lineVals={}\n",
    "        \n",
    "#         print(\"values\"+str(len(values)))\n",
    "#         print(\"fields\"+str(len(fields)))\n",
    "              \n",
    "        for i,field in enumerate(fields):\n",
    "            lineVals[field]=values[i]\n",
    "        \n",
    "        lines[lineVals['lineID']]=lineVals\n",
    "    \n",
    "    return lines\n",
    "\n",
    "def loadConversations(movie_conversations,lines,fields):\n",
    "    conversations=[]\n",
    "    \n",
    "    for convo in movie_conversations:\n",
    "        values=convo.split(\" +++$+++ \")\n",
    "        conVals={}\n",
    "       \n",
    "        for i,field in enumerate(fields):\n",
    "            conVals[field]=values[i]\n",
    "        \n",
    "        lineIDs=eval(conVals[\"utteranceIDs\"])\n",
    "        \n",
    "        conVals[\"lines\"]=[]\n",
    "        \n",
    "        for lineID in lineIDs:\n",
    "            conVals[\"lines\"].append(lines[lineID])\n",
    "        conversations.append(conVals)\n",
    "        \n",
    "    return conversations\n",
    "\n",
    "def sentencePairs(conversations):\n",
    "    qr_pairs=[]\n",
    "    \n",
    "    for conversation in conversations:\n",
    "        for i in range(len(conversation[\"lines\"])-1):\n",
    "            query=conversation[\"lines\"][i][\"text\"].strip()\n",
    "            response=conversation[\"lines\"][i+1][\"text\"].strip()\n",
    "            \n",
    "            if query and response:\n",
    "                qr_pairs.append([query,response])\n",
    "        \n",
    "    return qr_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Separating meaningfull information for our model...\n",
      "The number of query-response pairs are: 221282\n",
      "Separation took place in: 5.064160585403442\n"
     ]
    }
   ],
   "source": [
    "t1=time.time()\n",
    "print(\"Separating meaningfull information for our model...\")\n",
    "\n",
    "lines={}\n",
    "conversations=[]\n",
    "qr_pairs=[]\n",
    "\n",
    "movie_lines_fields=[\"lineID\",\"characterID\",\"movieID\",\"character\",\"text\"]\n",
    "movie_convo_fields=[\"charcaterID\",\"character2ID\",\"movieID\",\"utteranceIDs\"]\n",
    "\n",
    "lines=loadLines(movie_lines,movie_lines_fields)\n",
    "conversations=loadConversations(movie_conversations,lines,movie_convo_fields)\n",
    "qr_pairs=sentencePairs(conversations)\n",
    "\n",
    "print(\"The number of query-response pairs are: \"+str(len(qr_pairs)))\n",
    "print(\"Separation took place in: \"+str(time.time()-t1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_Token=0\n",
    "START_Token=1\n",
    "END_Token=2\n",
    "\n",
    "class Vocabulary:\n",
    "    def __init__(self):\n",
    "        self.trimmed=False\n",
    "        self.word2count={}\n",
    "        self.index2word={PAD_Token:\"PAD\",START_Token:\"SOS\",END_Token:\"EOS\"}\n",
    "        self.word2index={\"PAD\":PAD_Token,\"SOS\":START_Token,\"EOS\":END_Token}\n",
    "        self.num_words=3\n",
    "        \n",
    "    def addSentence(self,sentence):\n",
    "        for word in sentence.split(\" \"):\n",
    "            self.addWord(word)\n",
    "    def addWord(self,word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word]=self.num_words\n",
    "            self.index2word[self.num_words]=word\n",
    "            self.word2count[word]=1\n",
    "            self.num_words=self.num_words+1\n",
    "        else:\n",
    "            self.word2count[word]+=1\n",
    "            \n",
    "    def trim(self,min_count):\n",
    "        \n",
    "        if self.trimmed:\n",
    "            return\n",
    "        self.trimmed=True\n",
    "        \n",
    "        keep_words=[]\n",
    "        \n",
    "        for word,freq in self.word2count.items():\n",
    "            if freq>=min_count:\n",
    "                keep_words.append(word)\n",
    "        \n",
    "        self.word2count={}\n",
    "        self.index2word={PAD_Token:\"PAD\",START_Token:\"SOS\",END_Token:\"EOS\"}\n",
    "        self.word2index={\"PAD\":PAD_Token,\"SOS\":START_Token,\"EOS\":END_Token}\n",
    "        self.num_words=3\n",
    "        \n",
    "        for word in keep_words:\n",
    "            self.addWord(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataset and corresponding vocabulary...\n",
      "Preparation took place in: 17.518337965011597\n"
     ]
    }
   ],
   "source": [
    "Max_Length=10\n",
    "\n",
    "def normalizeString(s):\n",
    "    s=s.lower().strip()\n",
    "    s=re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s=re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    s=re.sub(r\"\\s+\", r\" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def readVocs(qr_pairs):\n",
    "    \n",
    "    for qr_pair in qr_pairs:\n",
    "        qr_pair[0]=normalizeString(qr_pair[0])\n",
    "        qr_pair[1]=normalizeString(qr_pair[1])\n",
    "    \n",
    "    voc=Vocabulary()\n",
    "    return voc,qr_pairs\n",
    "\n",
    "def filterPair(pair):\n",
    "    return len(pair[0].split(\" \"))<Max_Length and len(pair[1].split(\" \"))<Max_Length\n",
    "\n",
    "def filterPairs(qr_pairs):\n",
    "    return [pair for pair in qr_pairs if filterPair(pair)]\n",
    "\n",
    "def prepareDataset(qr_pairs):\n",
    "    voc, qr_pairs=readVocs(qr_pairs)\n",
    "    qr_pairs=filterPairs(qr_pairs)\n",
    "       \n",
    "    for pair in qr_pairs:\n",
    "        voc.addSentence(pair[0])\n",
    "        voc.addSentence(pair[1])\n",
    "#     print(\"Number\"+str(voc.num_words))\n",
    "    return voc,qr_pairs\n",
    "\n",
    "t1=time.time()\n",
    "print(\"Preparing dataset and corresponding vocabulary...\")\n",
    "voc, pairs=prepareDataset(qr_pairs)\n",
    "print(\"Preparation took place in: \"+str(time.time()-t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trimming rare words from vocabulary and dataset..\n",
      "Trimming took place in: 0.39127397537231445\n"
     ]
    }
   ],
   "source": [
    "Min_Count=3\n",
    "\n",
    "def trimRareWords(voc,qr_pairs):\n",
    "    \n",
    "    voc.trim(Min_Count)\n",
    "    keep_pairs=[]\n",
    "    \n",
    "    for pair in qr_pairs:\n",
    "        input_sentence=pair[0]\n",
    "        output_sentence=pair[1]\n",
    "        \n",
    "        keep_input=True\n",
    "        keep_output=True\n",
    "        \n",
    "        for word in input_sentence.split(\" \"):\n",
    "            if word not in voc.word2index:\n",
    "                keep_input=False\n",
    "                break\n",
    "        \n",
    "        for word in output_sentence.split(\" \"):\n",
    "            if word not in voc.word2index:\n",
    "                keep_output=False\n",
    "                break\n",
    "                \n",
    "        if keep_input and keep_output:\n",
    "            keep_pairs.append(pair)\n",
    "            \n",
    "    return keep_pairs\n",
    "\n",
    "t1=time.time()\n",
    "print(\"Trimming rare words from vocabulary and dataset..\")\n",
    "\n",
    "pairs=trimRareWords(voc,pairs)\n",
    "\n",
    "print(\"Trimming took place in: \"+str(time.time()-t1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(voc,sentence):\n",
    "    tokenised_sentence=[]\n",
    "    tokenised_sentence.append(START_Token)\n",
    "    \n",
    "    for word in sentence.split(\" \"):\n",
    "        tokenised_sentence.append(voc.word2index[word])\n",
    "        \n",
    "    tokenised_sentence.append(END_Token)\n",
    "    \n",
    "    assert len(tokenised_sentence)<=Max_Length+2\n",
    "    for _ in range(Max_Length+2-len(tokenised_sentence)):\n",
    "        tokenised_sentence.append(PAD_Token)\n",
    "        \n",
    "    return tokenised_sentence\n",
    "\n",
    "def binaryMatrix(l,value=PAD_Token):\n",
    "    m=[]\n",
    "    for i,seq in enumerate(l):\n",
    "        m.append([])\n",
    "        for token in seq:\n",
    "            if token==value:\n",
    "                m[i].append(0)\n",
    "            else:\n",
    "                m[i].append(1)\n",
    "        \n",
    "    return m\n",
    "\n",
    "def inputVar(voc,l):\n",
    "    \n",
    "    indexes_batch=[indexesFromSentence(voc,sentence) for sentence in l]\n",
    "    input_lengths=torch.tensor([len(index) for index in indexes_batch])\n",
    "    padVar=torch.LongTensor(indexes_batch)\n",
    "    return input_lengths,padVar\n",
    "\n",
    "def outputVar(voc,l):\n",
    "    indexes_batch=[indexesFromSentence(voc,sentence) for sentence in l]\n",
    "    max_target_len=torch.tensor([len(index) for index in indexes_batch])\n",
    "    mask=binaryMatrix(indexes_batch)\n",
    "    mask=torch.ByteTensor(mask)\n",
    "    padVar=torch.LongTensor(indexes_batch)\n",
    "    return max_target_len, mask, padVar\n",
    "\n",
    "def batch2TrainData(voc,pair_batch):\n",
    "    #sort function see \n",
    "    input_batch=[]\n",
    "    output_batch=[]\n",
    "\n",
    "    for pair in pair_batch:\n",
    "        input_batch.append(pair[0])\n",
    "        output_batch.append(pair[1])\n",
    "                                  \n",
    "    \n",
    "    input_lengths,tokenised_input=inputVar(voc,input_batch)\n",
    "    max_out_length,mask,tokenised_output=outputVar(voc,output_batch)\n",
    "    return input_lengths,tokenised_input,max_out_length,mask,tokenised_output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query-response pairs after all the preprocessing: 53113\n",
      "Input length: tensor([12, 12, 12, 12, 12]) Size: torch.Size([5])\n",
      "--------------------------------------------------------------------------------\n",
      "Tokenised Input: tensor([[   1,   33,    2,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [   1,   76,   37,   67,   12,  465, 1300,    4,    2,    0,    0,    0],\n",
      "        [   1, 1425,   66,    2,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [   1,  562,    4,    7, 1095,   45,  129,    6,    2,    0,    0,    0],\n",
      "        [   1, 5302,    4,    2,    0,    0,    0,    0,    0,    0,    0,    0]]) Size: torch.Size([5, 12])\n",
      "--------------------------------------------------------------------------------\n",
      "Max out length: tensor([12, 12, 12, 12, 12]) Size: torch.Size([5])\n",
      "--------------------------------------------------------------------------------\n",
      "Mask: tensor([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]], dtype=torch.uint8) Size: torch.Size([5, 12])\n",
      "--------------------------------------------------------------------------------\n",
      "Tokenised Output: tensor([[   1,   33,   50,    6,    2,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [   1,   45, 6072,   96,  169, 7204,    4,    2,    0,    0,    0,    0],\n",
      "        [   1,  153,    7, 2896,    6,    2,    0,    0,    0,    0,    0,    0],\n",
      "        [   1, 1291,   56,  795,    4,    2,    0,    0,    0,    0,    0,    0],\n",
      "        [   1,   50,   37,   61,   18,    6,    2,    0,    0,    0,    0,    0]]) Size: torch.Size([5, 12])\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of query-response pairs after all the preprocessing: \"+str(len(pairs)))\n",
    "\n",
    "#Sample batch\n",
    "batch=[random.choice(pairs) for _ in range(5)]\n",
    "input_lengths,tokenised_input,max_out_length,mask,tokenised_output=batch2TrainData(voc,batch)\n",
    "\n",
    "print(\"Input length: \"+str(input_lengths)+\" Size: \"+str(input_lengths.shape))\n",
    "print(\"-\"*80)\n",
    "print(\"Tokenised Input: \"+str(tokenised_input)+\" Size: \"+str(tokenised_input.shape))\n",
    "print(\"-\"*80)\n",
    "print(\"Max out length: \"+str(max_out_length)+\" Size: \"+str(max_out_length.shape))\n",
    "print(\"-\"*80)\n",
    "print(\"Mask: \"+str(mask)+\" Size: \"+str(mask.shape))\n",
    "print(\"-\"*80)\n",
    "print(\"Tokenised Output: \"+str(tokenised_output)+\" Size: \"+str(tokenised_output.shape))\n",
    "print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A standard Encoder-Decoder architecture. Base for this and many \n",
    "    other models.\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.generator = generator\n",
    "        \n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        \"Take in and process masked src and target sequences.\"\n",
    "        return self.decode(self.encode(src, src_mask), src_mask,\n",
    "                            tgt, tgt_mask)\n",
    "    \n",
    "    def encode(self, src, src_mask):\n",
    "        return self.encoder(self.src_embed(src), src_mask)\n",
    "    \n",
    "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
    "        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"Define standard linear + softmax generation step.\"\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Generator, self).__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.log_softmax(self.proj(x), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clones(module, N):\n",
    "    \"Produce N identical layers.\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"Core encoder is a stack of N layers\"\n",
    "    def __init__(self, layer, N):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        \"Pass the input (and mask) through each layer in turn.\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"Construct a layernorm module (See citation for details).\"\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual connection followed by a layer norm.\n",
    "    Note for code simplicity the norm is first as opposed to last.\n",
    "    \"\"\"\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"Apply residual connection to any sublayer with the same size.\"\n",
    "        return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \"Encoder is made up of self-attn and feed forward (defined below)\"\n",
    "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"Follow Figure 1 (left) for connections.\"\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
    "        return self.sublayer[1](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"Generic N layer decoder with masking.\"\n",
    "    def __init__(self, layer, N):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "        \n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, memory, src_mask, tgt_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \"Decoder is made of self-attn, src-attn, and feed forward (defined below)\"\n",
    "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.size = size\n",
    "        self.self_attn = self_attn\n",
    "        self.src_attn = src_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n",
    " \n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        \"Follow Figure 1 (right) for connections.\"\n",
    "        m = memory\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
    "        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n",
    "        return self.sublayer[2](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsequent_mask(size):\n",
    "    \"Mask out subsequent positions.\"\n",
    "    attn_shape = (1, size, size)\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
    "    return torch.from_numpy(subsequent_mask) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    \"Compute 'Scaled Dot Product Attention'\"\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) \\\n",
    "             / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    p_attn = F.softmax(scores, dim = -1)\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    return torch.matmul(p_attn, value), p_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        \"Take in model size and number of heads.\"\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert d_model % h == 0\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"Implements Figure 2\"\n",
    "        if mask is not None:\n",
    "            # Same mask applied to all h heads.\n",
    "            mask = mask.unsqueeze(1)\n",
    "        nbatches = query.size(0)\n",
    "        \n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k \n",
    "        query, key, value = \\\n",
    "            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "             for l, x in zip(self.linears, (query, key, value))]\n",
    "        \n",
    "        # 2) Apply attention on all the projected vectors in batch. \n",
    "        x, self.attn = attention(query, key, value, mask=mask, \n",
    "                                 dropout=self.dropout)\n",
    "        \n",
    "        # 3) \"Concat\" using a view and apply a final linear. \n",
    "        x = x.transpose(1, 2).contiguous() \\\n",
    "             .view(nbatches, -1, self.h * self.d_k)\n",
    "        return self.linears[-1](x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"Implements FFN equation.\"\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(F.relu(self.w_1(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.lut = nn.Embedding(vocab, d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lut(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"Implement the PE function.\"\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) *\n",
    "                             -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + Variable(self.pe[:, :x.size(1)], \n",
    "                         requires_grad=False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(src_vocab, tgt_vocab, N=6, \n",
    "               d_model=512, d_ff=2048, h=8, dropout=0.1):\n",
    "    \"Helper: Construct a model from hyperparameters.\"\n",
    "    c = copy.deepcopy\n",
    "    attn = MultiHeadedAttention(h, d_model)\n",
    "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "    position = PositionalEncoding(d_model, dropout)\n",
    "    model = EncoderDecoder(\n",
    "        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
    "        Decoder(DecoderLayer(d_model, c(attn), c(attn), \n",
    "                             c(ff), dropout), N),\n",
    "        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),\n",
    "        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),\n",
    "        Generator(d_model, tgt_vocab))\n",
    "    \n",
    "    # This was important from their code. \n",
    "    # Initialize parameters with Glorot / fan_avg.\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform(p)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    \n",
    "    def __init__(self,encoder,decoder,source_embed,target_embed,generator):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder=encoder\n",
    "        self.decoder=decoder\n",
    "        \n",
    "        self.source_embed=source_embed\n",
    "        self.target_embed=target_embed\n",
    "        \n",
    "        self.generator=generator # Linear + Log_softmax\n",
    "        \n",
    "    def forward(self,source,target,source_mask,target_mask):\n",
    "        return self.decode(self.encode(source,source_mask),source_mask,target,target_mask)\n",
    "    \n",
    "    def encode(self,source,source_mask):\n",
    "        return self.encoder(self.source_embed(source),source_mask)\n",
    "    \n",
    "    def decode(self,memory, source_mask,target,target_mask):\n",
    "        return self.decoder(self.target_embed(target),memory,source_mask,target_mask)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \n",
    "    def __init__(self,d_model,vocab_size):\n",
    "        super().__init__()\n",
    "        self.projection=nn.Linear(d_model,vocab_size)\n",
    "        \n",
    "    def forward(self,decoder_output):\n",
    "        return F.log_softmax(self.projection(decoder_output),dim=-1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clones(module,N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self,layer,N):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layers=clones(layer,N)\n",
    "        self.norm=LayerNorm(layer.size)\n",
    "    \n",
    "    def forward(self,x,mask):\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x=layer(x,mask)\n",
    "        \n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self,size,self_attn,feed_forward,dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attn=self_attn\n",
    "        self.feed_forward=feed_forward\n",
    "        self.sublayer=clones(SublayerConnection(size,dropout),2)\n",
    "        self.size=size\n",
    "        \n",
    "    def forward(self,x,mask):\n",
    "        \n",
    "        x=self.sublayer[0](x,lambda x: self.attn(x,x,x,mask))\n",
    "        return self.sublayer[1](x,self.feed_forward)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \n",
    "    def __init__(self,features,eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.a_2=nn.Parameter(torch.ones(features))\n",
    "        self.b_2=nn.Parameter(torch.zeros(features))\n",
    "        self.eps=eps\n",
    "        \n",
    "    def forward(self,x):\n",
    "        mean=x.mean(-1,keepdim=True)\n",
    "        std=x.std(-1,keepdim=True)\n",
    "        return self.a_2*(x-mean)/(x+std)+self.b_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SublayerConnection(nn.Module):\n",
    "    \n",
    "    def __init__(self,size,dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "        self.norm=LayerNorm(size)\n",
    "        \n",
    "    def forward(self,x,sublayer):\n",
    "        return x+self.dropout(sublayer(self.norm(x)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self,layer,N):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layers=clones(layer,N)\n",
    "        self.norm=LayerNorm(layer.size)\n",
    "    \n",
    "    def forward(self,x,memory,curr_mask,tgt_mask):\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x=layer(x,memory,curr_mask,tgt_mask)\n",
    "            \n",
    "        return self.norm(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self,size,self_attn,src_attn,feed_forward,dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.size=size\n",
    "        self.self_attn=self_attn\n",
    "        self.src_attn=src_attn\n",
    "        self.feed_forward=feed_forward\n",
    "        \n",
    "        self.sublayer=clones(SublayerConnection(size,dropout),3)\n",
    "        \n",
    "    def forward(self,x,memory,src_mask,tgt_mask):\n",
    "        \n",
    "        m=memory\n",
    "        x=self.sublayer[0](x,lambda x:self.self_attn(x,x,x,tgt_mask))\n",
    "        x=self.sublayer[1](x,lambda x: self.src_attn(x,m,m,src_mask))\n",
    "        return self.sublayer[2](x,self.feed_forward)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self,h,d_model,dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert d_model%h==0\n",
    "        \n",
    "        self.d_k=d_model//h\n",
    "        self.h=h\n",
    "        self.linears=clones(nn.Linear(d_model,d_model),4)\n",
    "        self.attn=None\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self,query,key,values,mask=None):\n",
    "        \n",
    "        if mask is not None:\n",
    "            mask=mask.unsqueeze(1)\n",
    "            \n",
    "        nbatches=query.size(0)\n",
    "        \n",
    "        query,key,values=[l(x).view(nbatches,-1,self.h,self.d_k).transpose(1,2) for l, x in zip(self.linears,(query,key,values))]\n",
    "        \n",
    "        x,self.attn=attention(query,key,values,mask=mask,dropout=self.dropout)\n",
    "        \n",
    "        x=x.transpose(1,2).contiguous().view(nbatches,-1,self.h*self.d_k)\n",
    "        \n",
    "        return self.linears[-1](x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(query,key,value,mask=None,dropout=None):\n",
    "    \n",
    "    d_k=query.size(-1)\n",
    "\n",
    "    scores=torch.matmul(query,key.transpose(-2,-1))/math.sqrt(d_k)\n",
    "    \n",
    "    if mask is not None:\n",
    "        scores=scores.masked_fill(mask==0,-1e9)\n",
    "        \n",
    "    p_attn=F.softmax(scores,dim=-1)\n",
    "    \n",
    "    if dropout is not None:\n",
    "        p_attn=dropout(p_attn)\n",
    "        \n",
    "    return torch.matmul(p_attn,value),p_attn\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \n",
    "    def __init__(self,d_model,d_ff,dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.w_1=nn.Linear(d_model,d_ff)\n",
    "        self.w_2=nn.Linear(d_ff,d_model)\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.w_2(self.dropout(F.relu(self.w_1(x))))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    \n",
    "    def __init__(self,d_model,vocab):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embed=nn.Embedding(vocab,d_model)\n",
    "        self.d_model=d_model\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.embed(x)*math.sqrt(self.d_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \n",
    "    def __init__(self,d_model,dropout,max_len=5000):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "        pe=torch.zeros(max_len,d_model,dtype=torch.float)\n",
    "        position=torch.arange(0.,max_len).unsqueeze(1)\n",
    "        div_term=torch.exp(torch.arange(0.,d_model,2)*-(math.log(10000.0)/d_model))\n",
    "        \n",
    "        pe[:,0::2]=torch.sin(position*div_term)\n",
    "        pe[:,1::2]=torch.cos(position*div_term)\n",
    "        \n",
    "        pe=pe.unsqueeze(0)\n",
    "        self.register_buffer('pe',pe)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        x=x+Variable(self.pe[:,:x.size(1)],requires_grad=False)\n",
    "        return self.dropout(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model2(src_vocab,tgt_vocab,N=6,d_model=512,d_ff=2048,h=8,dropout=0.1):\n",
    "    \n",
    "    c=copy.deepcopy\n",
    "    attn=MultiHeadedAttention(h,d_model)\n",
    "    ff=PositionwiseFeedForward(d_model,d_ff,dropout)\n",
    "    position=PositionalEncoding(d_model,dropout)\n",
    "    model=EncoderDecoder(Encoder(EncoderLayer(d_model,c(attn),c(ff),dropout),N),\n",
    "                        Decoder(DecoderLayer(d_model,c(attn),c(attn),c(ff),dropout),N),\n",
    "                        nn.Sequential(Embeddings(d_model,src_vocab),c(position)),\n",
    "                        nn.Sequential(Embeddings(d_model,tgt_vocab),c(position)),\n",
    "                        Generator(d_model,tgt_vocab))\n",
    "    \n",
    "    for p in model.parameters():\n",
    "        if p.dim()>1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\deepa\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:20: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n"
     ]
    }
   ],
   "source": [
    "sample_model=make_model(voc.num_words,voc.num_words,1,512,2048,8,0.1)\n",
    "# print(sample_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Output size: torch.Size([5, 12, 512])\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#Sample Run\n",
    "source=torch.ones(5,12,dtype=torch.long)\n",
    "target=torch.ones(5,12,dtype=torch.long)\n",
    "source_mask=None\n",
    "target_mask=torch.ones(5,12,12,dtype=torch.long)\n",
    "out=sample_model(source,target,source_mask,target_mask)\n",
    "print(\"-\"*80)\n",
    "print(\"Output size: \"+str(out.shape))\n",
    "print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "triu function generates a copy of matrix with elemens below kth diagonal zeroed.\n",
    "The main diagonal is zeroeth diagonal above is first(k=1) and so on.\n",
    "\n",
    "Eg:\n",
    "A=[[1,2,3],[4,5,6],[7,8,9]]\n",
    "for above matrix:\n",
    "triu(A,k=1)\n",
    "will give [[0,2,3],[0,0,6],[0,0,0]]\n",
    "\"\"\"\n",
    "\n",
    "def subsequent_mask(size):\n",
    "    attn_shape=(1,size,size)\n",
    "    mask=np.triu(np.ones(attn_shape),k=1).astype('uint8')\n",
    "    \n",
    "    return torch.from_numpy(mask)==0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generation(pairs,batch_size,n_batches):\n",
    "    \n",
    "    sample_batches=[batch2TrainData(voc,[random.choice(pairs) for _ in range(batch_size)]) for _ in range(n_batches)]\n",
    "    batches=[]\n",
    "    \n",
    "    for i in range(n_batches):\n",
    "        batches.append(Batch(sample_batches[i],PAD_Token))\n",
    "    \n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch:\n",
    "    \n",
    "    def __init__(self,sample_batch,pad):\n",
    "        \n",
    "        self.src=sample_batch[1]\n",
    "        self.src_mask=self.make_src_mask(self.src,pad)\n",
    "        self.trg=sample_batch[-1][:,:-1]\n",
    "        self.trg_mask=self.make_trg_mask(self.trg,pad)\n",
    "        self.trg_y=sample_batch[-1][:,1:]\n",
    "        self.ntokens=(self.trg_y!=pad).data.sum()\n",
    "        \n",
    "    @staticmethod\n",
    "    def make_src_mask(src,pad):\n",
    "        return (src!=pad).unsqueeze(-2)\n",
    "    @staticmethod    \n",
    "    def make_trg_mask(trg,pad):\n",
    "        trg_mask=(trg!=pad).unsqueeze(-2)\n",
    "        trg_mask=trg_mask&Variable(subsequent_mask(trg.size(-1)).type_as(trg_mask.data))\n",
    "        return trg_mask\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(data,model,loss_compute):\n",
    "    \n",
    "    start_time=time.time()\n",
    "    total_tokens=0\n",
    "    total_loss=0\n",
    "    tokens=0\n",
    "    \n",
    "    out=model(data.src,data.trg,data.src_mask,data.trg_mask)\n",
    "    loss=loss_compute(out,data.trg_y,data.ntokens)\n",
    "    \n",
    "    return loss\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 2])\n",
      "torch.Size([2, 2])\n",
      "tensor([[[0.8808, 0.1192],\n",
      "         [0.7311, 0.2689]],\n",
      "\n",
      "        [[0.1192, 0.8808],\n",
      "         [0.1192, 0.8808]]])\n",
      "tensor(2.1269)\n",
      "tensor(1.3133)\n",
      "tensor(2.1269)\n",
      "tensor(2.1269)\n",
      "tensor(7.6940)\n"
     ]
    }
   ],
   "source": [
    "x=torch.tensor([[[4,2],[3,2]],[[2,4],[5,7]]],dtype=torch.float)\n",
    "trg=torch.tensor([[1,1],[0,0]])\n",
    "print(x.shape)\n",
    "print(trg.shape)\n",
    "print(F.softmax(x,dim=-1))\n",
    "y=F.softmax(x,dim=-1)\n",
    "loss=0\n",
    "for i in range(trg.size()[0]):\n",
    "    for j in range(trg.size()[1]):\n",
    "        target=trg[i][j]\n",
    "        currVal=-torch.log(y[i][j][target])\n",
    "        loss+=currVal\n",
    "        print(currVal)\n",
    "print(loss)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def customLossFunction(outputs,target):\n",
    "    batch_size=outputs.size()[0]\n",
    "    numberOfWords=outputs.size()[1]\n",
    "    outputs=F.softmax(outputs,dim=-1)\n",
    "    loss=0\n",
    "    normalisingVal=0\n",
    "#     print(outputs)\n",
    "#     print(target)\n",
    "    for i in range(batch_size):\n",
    "        for j in range(numberOfWords):\n",
    "            trg=target[i][j]\n",
    "            if trg!=0:\n",
    "                \n",
    "                currLoss=-(outputs[i][j][trg]+5)\n",
    "                loss+=currLoss\n",
    "                normalisingVal+=1\n",
    "    return loss/normalisingVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-5.1941)\n"
     ]
    }
   ],
   "source": [
    "x=torch.tensor([[[4,2],[3,2]],[[2,4],[5,7]]],dtype=torch.float)\n",
    "trg=torch.tensor([[1,1],[0,0]])\n",
    "print(customLossFunction(x,trg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class LabelSmoothing(nn.Module):\n",
    "    \n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.criteria=customLossFunction()\n",
    "#     def forward(self,x,target):\n",
    "#         return self.criteria(x,target)\n",
    "# class LabelSmoothing(nn.Module):\n",
    "#     \"Implement label smoothing.\"\n",
    "#     def __init__(self, size, padding_idx, smoothing=0.0):\n",
    "#         super(LabelSmoothing, self).__init__()\n",
    "#         self.criterion = nn.KLDivLoss(size_average=False)\n",
    "#         self.padding_idx = padding_idx\n",
    "#         self.confidence = 1.0 - smoothing\n",
    "#         self.smoothing = smoothing\n",
    "#         self.size = size\n",
    "#         self.true_dist = None\n",
    "        \n",
    "#     def forward(self, x, target):\n",
    "#         assert x.size(1) == self.size\n",
    "#         true_dist = x.data.clone()\n",
    "#         true_dist.fill_(self.smoothing / (self.size - 2))\n",
    "#         true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "#         true_dist[:, self.padding_idx] = 0\n",
    "#         mask = torch.nonzero(target.data == self.padding_idx)\n",
    "#         if mask.dim() > 0:\n",
    "#             true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
    "#         self.true_dist = true_dist\n",
    "#         return self.criterion(x, Variable(true_dist, requires_grad=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossCompute:\n",
    "    \n",
    "    def __init__(self,model,opt):\n",
    "        \n",
    "        self.opt=opt\n",
    "        self.model=model\n",
    "    \n",
    "    def __call__(self,x,y,norm):\n",
    "        \n",
    "        x=self.model.generator(x)\n",
    "        loss=customLossFunction(x,y)\n",
    "        \n",
    "    \n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        _=nn.utils.clip_grad_norm_(model.parameters(),50.0)\n",
    "        \n",
    "#         plot_grad_flow(self.model.named_parameters())\n",
    "        \n",
    "        self.opt.step()\n",
    "        self.opt.zero_grad()\n",
    "        \n",
    "        return loss.item()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches=data_generation(pairs,5,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialising and creating models....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\deepa\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:20: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Creating Models took: 0.7191314697265625\n",
      "Epoch: 0 Loss Value: -5.000136375427246\n",
      "Epoch: 1 Loss Value: -5.000407695770264\n",
      "Epoch: 2 Loss Value: -5.001376152038574\n",
      "Epoch: 3 Loss Value: -5.003411293029785\n",
      "Epoch: 4 Loss Value: -5.000217437744141\n",
      "Epoch: 5 Loss Value: -5.0172905921936035\n",
      "Epoch: 6 Loss Value: -5.01046895980835\n",
      "Epoch: 7 Loss Value: -5.015929222106934\n",
      "Epoch: 8 Loss Value: -5.00731086730957\n",
      "Epoch: 9 Loss Value: -5.012537002563477\n",
      "Epoch: 10 Loss Value: -5.027617931365967\n",
      "Epoch: 11 Loss Value: -5.045120716094971\n",
      "Epoch: 12 Loss Value: -5.033352851867676\n",
      "Epoch: 13 Loss Value: -5.031441688537598\n",
      "Epoch: 14 Loss Value: -5.000166893005371\n",
      "Epoch: 15 Loss Value: -5.067634105682373\n",
      "Epoch: 16 Loss Value: -5.02806282043457\n",
      "Epoch: 17 Loss Value: -5.039186477661133\n",
      "Epoch: 18 Loss Value: -5.015294075012207\n",
      "Epoch: 19 Loss Value: -5.027140140533447\n",
      "Epoch: 20 Loss Value: -5.055109024047852\n",
      "Epoch: 21 Loss Value: -5.085672378540039\n",
      "Epoch: 22 Loss Value: -5.061043739318848\n",
      "Epoch: 23 Loss Value: -5.055520057678223\n",
      "Epoch: 24 Loss Value: -5.000194072723389\n",
      "Epoch: 25 Loss Value: -5.1165995597839355\n",
      "Epoch: 26 Loss Value: -5.0474371910095215\n",
      "Epoch: 27 Loss Value: -5.064007759094238\n",
      "Epoch: 28 Loss Value: -5.024706840515137\n",
      "Epoch: 29 Loss Value: -5.042237758636475\n",
      "Epoch: 30 Loss Value: -5.084460258483887\n",
      "Epoch: 31 Loss Value: -5.128358364105225\n",
      "Epoch: 32 Loss Value: -5.089320182800293\n",
      "Epoch: 33 Loss Value: -5.079726219177246\n",
      "Epoch: 34 Loss Value: -5.000182628631592\n",
      "Epoch: 35 Loss Value: -5.160926818847656\n",
      "Epoch: 36 Loss Value: -5.064203262329102\n",
      "Epoch: 37 Loss Value: -5.08476448059082\n",
      "Epoch: 38 Loss Value: -5.032039642333984\n",
      "Epoch: 39 Loss Value: -5.054165840148926\n",
      "Epoch: 40 Loss Value: -5.106203556060791\n",
      "Epoch: 41 Loss Value: -5.158720970153809\n",
      "Epoch: 42 Loss Value: -5.108397006988525\n",
      "Epoch: 43 Loss Value: -5.0950164794921875\n",
      "Epoch: 44 Loss Value: -5.000164985656738\n",
      "Epoch: 45 Loss Value: -5.187113285064697\n",
      "Epoch: 46 Loss Value: -5.073577880859375\n",
      "Epoch: 47 Loss Value: -5.0962090492248535\n",
      "Epoch: 48 Loss Value: -5.035914897918701\n",
      "Epoch: 49 Loss Value: -5.0601983070373535\n",
      "Epoch: 50 Loss Value: -5.1172308921813965\n",
      "Epoch: 51 Loss Value: -5.173793792724609\n",
      "Epoch: 52 Loss Value: -5.117745876312256\n",
      "Epoch: 53 Loss Value: -5.102550983428955\n",
      "Epoch: 54 Loss Value: -5.0001726150512695\n",
      "Epoch: 55 Loss Value: -5.199425220489502\n",
      "Epoch: 56 Loss Value: -5.078004360198975\n",
      "Epoch: 57 Loss Value: -5.101400375366211\n",
      "Epoch: 58 Loss Value: -5.037752151489258\n",
      "Epoch: 59 Loss Value: -5.062958717346191\n",
      "Epoch: 60 Loss Value: -5.1220479011535645\n",
      "Epoch: 61 Loss Value: -5.180654525756836\n",
      "Epoch: 62 Loss Value: -5.121847152709961\n",
      "Epoch: 63 Loss Value: -5.105849266052246\n",
      "Epoch: 64 Loss Value: -5.000192165374756\n",
      "Epoch: 65 Loss Value: -5.204876899719238\n",
      "Epoch: 66 Loss Value: -5.07995080947876\n",
      "Epoch: 67 Loss Value: -5.103785037994385\n",
      "Epoch: 68 Loss Value: -5.03858757019043\n",
      "Epoch: 69 Loss Value: -5.064250469207764\n",
      "Epoch: 70 Loss Value: -5.124370098114014\n",
      "Epoch: 71 Loss Value: -5.183780670166016\n",
      "Epoch: 72 Loss Value: -5.123865127563477\n",
      "Epoch: 73 Loss Value: -5.107522487640381\n",
      "Epoch: 74 Loss Value: -5.000248908996582\n",
      "Epoch: 75 Loss Value: -5.207629203796387\n",
      "Epoch: 76 Loss Value: -5.080989837646484\n",
      "Epoch: 77 Loss Value: -5.105005264282227\n",
      "Epoch: 78 Loss Value: -5.039070129394531\n",
      "Epoch: 79 Loss Value: -5.064970970153809\n",
      "Epoch: 80 Loss Value: -5.125634670257568\n",
      "Epoch: 81 Loss Value: -5.185561180114746\n",
      "Epoch: 82 Loss Value: -5.1249494552612305\n",
      "Epoch: 83 Loss Value: -5.1084113121032715\n",
      "Epoch: 84 Loss Value: -5.000365257263184\n",
      "Epoch: 85 Loss Value: -5.209169864654541\n",
      "Epoch: 86 Loss Value: -5.081600189208984\n",
      "Epoch: 87 Loss Value: -5.105729579925537\n",
      "Epoch: 88 Loss Value: -5.039418697357178\n",
      "Epoch: 89 Loss Value: -5.065430164337158\n",
      "Epoch: 90 Loss Value: -5.126358985900879\n",
      "Epoch: 91 Loss Value: -5.186601161956787\n",
      "Epoch: 92 Loss Value: -5.125604152679443\n",
      "Epoch: 93 Loss Value: -5.108978748321533\n",
      "Epoch: 94 Loss Value: -5.0006303787231445\n",
      "Epoch: 95 Loss Value: -5.210083484649658\n",
      "Epoch: 96 Loss Value: -5.082150459289551\n",
      "Epoch: 97 Loss Value: -5.1062445640563965\n",
      "Epoch: 98 Loss Value: -5.0398054122924805\n",
      "Epoch: 99 Loss Value: -5.0658392906188965\n",
      "Epoch: 100 Loss Value: -5.126938343048096\n",
      "Epoch: 101 Loss Value: -5.187443256378174\n",
      "Epoch: 102 Loss Value: -5.126029014587402\n",
      "Epoch: 103 Loss Value: -5.109402179718018\n",
      "Epoch: 104 Loss Value: -5.00132942199707\n",
      "Epoch: 105 Loss Value: -5.210574626922607\n",
      "Epoch: 106 Loss Value: -5.082842826843262\n",
      "Epoch: 107 Loss Value: -5.106652736663818\n",
      "Epoch: 108 Loss Value: -5.0407633781433105\n",
      "Epoch: 109 Loss Value: -5.066588878631592\n",
      "Epoch: 110 Loss Value: -5.127430438995361\n",
      "Epoch: 111 Loss Value: -5.188218593597412\n",
      "Epoch: 112 Loss Value: -5.12630033493042\n",
      "Epoch: 113 Loss Value: -5.10999870300293\n",
      "Epoch: 114 Loss Value: -5.004913330078125\n",
      "Epoch: 115 Loss Value: -5.210014343261719\n",
      "Epoch: 116 Loss Value: -5.0859055519104\n",
      "Epoch: 117 Loss Value: -5.107889175415039\n",
      "Epoch: 118 Loss Value: -5.049099922180176\n",
      "Epoch: 119 Loss Value: -5.075993061065674\n",
      "Epoch: 120 Loss Value: -5.134052276611328\n",
      "Epoch: 121 Loss Value: -5.204100131988525\n",
      "Epoch: 122 Loss Value: -5.1268534660339355\n",
      "Epoch: 123 Loss Value: -5.127913951873779\n",
      "Epoch: 124 Loss Value: -5.191059112548828\n",
      "Epoch: 125 Loss Value: -5.175019264221191\n",
      "Epoch: 126 Loss Value: -5.195605754852295\n",
      "Epoch: 127 Loss Value: -5.128228187561035\n",
      "Epoch: 128 Loss Value: -5.187781810760498\n",
      "Epoch: 129 Loss Value: -5.157267093658447\n",
      "Epoch: 130 Loss Value: -5.153503894805908\n",
      "Epoch: 131 Loss Value: -5.2283806800842285\n",
      "Epoch: 132 Loss Value: -5.123878479003906\n",
      "Epoch: 133 Loss Value: -5.134683609008789\n",
      "Epoch: 134 Loss Value: -5.230563640594482\n",
      "Epoch: 135 Loss Value: -5.1749267578125\n",
      "Epoch: 136 Loss Value: -5.2033514976501465\n",
      "Epoch: 137 Loss Value: -5.132606506347656\n",
      "Epoch: 138 Loss Value: -5.195552825927734\n",
      "Epoch: 139 Loss Value: -5.163478374481201\n",
      "Epoch: 140 Loss Value: -5.1589813232421875\n",
      "Epoch: 141 Loss Value: -5.234987735748291\n",
      "Epoch: 142 Loss Value: -5.1268534660339355\n",
      "Epoch: 143 Loss Value: -5.137275218963623\n",
      "Epoch: 144 Loss Value: -5.2335638999938965\n",
      "Epoch: 145 Loss Value: -5.177398681640625\n",
      "Epoch: 146 Loss Value: -5.205451488494873\n",
      "Epoch: 147 Loss Value: -5.133912563323975\n",
      "Epoch: 148 Loss Value: -5.1970930099487305\n",
      "Epoch: 149 Loss Value: -5.164702415466309\n",
      "Epoch: 150 Loss Value: -5.160049915313721\n",
      "Epoch: 151 Loss Value: -5.2363739013671875\n",
      "Epoch: 152 Loss Value: -5.127511024475098\n",
      "Epoch: 153 Loss Value: -5.13797664642334\n",
      "Epoch: 154 Loss Value: -5.234979152679443\n",
      "Epoch: 155 Loss Value: -5.178097724914551\n",
      "Epoch: 156 Loss Value: -5.206428527832031\n",
      "Epoch: 157 Loss Value: -5.134321689605713\n",
      "Epoch: 158 Loss Value: -5.19808292388916\n",
      "Epoch: 159 Loss Value: -5.165353298187256\n",
      "Epoch: 160 Loss Value: -5.16044282913208\n",
      "Epoch: 161 Loss Value: -5.236897945404053\n",
      "Epoch: 162 Loss Value: -5.127745628356934\n",
      "Epoch: 163 Loss Value: -5.13824462890625\n",
      "Epoch: 164 Loss Value: -5.235855579376221\n",
      "Epoch: 165 Loss Value: -5.17816162109375\n",
      "Epoch: 166 Loss Value: -5.206927299499512\n",
      "Epoch: 167 Loss Value: -5.13456392288208\n",
      "Epoch: 168 Loss Value: -5.198561191558838\n",
      "Epoch: 169 Loss Value: -5.165646553039551\n",
      "Epoch: 170 Loss Value: -5.160648822784424\n",
      "Epoch: 171 Loss Value: -5.237241268157959\n",
      "Epoch: 172 Loss Value: -5.127806186676025\n",
      "Epoch: 173 Loss Value: -5.138350009918213\n",
      "Epoch: 174 Loss Value: -5.236321449279785\n",
      "Epoch: 175 Loss Value: -5.178260326385498\n",
      "Epoch: 176 Loss Value: -5.207239627838135\n",
      "Epoch: 177 Loss Value: -5.134674072265625\n",
      "Epoch: 178 Loss Value: -5.198822021484375\n",
      "Epoch: 179 Loss Value: -5.165835857391357\n",
      "Epoch: 180 Loss Value: -5.160756587982178\n",
      "Epoch: 181 Loss Value: -5.237351417541504\n",
      "Epoch: 182 Loss Value: -5.127890110015869\n",
      "Epoch: 183 Loss Value: -5.138449668884277\n",
      "Epoch: 184 Loss Value: -5.2366533279418945\n",
      "Epoch: 185 Loss Value: -5.178307056427002\n",
      "Epoch: 186 Loss Value: -5.207400321960449\n",
      "Epoch: 187 Loss Value: -5.134735584259033\n",
      "Epoch: 188 Loss Value: -5.199010848999023\n",
      "Epoch: 189 Loss Value: -5.165984153747559\n",
      "Epoch: 190 Loss Value: -5.160855293273926\n",
      "Epoch: 191 Loss Value: -5.2374491691589355\n",
      "Epoch: 192 Loss Value: -5.127936840057373\n",
      "Epoch: 193 Loss Value: -5.13851261138916\n",
      "Epoch: 194 Loss Value: -5.236841678619385\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 195 Loss Value: -5.178333759307861\n",
      "Epoch: 196 Loss Value: -5.207535266876221\n",
      "Epoch: 197 Loss Value: -5.134806156158447\n",
      "Epoch: 198 Loss Value: -5.199157238006592\n",
      "Epoch: 199 Loss Value: -5.166072368621826\n",
      "Epoch: 200 Loss Value: -5.160897254943848\n",
      "Epoch: 201 Loss Value: -5.237556457519531\n",
      "Epoch: 202 Loss Value: -5.127991676330566\n",
      "Epoch: 203 Loss Value: -5.138577938079834\n",
      "Epoch: 204 Loss Value: -5.236991882324219\n",
      "Epoch: 205 Loss Value: -5.178390979766846\n",
      "Epoch: 206 Loss Value: -5.207624912261963\n",
      "Epoch: 207 Loss Value: -5.134852409362793\n",
      "Epoch: 208 Loss Value: -5.19924783706665\n",
      "Epoch: 209 Loss Value: -5.166126728057861\n",
      "Epoch: 210 Loss Value: -5.160961151123047\n",
      "Epoch: 211 Loss Value: -5.237647533416748\n",
      "Epoch: 212 Loss Value: -5.128021240234375\n",
      "Epoch: 213 Loss Value: -5.13861608505249\n",
      "Epoch: 214 Loss Value: -5.237084865570068\n",
      "Epoch: 215 Loss Value: -5.178433895111084\n",
      "Epoch: 216 Loss Value: -5.207726955413818\n",
      "Epoch: 217 Loss Value: -5.134887218475342\n",
      "Epoch: 218 Loss Value: -5.199328422546387\n",
      "Epoch: 219 Loss Value: -5.1661906242370605\n",
      "Epoch: 220 Loss Value: -5.161016464233398\n",
      "Epoch: 221 Loss Value: -5.237715244293213\n",
      "Epoch: 222 Loss Value: -5.128045558929443\n",
      "Epoch: 223 Loss Value: -5.1386590003967285\n",
      "Epoch: 224 Loss Value: -5.237186431884766\n",
      "Epoch: 225 Loss Value: -5.1784820556640625\n",
      "Epoch: 226 Loss Value: -5.207770347595215\n",
      "Epoch: 227 Loss Value: -5.13491678237915\n",
      "Epoch: 228 Loss Value: -5.199382305145264\n",
      "Epoch: 229 Loss Value: -5.166245937347412\n",
      "Epoch: 230 Loss Value: -5.16105842590332\n",
      "Epoch: 231 Loss Value: -5.237752914428711\n",
      "Epoch: 232 Loss Value: -5.128085613250732\n",
      "Epoch: 233 Loss Value: -5.138700008392334\n",
      "Epoch: 234 Loss Value: -5.237265586853027\n",
      "Epoch: 235 Loss Value: -5.178534030914307\n",
      "Epoch: 236 Loss Value: -5.207821846008301\n",
      "Epoch: 237 Loss Value: -5.1349616050720215\n",
      "Epoch: 238 Loss Value: -5.199444770812988\n",
      "Epoch: 239 Loss Value: -5.166277885437012\n",
      "Epoch: 240 Loss Value: -5.161086082458496\n",
      "Epoch: 241 Loss Value: -5.237824440002441\n",
      "Epoch: 242 Loss Value: -5.128120422363281\n",
      "Epoch: 243 Loss Value: -5.138742923736572\n",
      "Epoch: 244 Loss Value: -5.23730993270874\n",
      "Epoch: 245 Loss Value: -5.178581714630127\n",
      "Epoch: 246 Loss Value: -5.207866191864014\n",
      "Epoch: 247 Loss Value: -5.135011196136475\n",
      "Epoch: 248 Loss Value: -5.19948673248291\n",
      "Epoch: 249 Loss Value: -5.166318893432617\n",
      "Epoch: 250 Loss Value: -5.161137104034424\n",
      "Epoch: 251 Loss Value: -5.2378716468811035\n",
      "Epoch: 252 Loss Value: -5.1281538009643555\n",
      "Epoch: 253 Loss Value: -5.138779163360596\n",
      "Epoch: 254 Loss Value: -5.237348556518555\n",
      "Epoch: 255 Loss Value: -5.178661346435547\n",
      "Epoch: 256 Loss Value: -5.207918643951416\n",
      "Epoch: 257 Loss Value: -5.135047912597656\n",
      "Epoch: 258 Loss Value: -5.199516773223877\n",
      "Epoch: 259 Loss Value: -5.16636848449707\n",
      "Epoch: 260 Loss Value: -5.1612114906311035\n",
      "Epoch: 261 Loss Value: -5.2380051612854\n",
      "Epoch: 262 Loss Value: -5.128200531005859\n",
      "Epoch: 263 Loss Value: -5.138827800750732\n",
      "Epoch: 264 Loss Value: -5.2373833656311035\n",
      "Epoch: 265 Loss Value: -5.178730487823486\n",
      "Epoch: 266 Loss Value: -5.207939624786377\n",
      "Epoch: 267 Loss Value: -5.1351189613342285\n",
      "Epoch: 268 Loss Value: -5.199559211730957\n",
      "Epoch: 269 Loss Value: -5.1664042472839355\n",
      "Epoch: 270 Loss Value: -5.161285400390625\n",
      "Epoch: 271 Loss Value: -5.238121032714844\n",
      "Epoch: 272 Loss Value: -5.128319263458252\n",
      "Epoch: 273 Loss Value: -5.138904094696045\n",
      "Epoch: 274 Loss Value: -5.237345218658447\n",
      "Epoch: 275 Loss Value: -5.17899227142334\n",
      "Epoch: 276 Loss Value: -5.208019733428955\n",
      "Epoch: 277 Loss Value: -5.135225772857666\n",
      "Epoch: 278 Loss Value: -5.199588775634766\n",
      "Epoch: 279 Loss Value: -5.166484355926514\n",
      "Epoch: 280 Loss Value: -5.161494255065918\n",
      "Epoch: 281 Loss Value: -5.238521575927734\n",
      "Epoch: 282 Loss Value: -5.128530502319336\n",
      "Epoch: 283 Loss Value: -5.139218330383301\n",
      "Epoch: 284 Loss Value: -5.237114429473877\n",
      "Epoch: 285 Loss Value: -5.179894924163818\n",
      "Epoch: 286 Loss Value: -5.2081780433654785\n",
      "Epoch: 287 Loss Value: -5.135931968688965\n",
      "Epoch: 288 Loss Value: -5.199772834777832\n",
      "Epoch: 289 Loss Value: -5.167012691497803\n",
      "Epoch: 290 Loss Value: -5.163065433502197\n",
      "Epoch: 291 Loss Value: -5.2420783042907715\n",
      "Epoch: 292 Loss Value: -5.132714748382568\n",
      "Epoch: 293 Loss Value: -5.1463494300842285\n",
      "Epoch: 294 Loss Value: -5.227432727813721\n",
      "Epoch: 295 Loss Value: -5.223445892333984\n",
      "Epoch: 296 Loss Value: -5.2273430824279785\n",
      "Epoch: 297 Loss Value: -5.207092761993408\n",
      "Epoch: 298 Loss Value: -5.219907760620117\n",
      "Epoch: 299 Loss Value: -5.213095664978027\n",
      "Epoch: 300 Loss Value: -5.283848285675049\n",
      "Epoch: 301 Loss Value: -5.419816017150879\n",
      "Epoch: 302 Loss Value: -5.25176477432251\n",
      "Epoch: 303 Loss Value: -5.246144771575928\n",
      "Epoch: 304 Loss Value: -5.189542293548584\n",
      "Epoch: 305 Loss Value: -5.387594699859619\n",
      "Epoch: 306 Loss Value: -5.28887939453125\n",
      "Epoch: 307 Loss Value: -5.240814685821533\n",
      "Epoch: 308 Loss Value: -5.238516330718994\n",
      "Epoch: 309 Loss Value: -5.231836795806885\n",
      "Epoch: 310 Loss Value: -5.28701114654541\n",
      "Epoch: 311 Loss Value: -5.425596237182617\n",
      "Epoch: 312 Loss Value: -5.254675388336182\n",
      "Epoch: 313 Loss Value: -5.248502254486084\n",
      "Epoch: 314 Loss Value: -5.23490047454834\n",
      "Epoch: 315 Loss Value: -5.390408515930176\n",
      "Epoch: 316 Loss Value: -5.290288925170898\n",
      "Epoch: 317 Loss Value: -5.24201774597168\n",
      "Epoch: 318 Loss Value: -5.239151477813721\n",
      "Epoch: 319 Loss Value: -5.232410907745361\n",
      "Epoch: 320 Loss Value: -5.288952350616455\n",
      "Epoch: 321 Loss Value: -5.426581382751465\n",
      "Epoch: 322 Loss Value: -5.255211353302002\n",
      "Epoch: 323 Loss Value: -5.248866081237793\n",
      "Epoch: 324 Loss Value: -5.2375807762146\n",
      "Epoch: 325 Loss Value: -5.390963554382324\n",
      "Epoch: 326 Loss Value: -5.249512195587158\n",
      "Epoch: 327 Loss Value: -5.242301940917969\n",
      "Epoch: 328 Loss Value: -5.239358901977539\n",
      "Epoch: 329 Loss Value: -5.2326226234436035\n",
      "Epoch: 330 Loss Value: -5.289266586303711\n",
      "Epoch: 331 Loss Value: -5.4269633293151855\n",
      "Epoch: 332 Loss Value: -5.255471706390381\n",
      "Epoch: 333 Loss Value: -5.249140739440918\n",
      "Epoch: 334 Loss Value: -5.237133026123047\n",
      "Epoch: 335 Loss Value: -5.39138126373291\n",
      "Epoch: 336 Loss Value: -5.249510288238525\n",
      "Epoch: 337 Loss Value: -5.242481231689453\n",
      "Epoch: 338 Loss Value: -5.2394700050354\n",
      "Epoch: 339 Loss Value: -5.232741355895996\n",
      "Epoch: 340 Loss Value: -5.289449214935303\n",
      "Epoch: 341 Loss Value: -5.426651477813721\n",
      "Epoch: 342 Loss Value: -5.255630016326904\n",
      "Epoch: 343 Loss Value: -5.2492876052856445\n",
      "Epoch: 344 Loss Value: -5.237716197967529\n",
      "Epoch: 345 Loss Value: -5.391610622406006\n",
      "Epoch: 346 Loss Value: -5.249580383300781\n",
      "Epoch: 347 Loss Value: -5.242546081542969\n",
      "Epoch: 348 Loss Value: -5.2395243644714355\n",
      "Epoch: 349 Loss Value: -5.23281192779541\n",
      "Epoch: 350 Loss Value: -5.289559364318848\n",
      "Epoch: 351 Loss Value: -5.42746639251709\n",
      "Epoch: 352 Loss Value: -5.255714416503906\n",
      "Epoch: 353 Loss Value: -5.249358654022217\n",
      "Epoch: 354 Loss Value: -5.237772464752197\n",
      "Epoch: 355 Loss Value: -5.391793727874756\n",
      "Epoch: 356 Loss Value: -5.2908854484558105\n",
      "Epoch: 357 Loss Value: -5.242650508880615\n",
      "Epoch: 358 Loss Value: -5.239587306976318\n",
      "Epoch: 359 Loss Value: -5.232882499694824\n",
      "Epoch: 360 Loss Value: -5.289661884307861\n",
      "Epoch: 361 Loss Value: -5.427616119384766\n",
      "Epoch: 362 Loss Value: -5.255816459655762\n",
      "Epoch: 363 Loss Value: -5.249461650848389\n",
      "Epoch: 364 Loss Value: -5.237786293029785\n",
      "Epoch: 365 Loss Value: -5.3919501304626465\n",
      "Epoch: 366 Loss Value: -5.291133880615234\n",
      "Epoch: 367 Loss Value: -5.242744445800781\n",
      "Epoch: 368 Loss Value: -5.239643573760986\n",
      "Epoch: 369 Loss Value: -5.232935428619385\n",
      "Epoch: 370 Loss Value: -5.289747714996338\n",
      "Epoch: 371 Loss Value: -5.427736759185791\n",
      "Epoch: 372 Loss Value: -5.2558770179748535\n",
      "Epoch: 373 Loss Value: -5.249518871307373\n",
      "Epoch: 374 Loss Value: -5.2378106117248535\n",
      "Epoch: 375 Loss Value: -5.392031669616699\n",
      "Epoch: 376 Loss Value: -5.291196823120117\n",
      "Epoch: 377 Loss Value: -5.242793560028076\n",
      "Epoch: 378 Loss Value: -5.239655017852783\n",
      "Epoch: 379 Loss Value: -5.232973098754883\n",
      "Epoch: 380 Loss Value: -5.289788722991943\n",
      "Epoch: 381 Loss Value: -5.427798748016357\n",
      "Epoch: 382 Loss Value: -5.255936145782471\n",
      "Epoch: 383 Loss Value: -5.2495622634887695\n",
      "Epoch: 384 Loss Value: -5.23783540725708\n",
      "Epoch: 385 Loss Value: -5.392120361328125\n",
      "Epoch: 386 Loss Value: -5.291238307952881\n",
      "Epoch: 387 Loss Value: -5.242832183837891\n",
      "Epoch: 388 Loss Value: -5.2397027015686035\n",
      "Epoch: 389 Loss Value: -5.233006000518799\n",
      "Epoch: 390 Loss Value: -5.289849758148193\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 391 Loss Value: -5.427879810333252\n",
      "Epoch: 392 Loss Value: -5.255983829498291\n",
      "Epoch: 393 Loss Value: -5.2495646476745605\n",
      "Epoch: 394 Loss Value: -5.237850666046143\n",
      "Epoch: 395 Loss Value: -5.39218282699585\n",
      "Epoch: 396 Loss Value: -5.29127836227417\n",
      "Epoch: 397 Loss Value: -5.242871284484863\n",
      "Epoch: 398 Loss Value: -5.239727973937988\n",
      "Epoch: 399 Loss Value: -5.233037948608398\n",
      "Epoch: 400 Loss Value: -5.289891242980957\n",
      "Epoch: 401 Loss Value: -5.4279351234436035\n",
      "Epoch: 402 Loss Value: -5.256014347076416\n",
      "Epoch: 403 Loss Value: -5.249634742736816\n",
      "Epoch: 404 Loss Value: -5.237874507904053\n",
      "Epoch: 405 Loss Value: -5.392242908477783\n",
      "Epoch: 406 Loss Value: -5.291306018829346\n",
      "Epoch: 407 Loss Value: -5.242901802062988\n",
      "Epoch: 408 Loss Value: -5.239750862121582\n",
      "Epoch: 409 Loss Value: -5.233057975769043\n",
      "Epoch: 410 Loss Value: -5.289928913116455\n",
      "Epoch: 411 Loss Value: -5.427987098693848\n",
      "Epoch: 412 Loss Value: -5.256049156188965\n",
      "Epoch: 413 Loss Value: -5.249666690826416\n",
      "Epoch: 414 Loss Value: -5.237889766693115\n",
      "Epoch: 415 Loss Value: -5.392289638519287\n",
      "Epoch: 416 Loss Value: -5.291341781616211\n",
      "Epoch: 417 Loss Value: -5.242934226989746\n",
      "Epoch: 418 Loss Value: -5.239770412445068\n",
      "Epoch: 419 Loss Value: -5.23307991027832\n",
      "Epoch: 420 Loss Value: -5.289959907531738\n",
      "Epoch: 421 Loss Value: -5.428029537200928\n",
      "Epoch: 422 Loss Value: -5.2560811042785645\n",
      "Epoch: 423 Loss Value: -5.249698162078857\n",
      "Epoch: 424 Loss Value: -5.237888336181641\n",
      "Epoch: 425 Loss Value: -5.392343044281006\n",
      "Epoch: 426 Loss Value: -5.291368007659912\n",
      "Epoch: 427 Loss Value: -5.242959022521973\n",
      "Epoch: 428 Loss Value: -5.239788055419922\n",
      "Epoch: 429 Loss Value: -5.233100414276123\n",
      "Epoch: 430 Loss Value: -5.289989948272705\n",
      "Epoch: 431 Loss Value: -5.428086757659912\n",
      "Epoch: 432 Loss Value: -5.256107807159424\n",
      "Epoch: 433 Loss Value: -5.2497172355651855\n",
      "Epoch: 434 Loss Value: -5.237465858459473\n",
      "Epoch: 435 Loss Value: -5.392381191253662\n",
      "Epoch: 436 Loss Value: -5.291390895843506\n",
      "Epoch: 437 Loss Value: -5.242979526519775\n",
      "Epoch: 438 Loss Value: -5.239780426025391\n",
      "Epoch: 439 Loss Value: -5.233120441436768\n",
      "Epoch: 440 Loss Value: -5.290013313293457\n",
      "Epoch: 441 Loss Value: -5.428105354309082\n",
      "Epoch: 442 Loss Value: -5.2561235427856445\n",
      "Epoch: 443 Loss Value: -5.249704360961914\n",
      "Epoch: 444 Loss Value: -5.237928867340088\n",
      "Epoch: 445 Loss Value: -5.392409801483154\n",
      "Epoch: 446 Loss Value: -5.249836444854736\n",
      "Epoch: 447 Loss Value: -5.243000507354736\n",
      "Epoch: 448 Loss Value: -5.2397847175598145\n",
      "Epoch: 449 Loss Value: -5.233132362365723\n",
      "Epoch: 450 Loss Value: -5.290030002593994\n",
      "Epoch: 451 Loss Value: -5.428146839141846\n",
      "Epoch: 452 Loss Value: -5.256145477294922\n",
      "Epoch: 453 Loss Value: -5.249757766723633\n",
      "Epoch: 454 Loss Value: -5.237950325012207\n",
      "Epoch: 455 Loss Value: -5.392447471618652\n",
      "Epoch: 456 Loss Value: -5.249852657318115\n",
      "Epoch: 457 Loss Value: -5.243014812469482\n",
      "Epoch: 458 Loss Value: -5.239834785461426\n",
      "Epoch: 459 Loss Value: -5.233152389526367\n",
      "Epoch: 460 Loss Value: -5.290055274963379\n",
      "Epoch: 461 Loss Value: -5.428164482116699\n",
      "Epoch: 462 Loss Value: -5.256157875061035\n",
      "Epoch: 463 Loss Value: -5.249763011932373\n",
      "Epoch: 464 Loss Value: -5.237963676452637\n",
      "Epoch: 465 Loss Value: -5.392481327056885\n",
      "Epoch: 466 Loss Value: -5.249868392944336\n",
      "Epoch: 467 Loss Value: -5.243034839630127\n",
      "Epoch: 468 Loss Value: -5.2398457527160645\n",
      "Epoch: 469 Loss Value: -5.233163833618164\n",
      "Epoch: 470 Loss Value: -5.29006814956665\n",
      "Epoch: 471 Loss Value: -5.428178787231445\n",
      "Epoch: 472 Loss Value: -5.256190776824951\n",
      "Epoch: 473 Loss Value: -5.2497968673706055\n",
      "Epoch: 474 Loss Value: -5.237977504730225\n",
      "Epoch: 475 Loss Value: -5.392510414123535\n",
      "Epoch: 476 Loss Value: -5.2498779296875\n",
      "Epoch: 477 Loss Value: -5.243049621582031\n",
      "Epoch: 478 Loss Value: -5.239855766296387\n",
      "Epoch: 479 Loss Value: -5.2331767082214355\n",
      "Epoch: 480 Loss Value: -5.2900872230529785\n",
      "Epoch: 481 Loss Value: -5.428232192993164\n",
      "Epoch: 482 Loss Value: -5.256203651428223\n",
      "Epoch: 483 Loss Value: -5.249809741973877\n",
      "Epoch: 484 Loss Value: -5.237984657287598\n",
      "Epoch: 485 Loss Value: -5.39253568649292\n",
      "Epoch: 486 Loss Value: -5.2499003410339355\n",
      "Epoch: 487 Loss Value: -5.243066787719727\n",
      "Epoch: 488 Loss Value: -5.239869594573975\n",
      "Epoch: 489 Loss Value: -5.233185291290283\n",
      "Epoch: 490 Loss Value: -5.290116310119629\n",
      "Epoch: 491 Loss Value: -5.428258419036865\n",
      "Epoch: 492 Loss Value: -5.256214141845703\n",
      "Epoch: 493 Loss Value: -5.249823570251465\n",
      "Epoch: 494 Loss Value: -5.2379913330078125\n",
      "Epoch: 495 Loss Value: -5.392550468444824\n",
      "Epoch: 496 Loss Value: -5.249900817871094\n",
      "Epoch: 497 Loss Value: -5.243077278137207\n",
      "Epoch: 498 Loss Value: -5.239876747131348\n",
      "Epoch: 499 Loss Value: -5.2332000732421875\n",
      "Epoch: 500 Loss Value: -5.290129661560059\n",
      "Epoch: 501 Loss Value: -5.428255558013916\n",
      "Epoch: 502 Loss Value: -5.256235122680664\n",
      "Epoch: 503 Loss Value: -5.249836444854736\n",
      "Epoch: 504 Loss Value: -5.2379984855651855\n",
      "Epoch: 505 Loss Value: -5.392580986022949\n",
      "Epoch: 506 Loss Value: -5.2498979568481445\n",
      "Epoch: 507 Loss Value: -5.243089199066162\n",
      "Epoch: 508 Loss Value: -5.2398834228515625\n",
      "Epoch: 509 Loss Value: -5.233208179473877\n",
      "Epoch: 510 Loss Value: -5.290143966674805\n",
      "Epoch: 511 Loss Value: -5.428305625915527\n",
      "Epoch: 512 Loss Value: -5.256246566772461\n",
      "Epoch: 513 Loss Value: -5.249849796295166\n",
      "Epoch: 514 Loss Value: -5.238007068634033\n",
      "Epoch: 515 Loss Value: -5.392599582672119\n",
      "Epoch: 516 Loss Value: -5.291452884674072\n",
      "Epoch: 517 Loss Value: -5.243100166320801\n",
      "Epoch: 518 Loss Value: -5.239894390106201\n",
      "Epoch: 519 Loss Value: -5.23321533203125\n",
      "Epoch: 520 Loss Value: -5.290156364440918\n",
      "Epoch: 521 Loss Value: -5.428321361541748\n",
      "Epoch: 522 Loss Value: -5.2562575340271\n",
      "Epoch: 523 Loss Value: -5.249858379364014\n",
      "Epoch: 524 Loss Value: -5.238008975982666\n",
      "Epoch: 525 Loss Value: -5.3926191329956055\n",
      "Epoch: 526 Loss Value: -5.2501373291015625\n",
      "Epoch: 527 Loss Value: -5.243108749389648\n",
      "Epoch: 528 Loss Value: -5.239900588989258\n",
      "Epoch: 529 Loss Value: -5.233218193054199\n",
      "Epoch: 530 Loss Value: -5.290165424346924\n",
      "Epoch: 531 Loss Value: -5.428336143493652\n",
      "Epoch: 532 Loss Value: -5.256266117095947\n",
      "Epoch: 533 Loss Value: -5.249866962432861\n",
      "Epoch: 534 Loss Value: -5.238011837005615\n",
      "Epoch: 535 Loss Value: -5.392635345458984\n",
      "Epoch: 536 Loss Value: -5.291529178619385\n",
      "Epoch: 537 Loss Value: -5.243116855621338\n",
      "Epoch: 538 Loss Value: -5.239903450012207\n",
      "Epoch: 539 Loss Value: -5.23322868347168\n",
      "Epoch: 540 Loss Value: -5.2901740074157715\n",
      "Epoch: 541 Loss Value: -5.4283552169799805\n",
      "Epoch: 542 Loss Value: -5.256278991699219\n",
      "Epoch: 543 Loss Value: -5.249876976013184\n",
      "Epoch: 544 Loss Value: -5.238012313842773\n",
      "Epoch: 545 Loss Value: -5.3926496505737305\n",
      "Epoch: 546 Loss Value: -5.2915425300598145\n",
      "Epoch: 547 Loss Value: -5.2431254386901855\n",
      "Epoch: 548 Loss Value: -5.239911079406738\n",
      "Epoch: 549 Loss Value: -5.233238697052002\n",
      "Epoch: 550 Loss Value: -5.290189743041992\n",
      "Epoch: 551 Loss Value: -5.428369522094727\n",
      "Epoch: 552 Loss Value: -5.256289482116699\n",
      "Epoch: 553 Loss Value: -5.249887466430664\n",
      "Epoch: 554 Loss Value: -5.238025665283203\n",
      "Epoch: 555 Loss Value: -5.392669677734375\n",
      "Epoch: 556 Loss Value: -5.2915520668029785\n",
      "Epoch: 557 Loss Value: -5.243137836456299\n",
      "Epoch: 558 Loss Value: -5.239917755126953\n",
      "Epoch: 559 Loss Value: -5.233245849609375\n",
      "Epoch: 560 Loss Value: -5.290200233459473\n",
      "Epoch: 561 Loss Value: -5.428389072418213\n",
      "Epoch: 562 Loss Value: -5.256296634674072\n",
      "Epoch: 563 Loss Value: -5.2498955726623535\n",
      "Epoch: 564 Loss Value: -5.238025188446045\n",
      "Epoch: 565 Loss Value: -5.3926801681518555\n",
      "Epoch: 566 Loss Value: -5.291558265686035\n",
      "Epoch: 567 Loss Value: -5.243144989013672\n",
      "Epoch: 568 Loss Value: -5.239923000335693\n",
      "Epoch: 569 Loss Value: -5.233250617980957\n",
      "Epoch: 570 Loss Value: -5.29020881652832\n",
      "Epoch: 571 Loss Value: -5.428401470184326\n",
      "Epoch: 572 Loss Value: -5.25630521774292\n",
      "Epoch: 573 Loss Value: -5.249902725219727\n",
      "Epoch: 574 Loss Value: -5.23803186416626\n",
      "Epoch: 575 Loss Value: -5.392695426940918\n",
      "Epoch: 576 Loss Value: -5.291569232940674\n",
      "Epoch: 577 Loss Value: -5.243153095245361\n",
      "Epoch: 578 Loss Value: -5.239931106567383\n",
      "Epoch: 579 Loss Value: -5.233254909515381\n",
      "Epoch: 580 Loss Value: -5.2902140617370605\n",
      "Epoch: 581 Loss Value: -5.428416728973389\n",
      "Epoch: 582 Loss Value: -5.256313800811768\n",
      "Epoch: 583 Loss Value: -5.249910354614258\n",
      "Epoch: 584 Loss Value: -5.238036155700684\n",
      "Epoch: 585 Loss Value: -5.392708778381348\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 586 Loss Value: -5.291572093963623\n",
      "Epoch: 587 Loss Value: -5.24315881729126\n",
      "Epoch: 588 Loss Value: -5.239933967590332\n",
      "Epoch: 589 Loss Value: -5.233262538909912\n",
      "Epoch: 590 Loss Value: -5.290223598480225\n",
      "Epoch: 591 Loss Value: -5.428426265716553\n",
      "Epoch: 592 Loss Value: -5.256320476531982\n",
      "Epoch: 593 Loss Value: -5.249917030334473\n",
      "Epoch: 594 Loss Value: -5.238025665283203\n",
      "Epoch: 595 Loss Value: -5.392719268798828\n",
      "Epoch: 596 Loss Value: -5.291580677032471\n",
      "Epoch: 597 Loss Value: -5.2431640625\n",
      "Epoch: 598 Loss Value: -5.239938735961914\n",
      "Epoch: 599 Loss Value: -5.233269214630127\n",
      "Epoch: 600 Loss Value: -5.290231227874756\n",
      "Epoch: 601 Loss Value: -5.42843770980835\n",
      "Epoch: 602 Loss Value: -5.256327152252197\n",
      "Epoch: 603 Loss Value: -5.249922275543213\n",
      "Epoch: 604 Loss Value: -5.238028526306152\n",
      "Epoch: 605 Loss Value: -5.392726898193359\n",
      "Epoch: 606 Loss Value: -5.291585445404053\n",
      "Epoch: 607 Loss Value: -5.243170261383057\n",
      "Epoch: 608 Loss Value: -5.23994255065918\n",
      "Epoch: 609 Loss Value: -5.233273029327393\n",
      "Epoch: 610 Loss Value: -5.290236949920654\n",
      "Epoch: 611 Loss Value: -5.428445816040039\n",
      "Epoch: 612 Loss Value: -5.256332874298096\n",
      "Epoch: 613 Loss Value: -5.249927043914795\n",
      "Epoch: 614 Loss Value: -5.238048076629639\n",
      "Epoch: 615 Loss Value: -5.392735958099365\n",
      "Epoch: 616 Loss Value: -5.291594982147217\n",
      "Epoch: 617 Loss Value: -5.243175506591797\n",
      "Epoch: 618 Loss Value: -5.2399468421936035\n",
      "Epoch: 619 Loss Value: -5.2332763671875\n",
      "Epoch: 620 Loss Value: -5.290243625640869\n",
      "Epoch: 621 Loss Value: -5.428455352783203\n",
      "Epoch: 622 Loss Value: -5.256337642669678\n",
      "Epoch: 623 Loss Value: -5.249932765960693\n",
      "Epoch: 624 Loss Value: -5.238048553466797\n",
      "Epoch: 625 Loss Value: -5.392745018005371\n",
      "Epoch: 626 Loss Value: -5.291598796844482\n",
      "Epoch: 627 Loss Value: -5.243179798126221\n",
      "Epoch: 628 Loss Value: -5.239950656890869\n",
      "Epoch: 629 Loss Value: -5.233279705047607\n",
      "Epoch: 630 Loss Value: -5.290249347686768\n",
      "Epoch: 631 Loss Value: -5.428462982177734\n",
      "Epoch: 632 Loss Value: -5.25634241104126\n",
      "Epoch: 633 Loss Value: -5.249937057495117\n",
      "Epoch: 634 Loss Value: -5.23805046081543\n",
      "Epoch: 635 Loss Value: -5.3927531242370605\n",
      "Epoch: 636 Loss Value: -5.291602611541748\n",
      "Epoch: 637 Loss Value: -5.2431840896606445\n",
      "Epoch: 638 Loss Value: -5.239953517913818\n",
      "Epoch: 639 Loss Value: -5.233283519744873\n",
      "Epoch: 640 Loss Value: -5.290253639221191\n",
      "Epoch: 641 Loss Value: -5.428469181060791\n",
      "Epoch: 642 Loss Value: -5.256346702575684\n",
      "Epoch: 643 Loss Value: -5.249941349029541\n",
      "Epoch: 644 Loss Value: -5.238053798675537\n",
      "Epoch: 645 Loss Value: -5.392756938934326\n",
      "Epoch: 646 Loss Value: -5.291604995727539\n",
      "Epoch: 647 Loss Value: -5.243188858032227\n",
      "Epoch: 648 Loss Value: -5.239955902099609\n",
      "Epoch: 649 Loss Value: -5.233286380767822\n",
      "Epoch: 650 Loss Value: -5.290256977081299\n",
      "Epoch: 651 Loss Value: -5.428475379943848\n",
      "Epoch: 652 Loss Value: -5.256350994110107\n",
      "Epoch: 653 Loss Value: -5.249944686889648\n",
      "Epoch: 654 Loss Value: -5.238058090209961\n",
      "Epoch: 655 Loss Value: -5.392765522003174\n",
      "Epoch: 656 Loss Value: -5.291610240936279\n",
      "Epoch: 657 Loss Value: -5.243190765380859\n",
      "Epoch: 658 Loss Value: -5.239959716796875\n",
      "Epoch: 659 Loss Value: -5.233290195465088\n",
      "Epoch: 660 Loss Value: -5.290262699127197\n",
      "Epoch: 661 Loss Value: -5.428482532501221\n",
      "Epoch: 662 Loss Value: -5.256352424621582\n",
      "Epoch: 663 Loss Value: -5.249948501586914\n",
      "Epoch: 664 Loss Value: -5.238060474395752\n",
      "Epoch: 665 Loss Value: -5.392773151397705\n",
      "Epoch: 666 Loss Value: -5.291613578796387\n",
      "Epoch: 667 Loss Value: -5.243194580078125\n",
      "Epoch: 668 Loss Value: -5.239962100982666\n",
      "Epoch: 669 Loss Value: -5.2332916259765625\n",
      "Epoch: 670 Loss Value: -5.2902655601501465\n",
      "Epoch: 671 Loss Value: -5.428485870361328\n",
      "Epoch: 672 Loss Value: -5.2563581466674805\n",
      "Epoch: 673 Loss Value: -5.2499518394470215\n",
      "Epoch: 674 Loss Value: -5.238062381744385\n",
      "Epoch: 675 Loss Value: -5.392777442932129\n",
      "Epoch: 676 Loss Value: -5.2916154861450195\n",
      "Epoch: 677 Loss Value: -5.243198394775391\n",
      "Epoch: 678 Loss Value: -5.239964008331299\n",
      "Epoch: 679 Loss Value: -5.233294486999512\n",
      "Epoch: 680 Loss Value: -5.290269374847412\n",
      "Epoch: 681 Loss Value: -5.428493022918701\n",
      "Epoch: 682 Loss Value: -5.2563629150390625\n",
      "Epoch: 683 Loss Value: -5.249954700469971\n",
      "Epoch: 684 Loss Value: -5.238065242767334\n",
      "Epoch: 685 Loss Value: -5.392782688140869\n",
      "Epoch: 686 Loss Value: -5.291618824005127\n",
      "Epoch: 687 Loss Value: -5.243200778961182\n",
      "Epoch: 688 Loss Value: -5.23996639251709\n",
      "Epoch: 689 Loss Value: -5.233297348022461\n",
      "Epoch: 690 Loss Value: -5.290272235870361\n",
      "Epoch: 691 Loss Value: -5.4284987449646\n",
      "Epoch: 692 Loss Value: -5.256364822387695\n",
      "Epoch: 693 Loss Value: -5.249956130981445\n",
      "Epoch: 694 Loss Value: -5.238065242767334\n",
      "Epoch: 695 Loss Value: -5.392786979675293\n",
      "Epoch: 696 Loss Value: -5.291623592376709\n",
      "Epoch: 697 Loss Value: -5.243204116821289\n",
      "Epoch: 698 Loss Value: -5.2399678230285645\n",
      "Epoch: 699 Loss Value: -5.233299255371094\n",
      "Epoch: 700 Loss Value: -5.290276527404785\n",
      "Epoch: 701 Loss Value: -5.428501129150391\n",
      "Epoch: 702 Loss Value: -5.256368160247803\n",
      "Epoch: 703 Loss Value: -5.249959945678711\n",
      "Epoch: 704 Loss Value: -5.238068103790283\n",
      "Epoch: 705 Loss Value: -5.392791748046875\n",
      "Epoch: 706 Loss Value: -5.2916259765625\n",
      "Epoch: 707 Loss Value: -5.243205547332764\n",
      "Epoch: 708 Loss Value: -5.239969253540039\n",
      "Epoch: 709 Loss Value: -5.233301162719727\n",
      "Epoch: 710 Loss Value: -5.290278911590576\n",
      "Epoch: 711 Loss Value: -5.428506851196289\n",
      "Epoch: 712 Loss Value: -5.2563700675964355\n",
      "Epoch: 713 Loss Value: -5.24996280670166\n",
      "Epoch: 714 Loss Value: -5.238056659698486\n",
      "Epoch: 715 Loss Value: -5.392795562744141\n",
      "Epoch: 716 Loss Value: -5.291629314422607\n",
      "Epoch: 717 Loss Value: -5.243208408355713\n",
      "Epoch: 718 Loss Value: -5.2399725914001465\n",
      "Epoch: 719 Loss Value: -5.233304500579834\n",
      "Epoch: 720 Loss Value: -5.290281295776367\n",
      "Epoch: 721 Loss Value: -5.428510665893555\n",
      "Epoch: 722 Loss Value: -5.256374359130859\n",
      "Epoch: 723 Loss Value: -5.249965667724609\n",
      "Epoch: 724 Loss Value: -5.238072395324707\n",
      "Epoch: 725 Loss Value: -5.3927998542785645\n",
      "Epoch: 726 Loss Value: -5.291630268096924\n",
      "Epoch: 727 Loss Value: -5.243211269378662\n",
      "Epoch: 728 Loss Value: -5.239974498748779\n",
      "Epoch: 729 Loss Value: -5.23330545425415\n",
      "Epoch: 730 Loss Value: -5.290284156799316\n",
      "Epoch: 731 Loss Value: -5.42851448059082\n",
      "Epoch: 732 Loss Value: -5.256375789642334\n",
      "Epoch: 733 Loss Value: -5.249967575073242\n",
      "Epoch: 734 Loss Value: -5.238072872161865\n",
      "Epoch: 735 Loss Value: -5.392803192138672\n",
      "Epoch: 736 Loss Value: -5.291633129119873\n",
      "Epoch: 737 Loss Value: -5.2432122230529785\n",
      "Epoch: 738 Loss Value: -5.239976406097412\n",
      "Epoch: 739 Loss Value: -5.233307361602783\n",
      "Epoch: 740 Loss Value: -5.290286540985107\n",
      "Epoch: 741 Loss Value: -5.428518772125244\n",
      "Epoch: 742 Loss Value: -5.256377220153809\n",
      "Epoch: 743 Loss Value: -5.2499680519104\n",
      "Epoch: 744 Loss Value: -5.23807430267334\n",
      "Epoch: 745 Loss Value: -5.39280366897583\n",
      "Epoch: 746 Loss Value: -5.291635513305664\n",
      "Epoch: 747 Loss Value: -5.2432146072387695\n",
      "Epoch: 748 Loss Value: -5.23997688293457\n",
      "Epoch: 749 Loss Value: -5.233307838439941\n",
      "Epoch: 750 Loss Value: -5.290287971496582\n",
      "Epoch: 751 Loss Value: -5.428522109985352\n",
      "Epoch: 752 Loss Value: -5.256379127502441\n",
      "Epoch: 753 Loss Value: -5.24997091293335\n",
      "Epoch: 754 Loss Value: -5.238076210021973\n",
      "Epoch: 755 Loss Value: -5.392809867858887\n",
      "Epoch: 756 Loss Value: -5.291637420654297\n",
      "Epoch: 757 Loss Value: -5.243216514587402\n",
      "Epoch: 758 Loss Value: -5.239978790283203\n",
      "Epoch: 759 Loss Value: -5.233309745788574\n",
      "Epoch: 760 Loss Value: -5.290290355682373\n",
      "Epoch: 761 Loss Value: -5.428524017333984\n",
      "Epoch: 762 Loss Value: -5.256381034851074\n",
      "Epoch: 763 Loss Value: -5.249973297119141\n",
      "Epoch: 764 Loss Value: -5.238077163696289\n",
      "Epoch: 765 Loss Value: -5.392812252044678\n",
      "Epoch: 766 Loss Value: -5.29163932800293\n",
      "Epoch: 767 Loss Value: -5.243217945098877\n",
      "Epoch: 768 Loss Value: -5.239980697631836\n",
      "Epoch: 769 Loss Value: -5.233311653137207\n",
      "Epoch: 770 Loss Value: -5.290292739868164\n",
      "Epoch: 771 Loss Value: -5.428526878356934\n",
      "Epoch: 772 Loss Value: -5.256382465362549\n",
      "Epoch: 773 Loss Value: -5.249974250793457\n",
      "Epoch: 774 Loss Value: -5.238077163696289\n",
      "Epoch: 775 Loss Value: -5.392815589904785\n",
      "Epoch: 776 Loss Value: -5.291640758514404\n",
      "Epoch: 777 Loss Value: -5.24321985244751\n",
      "Epoch: 778 Loss Value: -5.239980697631836\n",
      "Epoch: 779 Loss Value: -5.23331356048584\n",
      "Epoch: 780 Loss Value: -5.290294647216797\n",
      "Epoch: 781 Loss Value: -5.428529739379883\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 782 Loss Value: -5.256384372711182\n",
      "Epoch: 783 Loss Value: -5.24997615814209\n",
      "Epoch: 784 Loss Value: -5.238073825836182\n",
      "Epoch: 785 Loss Value: -5.392818450927734\n",
      "Epoch: 786 Loss Value: -5.291641712188721\n",
      "Epoch: 787 Loss Value: -5.243220806121826\n",
      "Epoch: 788 Loss Value: -5.2399821281433105\n",
      "Epoch: 789 Loss Value: -5.233314514160156\n",
      "Epoch: 790 Loss Value: -5.290297031402588\n",
      "Epoch: 791 Loss Value: -5.428532600402832\n",
      "Epoch: 792 Loss Value: -5.2563862800598145\n",
      "Epoch: 793 Loss Value: -5.249978065490723\n",
      "Epoch: 794 Loss Value: -5.238080024719238\n",
      "Epoch: 795 Loss Value: -5.392819881439209\n",
      "Epoch: 796 Loss Value: -5.2916436195373535\n",
      "Epoch: 797 Loss Value: -5.243221282958984\n",
      "Epoch: 798 Loss Value: -5.239983081817627\n",
      "Epoch: 799 Loss Value: -5.2333149909973145\n",
      "Epoch: 800 Loss Value: -5.290297031402588\n",
      "Epoch: 801 Loss Value: -5.428534507751465\n",
      "Epoch: 802 Loss Value: -5.256387710571289\n",
      "Epoch: 803 Loss Value: -5.249978065490723\n",
      "Epoch: 804 Loss Value: -5.2380805015563965\n",
      "Epoch: 805 Loss Value: -5.392822742462158\n",
      "Epoch: 806 Loss Value: -5.291645526885986\n",
      "Epoch: 807 Loss Value: -5.243223667144775\n",
      "Epoch: 808 Loss Value: -5.239984512329102\n",
      "Epoch: 809 Loss Value: -5.233316421508789\n",
      "Epoch: 810 Loss Value: -5.290299892425537\n",
      "Epoch: 811 Loss Value: -5.428537368774414\n",
      "Epoch: 812 Loss Value: -5.256389141082764\n",
      "Epoch: 813 Loss Value: -5.2499799728393555\n",
      "Epoch: 814 Loss Value: -5.238081932067871\n",
      "Epoch: 815 Loss Value: -5.392824649810791\n",
      "Epoch: 816 Loss Value: -5.291646480560303\n",
      "Epoch: 817 Loss Value: -5.24322509765625\n",
      "Epoch: 818 Loss Value: -5.239984512329102\n",
      "Epoch: 819 Loss Value: -5.233316898345947\n",
      "Epoch: 820 Loss Value: -5.290300369262695\n",
      "Epoch: 821 Loss Value: -5.428539752960205\n",
      "Epoch: 822 Loss Value: -5.2563910484313965\n",
      "Epoch: 823 Loss Value: -5.249981880187988\n",
      "Epoch: 824 Loss Value: -5.2380828857421875\n",
      "Epoch: 825 Loss Value: -5.392826557159424\n",
      "Epoch: 826 Loss Value: -5.291647911071777\n",
      "Epoch: 827 Loss Value: -5.243225574493408\n",
      "Epoch: 828 Loss Value: -5.239986419677734\n",
      "Epoch: 829 Loss Value: -5.23331880569458\n",
      "Epoch: 830 Loss Value: -5.290302753448486\n",
      "Epoch: 831 Loss Value: -5.4285407066345215\n",
      "Epoch: 832 Loss Value: -5.256392478942871\n",
      "Epoch: 833 Loss Value: -5.249982833862305\n",
      "Epoch: 834 Loss Value: -5.238083839416504\n",
      "Epoch: 835 Loss Value: -5.392828941345215\n",
      "Epoch: 836 Loss Value: -5.291648864746094\n",
      "Epoch: 837 Loss Value: -5.243227005004883\n",
      "Epoch: 838 Loss Value: -5.239987373352051\n",
      "Epoch: 839 Loss Value: -5.2333197593688965\n",
      "Epoch: 840 Loss Value: -5.2903032302856445\n",
      "Epoch: 841 Loss Value: -5.4285430908203125\n",
      "Epoch: 842 Loss Value: -5.256392955780029\n",
      "Epoch: 843 Loss Value: -5.249983310699463\n",
      "Epoch: 844 Loss Value: -5.238083362579346\n",
      "Epoch: 845 Loss Value: -5.392829895019531\n",
      "Epoch: 846 Loss Value: -5.291649341583252\n",
      "Epoch: 847 Loss Value: -5.243227481842041\n",
      "Epoch: 848 Loss Value: -5.239987373352051\n",
      "Epoch: 849 Loss Value: -5.233320713043213\n",
      "Epoch: 850 Loss Value: -5.2903032302856445\n",
      "Epoch: 851 Loss Value: -5.428544044494629\n",
      "Epoch: 852 Loss Value: -5.2563934326171875\n",
      "Epoch: 853 Loss Value: -5.249984264373779\n",
      "Epoch: 854 Loss Value: -5.2380852699279785\n",
      "Epoch: 855 Loss Value: -5.392831325531006\n",
      "Epoch: 856 Loss Value: -5.291651248931885\n",
      "Epoch: 857 Loss Value: -5.243228435516357\n",
      "Epoch: 858 Loss Value: -5.239988327026367\n",
      "Epoch: 859 Loss Value: -5.233321189880371\n",
      "Epoch: 860 Loss Value: -5.2903056144714355\n",
      "Epoch: 861 Loss Value: -5.428545951843262\n",
      "Epoch: 862 Loss Value: -5.256394386291504\n",
      "Epoch: 863 Loss Value: -5.249985694885254\n",
      "Epoch: 864 Loss Value: -5.2380852699279785\n",
      "Epoch: 865 Loss Value: -5.3928327560424805\n",
      "Epoch: 866 Loss Value: -5.291651725769043\n",
      "Epoch: 867 Loss Value: -5.243228912353516\n",
      "Epoch: 868 Loss Value: -5.239988803863525\n",
      "Epoch: 869 Loss Value: -5.2333221435546875\n",
      "Epoch: 870 Loss Value: -5.290305137634277\n",
      "Epoch: 871 Loss Value: -5.4285478591918945\n",
      "Epoch: 872 Loss Value: -5.25639533996582\n",
      "Epoch: 873 Loss Value: -5.24998664855957\n",
      "Epoch: 874 Loss Value: -5.238086223602295\n",
      "Epoch: 875 Loss Value: -5.392834186553955\n",
      "Epoch: 876 Loss Value: -5.291652202606201\n",
      "Epoch: 877 Loss Value: -5.24323034286499\n",
      "Epoch: 878 Loss Value: -5.239989757537842\n",
      "Epoch: 879 Loss Value: -5.233322620391846\n",
      "Epoch: 880 Loss Value: -5.290307521820068\n",
      "Epoch: 881 Loss Value: -5.428548812866211\n",
      "Epoch: 882 Loss Value: -5.256396770477295\n",
      "Epoch: 883 Loss Value: -5.2499871253967285\n",
      "Epoch: 884 Loss Value: -5.238086700439453\n",
      "Epoch: 885 Loss Value: -5.392836093902588\n",
      "Epoch: 886 Loss Value: -5.291653156280518\n",
      "Epoch: 887 Loss Value: -5.243231773376465\n",
      "Epoch: 888 Loss Value: -5.239990234375\n",
      "Epoch: 889 Loss Value: -5.233323097229004\n",
      "Epoch: 890 Loss Value: -5.290309429168701\n",
      "Epoch: 891 Loss Value: -5.428549766540527\n",
      "Epoch: 892 Loss Value: -5.256397724151611\n",
      "Epoch: 893 Loss Value: -5.249988079071045\n",
      "Epoch: 894 Loss Value: -5.238087177276611\n",
      "Epoch: 895 Loss Value: -5.3928375244140625\n",
      "Epoch: 896 Loss Value: -5.291654586791992\n",
      "Epoch: 897 Loss Value: -5.243232727050781\n",
      "Epoch: 898 Loss Value: -5.239991664886475\n",
      "Epoch: 899 Loss Value: -5.233323097229004\n",
      "Epoch: 900 Loss Value: -5.290309429168701\n",
      "Epoch: 901 Loss Value: -5.428552150726318\n",
      "Epoch: 902 Loss Value: -5.256397724151611\n",
      "Epoch: 903 Loss Value: -5.249989032745361\n",
      "Epoch: 904 Loss Value: -5.238009929656982\n",
      "Epoch: 905 Loss Value: -5.392838478088379\n",
      "Epoch: 906 Loss Value: -5.291653156280518\n",
      "Epoch: 907 Loss Value: -5.243232727050781\n",
      "Epoch: 908 Loss Value: -5.239990711212158\n",
      "Epoch: 909 Loss Value: -5.233323097229004\n",
      "Epoch: 910 Loss Value: -5.290307998657227\n",
      "Epoch: 911 Loss Value: -5.428549766540527\n",
      "Epoch: 912 Loss Value: -5.256396770477295\n",
      "Epoch: 913 Loss Value: -5.249987602233887\n",
      "Epoch: 914 Loss Value: -5.238082408905029\n",
      "Epoch: 915 Loss Value: -5.392834663391113\n",
      "Epoch: 916 Loss Value: -5.249989032745361\n",
      "Epoch: 917 Loss Value: -5.243231773376465\n",
      "Epoch: 918 Loss Value: -5.239988803863525\n",
      "Epoch: 919 Loss Value: -5.233323097229004\n",
      "Epoch: 920 Loss Value: -5.290308952331543\n",
      "Epoch: 921 Loss Value: -5.428551197052002\n",
      "Epoch: 922 Loss Value: -5.256397724151611\n",
      "Epoch: 923 Loss Value: -5.249969959259033\n",
      "Epoch: 924 Loss Value: -5.238085746765137\n",
      "Epoch: 925 Loss Value: -5.392838478088379\n",
      "Epoch: 926 Loss Value: -5.291646957397461\n",
      "Epoch: 927 Loss Value: -5.243232727050781\n",
      "Epoch: 928 Loss Value: -5.239992141723633\n",
      "Epoch: 929 Loss Value: -5.2333245277404785\n",
      "Epoch: 930 Loss Value: -5.290309906005859\n",
      "Epoch: 931 Loss Value: -5.42855167388916\n",
      "Epoch: 932 Loss Value: -5.2563982009887695\n",
      "Epoch: 933 Loss Value: -5.249988079071045\n",
      "Epoch: 934 Loss Value: -5.238089561462402\n",
      "Epoch: 935 Loss Value: -5.392836570739746\n",
      "Epoch: 936 Loss Value: -5.249994277954102\n",
      "Epoch: 937 Loss Value: -5.243232727050781\n",
      "Epoch: 938 Loss Value: -5.239992618560791\n",
      "Epoch: 939 Loss Value: -5.233325004577637\n",
      "Epoch: 940 Loss Value: -5.290310859680176\n",
      "Epoch: 941 Loss Value: -5.428555011749268\n",
      "Epoch: 942 Loss Value: -5.256400108337402\n",
      "Epoch: 943 Loss Value: -5.249990463256836\n",
      "Epoch: 944 Loss Value: -5.238089561462402\n",
      "Epoch: 945 Loss Value: -5.392842769622803\n",
      "Epoch: 946 Loss Value: -5.2916579246521\n",
      "Epoch: 947 Loss Value: -5.243234157562256\n",
      "Epoch: 948 Loss Value: -5.239993095397949\n",
      "Epoch: 949 Loss Value: -5.233326435089111\n",
      "Epoch: 950 Loss Value: -5.290313243865967\n",
      "Epoch: 951 Loss Value: -5.428557395935059\n",
      "Epoch: 952 Loss Value: -5.256401538848877\n",
      "Epoch: 953 Loss Value: -5.249992370605469\n",
      "Epoch: 954 Loss Value: -5.238089561462402\n",
      "Epoch: 955 Loss Value: -5.392842769622803\n",
      "Epoch: 956 Loss Value: -5.291656970977783\n",
      "Epoch: 957 Loss Value: -5.2432355880737305\n",
      "Epoch: 958 Loss Value: -5.239993095397949\n",
      "Epoch: 959 Loss Value: -5.233327388763428\n",
      "Epoch: 960 Loss Value: -5.290313720703125\n",
      "Epoch: 961 Loss Value: -5.428557395935059\n",
      "Epoch: 962 Loss Value: -5.256402015686035\n",
      "Epoch: 963 Loss Value: -5.249992370605469\n",
      "Epoch: 964 Loss Value: -5.2380900382995605\n",
      "Epoch: 965 Loss Value: -5.3928446769714355\n",
      "Epoch: 966 Loss Value: -5.291658401489258\n",
      "Epoch: 967 Loss Value: -5.243236064910889\n",
      "Epoch: 968 Loss Value: -5.239994049072266\n",
      "Epoch: 969 Loss Value: -5.233327388763428\n",
      "Epoch: 970 Loss Value: -5.290314197540283\n",
      "Epoch: 971 Loss Value: -5.428559303283691\n",
      "Epoch: 972 Loss Value: -5.256402969360352\n",
      "Epoch: 973 Loss Value: -5.249993324279785\n",
      "Epoch: 974 Loss Value: -5.238090991973877\n",
      "Epoch: 975 Loss Value: -5.392845153808594\n",
      "Epoch: 976 Loss Value: -5.291658878326416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 977 Loss Value: -5.243237018585205\n",
      "Epoch: 978 Loss Value: -5.239994525909424\n",
      "Epoch: 979 Loss Value: -5.233327865600586\n",
      "Epoch: 980 Loss Value: -5.290315628051758\n",
      "Epoch: 981 Loss Value: -5.428560256958008\n",
      "Epoch: 982 Loss Value: -5.25640344619751\n",
      "Epoch: 983 Loss Value: -5.249993801116943\n",
      "Epoch: 984 Loss Value: -5.238090991973877\n",
      "Epoch: 985 Loss Value: -5.39284610748291\n",
      "Epoch: 986 Loss Value: -5.249997615814209\n",
      "Epoch: 987 Loss Value: -5.243237495422363\n",
      "Epoch: 988 Loss Value: -5.239995002746582\n",
      "Epoch: 989 Loss Value: -5.233328342437744\n",
      "Epoch: 990 Loss Value: -5.290315628051758\n",
      "Epoch: 991 Loss Value: -5.428560733795166\n",
      "Epoch: 992 Loss Value: -5.256403923034668\n",
      "Epoch: 993 Loss Value: -5.249994277954102\n",
      "Epoch: 994 Loss Value: -5.238091468811035\n",
      "Epoch: 995 Loss Value: -5.392846584320068\n",
      "Epoch: 996 Loss Value: -5.291660785675049\n",
      "Epoch: 997 Loss Value: -5.2432379722595215\n",
      "Epoch: 998 Loss Value: -5.239995956420898\n",
      "Epoch: 999 Loss Value: -5.233328819274902\n"
     ]
    }
   ],
   "source": [
    "print(\"Initialising and creating models....\")\n",
    "t1=time.time()\n",
    "# criterion=LabelSmoothing()\n",
    "model=make_model(voc.num_words,voc.num_words)\n",
    "model_opt=torch.optim.Adam(model.parameters(),lr=0.0001,betas=(0.9,0.988),eps=1e-9)\n",
    "print(\"=\"*100)\n",
    "print(\"Creating Models took: \"+str(time.time()-t1))\n",
    "\n",
    "\n",
    "\n",
    "model.train()\n",
    "for epoch in range(1000):\n",
    "    \n",
    "    current_batch=batches[epoch%10]\n",
    "    loss_val=run_epoch(current_batch,model,LossCompute(model,model_opt))\n",
    "    print(\"Epoch: \"+str(epoch)+\" Loss Value: \"+str(loss_val))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    memory = model.encode(src, src_mask)\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type_as(src.data)\n",
    "    \n",
    "    for i in range(max_len-1):\n",
    "        out = model.decode(memory, src_mask, \n",
    "                           Variable(ys), \n",
    "                           Variable(subsequent_mask(ys.size(1))\n",
    "                                    .type_as(src.data)))\n",
    "#         print(out)\n",
    "        prob = model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim = 1)\n",
    "        next_word = next_word.data[0]\n",
    "        ys = torch.cat([ys, \n",
    "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1)\n",
    "    return ys\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderDecoder(\n",
       "  (encoder): Encoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): EncoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): EncoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): EncoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): EncoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): EncoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): EncoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm()\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): DecoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (src_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): DecoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (src_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): DecoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (src_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): DecoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (src_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): DecoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (src_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): DecoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (src_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm()\n",
       "  )\n",
       "  (src_embed): Sequential(\n",
       "    (0): Embeddings(\n",
       "      (lut): Embedding(7816, 512)\n",
       "    )\n",
       "    (1): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (tgt_embed): Sequential(\n",
       "    (0): Embeddings(\n",
       "      (lut): Embedding(7816, 512)\n",
       "    )\n",
       "    (1): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (generator): Generator(\n",
       "    (proj): Linear(in_features=512, out_features=7816, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,inp,y,z,output=batch2TrainData(voc,[pairs[25]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  1,  42,  61, 116, 117, 118,  40, 119,   4,   2,   0,   0]])\n",
      "tensor([[[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          False, False]]])\n"
     ]
    }
   ],
   "source": [
    "print(inp)\n",
    "inp_mask=(inp!=0).squeeze(-2)\n",
    "inp_mask=inp_mask.view(-1,1,12)\n",
    "print(inp_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.6587, -1.1393,  1.1520,  0.7710,  1.2487, -0.9607, -1.4645,\n",
      "          -0.5490, -0.8521,  1.4028,  0.9560,  1.0376, -1.5187, -1.3949,\n",
      "          -0.9483,  1.2080, -1.0340, -1.2880,  0.4887,  1.3494, -1.2419,\n",
      "           1.3007,  0.7861, -0.8768,  0.4941,  1.4953,  1.1497,  1.3361,\n",
      "          -1.2833,  1.3443, -0.4097,  1.4348, -0.9108,  0.8396, -1.4078,\n",
      "           1.2781,  1.0088,  0.5646, -0.5608,  1.0984,  1.2733, -0.6329,\n",
      "          -1.4326, -1.3093, -0.6450, -0.7692,  0.7827, -0.3980, -0.6728,\n",
      "          -0.9395,  1.0451, -0.5838, -1.2449, -0.2876, -0.5841,  1.6100,\n",
      "           1.4215, -1.3143,  0.9311, -1.3905, -1.0593,  0.9174, -1.5327,\n",
      "           1.3168, -0.5711,  0.7740,  0.5616, -0.4508,  0.6838,  0.4481,\n",
      "           0.6446, -1.4336,  1.2074,  1.2578,  0.8648, -1.0734,  1.4286,\n",
      "          -1.1363, -1.1198, -1.0466,  0.9114,  0.7474, -1.4010,  0.7102,\n",
      "          -0.7590,  0.5141,  1.4681,  0.6093, -1.1598, -0.8060, -1.3878,\n",
      "           0.6433, -0.6490, -0.5237, -1.3767,  0.9100, -1.2583, -0.5660,\n",
      "           0.6993,  0.7170, -0.8011,  0.9388,  1.3011, -1.4999,  1.3035,\n",
      "          -0.5764, -1.4030,  1.1360, -1.4887, -0.3689,  1.3538, -1.0178,\n",
      "           1.1510, -0.9607,  0.7673, -0.9398,  0.5917,  1.0311,  0.9055,\n",
      "           0.6956,  0.8428,  0.6895,  1.3078, -0.4180, -0.8680, -1.1608,\n",
      "          -1.3197,  0.5708, -1.0273,  0.7416,  0.9753,  0.8695, -1.3021,\n",
      "           0.5670, -1.4476, -0.4402, -1.0345,  1.2093,  0.8543, -0.5175,\n",
      "          -1.6096, -0.8766,  1.2642,  1.1354,  1.4106, -0.2828,  0.6109,\n",
      "          -1.5516,  1.2010, -0.7382,  1.2470, -1.2699,  1.4346, -0.6560,\n",
      "           1.4717, -0.9140, -1.3480,  0.8261,  1.3959, -0.8347, -0.5666,\n",
      "          -0.7964, -0.8893,  0.4363,  1.3103,  0.4631,  0.7919, -1.0218,\n",
      "          -0.6895,  0.7843, -1.2066,  1.1949,  1.1234, -0.6263,  0.6086,\n",
      "          -1.0177,  0.8406,  0.1937, -1.0391,  0.5925, -0.8342,  0.7356,\n",
      "           1.3030, -0.7993, -1.1352,  1.3578,  0.6541, -0.8972,  0.4027,\n",
      "          -0.2294,  0.7210, -1.4453, -0.3736, -0.6394, -1.2808, -0.6083,\n",
      "          -1.0162,  0.5539, -0.7645, -0.7921, -1.3830,  1.3477,  0.6399,\n",
      "           0.9715,  1.3329, -0.9394,  1.3328,  0.9118,  1.3075,  0.2829,\n",
      "           1.1045, -0.9939,  0.7388,  0.8251,  0.6884,  1.0873,  0.8988,\n",
      "          -0.0740,  1.3552,  0.8520, -1.2147,  1.3394,  1.1101,  0.6236,\n",
      "           0.6021,  1.1919,  0.8839, -0.5350,  0.8379,  1.3420,  0.2593,\n",
      "          -0.8592, -0.7188,  0.9367, -0.6380, -0.8625, -1.2083, -1.0609,\n",
      "          -0.8716, -0.5939,  0.5308, -1.0083, -1.0792, -1.1035, -1.0197,\n",
      "           0.7731, -1.3192, -1.4319, -0.4591,  1.1639,  0.6274,  1.2778,\n",
      "          -0.5075,  0.6160,  1.4063, -0.7291,  1.3718, -0.5343,  0.5522,\n",
      "          -1.2931, -0.5567, -0.9400,  0.9851,  0.0853,  1.3642, -0.6117,\n",
      "           0.6650, -0.6619,  1.0438, -0.9924, -0.4465, -1.4277, -0.5966,\n",
      "          -0.9725, -0.2804,  1.3111, -0.6997,  0.3773, -0.4867,  1.3551,\n",
      "           1.2796,  1.2006, -1.2490,  0.8796,  1.3045,  0.1447, -1.2030,\n",
      "          -0.6654,  0.2782,  0.6222, -1.2798,  0.4535, -1.0648, -1.1955,\n",
      "           0.6115,  1.2420, -0.9393, -0.7812, -1.0907,  1.2047, -0.6952,\n",
      "           0.5801,  1.1535,  1.3251, -0.7495, -1.4178, -1.4716,  1.4032,\n",
      "           0.8279, -1.1606,  0.3868,  0.7572,  0.8910,  0.2047,  0.2532,\n",
      "           0.4837,  1.3915, -0.6591,  1.0444,  0.3248,  0.8763,  0.8148,\n",
      "          -1.4217, -0.3725, -0.7817, -1.4299, -0.5813,  0.9969,  1.2640,\n",
      "           1.2873,  1.1410, -0.6299, -0.8669, -1.0290, -0.6858, -1.0472,\n",
      "          -0.8166,  1.2020, -1.3337,  0.7270, -0.7452, -1.5226,  0.4566,\n",
      "           1.2139, -0.9530, -1.5363, -1.3282, -1.1802,  1.1013,  0.0268,\n",
      "          -0.2863,  0.6155,  0.7212,  0.8143, -0.8884,  1.2754,  1.3574,\n",
      "          -0.8713, -1.0148,  1.0436, -1.4913,  1.4250, -0.4085, -1.2038,\n",
      "          -0.5326, -0.6627, -1.0018, -0.6500,  1.5553, -0.4081, -0.3542,\n",
      "          -0.6222, -0.9349,  1.2499,  0.9530, -0.3355,  1.2160, -1.4154,\n",
      "           1.0111, -1.4917, -0.7551, -1.3193,  0.7902, -1.3103, -1.1084,\n",
      "          -0.5632, -1.4078,  1.2292,  0.8141,  0.9920,  0.5972, -0.7229,\n",
      "           1.3687,  0.7123,  0.5431, -0.8212,  0.5591, -1.3743,  0.3948,\n",
      "          -1.1811, -0.5947,  0.5225,  0.5078,  0.8951, -1.4324,  0.9378,\n",
      "           0.7741, -0.6708, -0.4469,  1.0499,  0.7702,  1.3732, -1.3069,\n",
      "           1.2346,  0.9011, -1.2344, -0.8062, -0.6436,  1.3132, -0.6301,\n",
      "          -0.8995, -1.4138, -1.3919,  0.7072,  1.3158, -1.0844,  1.3893,\n",
      "           1.2655,  0.3107,  0.6929,  1.2406, -1.3707,  1.3709, -1.2864,\n",
      "           1.3452,  1.1344, -0.6816,  1.1963,  0.6207, -1.2665,  1.4372,\n",
      "          -0.7054, -0.3603, -0.7894,  1.1661,  1.2052, -0.9949, -0.9025,\n",
      "           1.2674, -0.9897, -0.9543,  1.2376,  1.1831,  0.5920,  0.9923,\n",
      "          -1.4815,  1.1809, -1.5225,  0.7238, -1.1394,  1.3335, -0.6046,\n",
      "          -0.8313,  0.8947,  0.8210, -0.9511, -1.3235, -1.3679, -0.4203,\n",
      "          -1.1769, -1.4471, -1.4054, -0.7388,  1.0664, -1.1202,  1.4606,\n",
      "           0.6311,  0.7006,  0.7653, -1.0679, -0.6190,  1.2777, -0.9934,\n",
      "          -1.2007,  0.9735,  1.2868,  0.5840, -1.5497, -1.1884,  0.7529,\n",
      "           1.2232, -0.9516,  0.8168,  0.6044, -0.4221,  0.9792, -0.7024,\n",
      "          -1.1422,  0.5446,  0.7786,  1.1953, -1.4777,  0.6350,  0.8046,\n",
      "           1.1702, -0.7158,  0.8441, -1.4335, -0.7632, -1.4903, -0.8258,\n",
      "          -1.2163]]], grad_fn=<AddBackward0>)\n",
      "tensor([[[-0.6587, -1.1393,  1.1520,  ..., -1.4903, -0.8258, -1.2163],\n",
      "         [-0.6134, -1.1855,  1.1761,  ..., -1.4589, -0.8031, -1.1927]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([[[-0.6587, -1.1393,  1.1520,  ..., -1.4903, -0.8258, -1.2163],\n",
      "         [-0.6134, -1.1855,  1.1761,  ..., -1.4589, -0.8031, -1.1927],\n",
      "         [-0.5995, -1.2467,  1.1841,  ..., -1.4521, -0.8035, -1.1941]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([[[-0.6587, -1.1393,  1.1520,  ..., -1.4903, -0.8258, -1.2163],\n",
      "         [-0.6134, -1.1855,  1.1761,  ..., -1.4589, -0.8031, -1.1927],\n",
      "         [-0.5995, -1.2467,  1.1841,  ..., -1.4521, -0.8035, -1.1941],\n",
      "         [-0.6299, -1.2801,  1.1548,  ..., -1.4434, -0.7968, -1.2014]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([[[-0.6587, -1.1393,  1.1520,  ..., -1.4903, -0.8258, -1.2163],\n",
      "         [-0.6134, -1.1855,  1.1761,  ..., -1.4589, -0.8031, -1.1927],\n",
      "         [-0.5995, -1.2467,  1.1841,  ..., -1.4521, -0.8035, -1.1941],\n",
      "         [-0.6299, -1.2801,  1.1548,  ..., -1.4434, -0.7968, -1.2014],\n",
      "         [-0.6699, -1.2643,  1.1150,  ..., -1.4387, -0.7879, -1.2062]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([[[-0.6587, -1.1393,  1.1520,  ..., -1.4903, -0.8258, -1.2163],\n",
      "         [-0.6134, -1.1855,  1.1761,  ..., -1.4589, -0.8031, -1.1927],\n",
      "         [-0.5995, -1.2467,  1.1841,  ..., -1.4521, -0.8035, -1.1941],\n",
      "         [-0.6299, -1.2801,  1.1548,  ..., -1.4434, -0.7968, -1.2014],\n",
      "         [-0.6699, -1.2643,  1.1150,  ..., -1.4387, -0.7879, -1.2062],\n",
      "         [-0.6783, -1.2156,  1.1013,  ..., -1.4390, -0.7808, -1.2084]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([[[-0.6587, -1.1393,  1.1520,  ..., -1.4903, -0.8258, -1.2163],\n",
      "         [-0.6134, -1.1855,  1.1761,  ..., -1.4589, -0.8031, -1.1927],\n",
      "         [-0.5995, -1.2467,  1.1841,  ..., -1.4521, -0.8035, -1.1941],\n",
      "         ...,\n",
      "         [-0.6699, -1.2643,  1.1150,  ..., -1.4387, -0.7879, -1.2062],\n",
      "         [-0.6783, -1.2156,  1.1013,  ..., -1.4390, -0.7808, -1.2084],\n",
      "         [-0.6472, -1.1790,  1.1289,  ..., -1.4410, -0.7791, -1.2080]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([[[-0.6587, -1.1393,  1.1520,  ..., -1.4903, -0.8258, -1.2163],\n",
      "         [-0.6134, -1.1855,  1.1761,  ..., -1.4589, -0.8031, -1.1927],\n",
      "         [-0.5995, -1.2467,  1.1841,  ..., -1.4521, -0.8035, -1.1941],\n",
      "         ...,\n",
      "         [-0.6783, -1.2156,  1.1013,  ..., -1.4390, -0.7808, -1.2084],\n",
      "         [-0.6472, -1.1790,  1.1289,  ..., -1.4410, -0.7791, -1.2080],\n",
      "         [-0.6048, -1.1857,  1.1753,  ..., -1.4417, -0.7819, -1.2057]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([[[-0.6587, -1.1393,  1.1520,  ..., -1.4903, -0.8258, -1.2163],\n",
      "         [-0.6134, -1.1855,  1.1761,  ..., -1.4589, -0.8031, -1.1927],\n",
      "         [-0.5995, -1.2467,  1.1841,  ..., -1.4521, -0.8035, -1.1941],\n",
      "         ...,\n",
      "         [-0.6472, -1.1790,  1.1289,  ..., -1.4410, -0.7791, -1.2080],\n",
      "         [-0.6048, -1.1857,  1.1753,  ..., -1.4417, -0.7819, -1.2057],\n",
      "         [-0.5912, -1.2271,  1.1984,  ..., -1.4394, -0.7837, -1.2041]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([[[-0.6587, -1.1393,  1.1520,  ..., -1.4903, -0.8258, -1.2163],\n",
      "         [-0.6134, -1.1855,  1.1761,  ..., -1.4589, -0.8031, -1.1927],\n",
      "         [-0.5995, -1.2467,  1.1841,  ..., -1.4521, -0.8035, -1.1941],\n",
      "         ...,\n",
      "         [-0.6048, -1.1857,  1.1753,  ..., -1.4417, -0.7819, -1.2057],\n",
      "         [-0.5912, -1.2271,  1.1984,  ..., -1.4394, -0.7837, -1.2041],\n",
      "         [-0.6216, -1.2622,  1.1765,  ..., -1.4370, -0.7856, -1.2053]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([[[-0.6587, -1.1393,  1.1520,  ..., -1.4903, -0.8258, -1.2163],\n",
      "         [-0.6134, -1.1855,  1.1761,  ..., -1.4589, -0.8031, -1.1927],\n",
      "         [-0.5995, -1.2467,  1.1841,  ..., -1.4521, -0.8035, -1.1941],\n",
      "         ...,\n",
      "         [-0.5912, -1.2271,  1.1984,  ..., -1.4394, -0.7837, -1.2041],\n",
      "         [-0.6216, -1.2622,  1.1765,  ..., -1.4370, -0.7856, -1.2053],\n",
      "         [-0.6704, -1.2568,  1.1260,  ..., -1.4355, -0.7881, -1.2098]]],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z=greedy_decode(model,inp,inp_mask,12,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]])\n"
     ]
    }
   ],
   "source": [
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2])\n"
     ]
    }
   ],
   "source": [
    "h=torch.tensor([1,2,3])\n",
    "print(h[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12])\n"
     ]
    }
   ],
   "source": [
    "print(batch.src[0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 4, 2, 4, 2, 4, 2, 4, 2, 2, 2]]) tensor([  1,   5, 115, 101,   6,   2,   0,   0,   0,   0,   0])\n",
      "tensor([[1, 2, 4, 2, 4, 2, 4, 2, 4, 2, 2, 2]]) tensor([  1, 857, 111,   4,  25, 118,  40, 359,   4,   2,   0])\n",
      "tensor([[1, 2, 4, 2, 4, 2, 4, 2, 4, 2, 2, 2]]) tensor([   1,  147,   37,   70, 1610,    6,    2,    0,    0,    0,    0])\n",
      "tensor([[1, 2, 4, 2, 4, 2, 4, 2, 4, 2, 2, 2]]) tensor([   1, 1153,   50,  179,    7, 1244,    4,    2,    0,    0,    0])\n",
      "tensor([[1, 2, 4, 2, 4, 2, 4, 2, 4, 2, 2, 2]]) tensor([   1, 2598, 1502,    4,    2,    0,    0,    0,    0,    0,    0])\n",
      "tensor([[1, 2, 4, 2, 4, 2, 4, 2, 4, 2, 2, 2]]) tensor([ 1, 34,  4,  2,  0,  0,  0,  0,  0,  0,  0])\n",
      "tensor([[1, 2, 4, 2, 4, 2, 4, 2, 4, 2, 2, 2]]) tensor([   1,  101, 2152,   21,    4,    2,    0,    0,    0,    0,    0])\n",
      "tensor([[1, 2, 4, 2, 4, 2, 4, 2, 4, 2, 2, 2]]) tensor([  1,  59,  83, 359,   4,   2,   0,   0,   0,   0,   0])\n",
      "tensor([[1, 2, 4, 2, 4, 2, 4, 2, 4, 2, 2, 2]]) tensor([ 1, 64,  6,  2,  0,  0,  0,  0,  0,  0,  0])\n",
      "tensor([[1, 2, 4, 2, 4, 2, 4, 2, 4, 2, 2, 2]]) tensor([  1,  64,  65, 215,   4,   2,   0,   0,   0,   0,   0])\n",
      "tensor([[1, 2, 4, 2, 4, 2, 4, 2, 4, 2, 2, 2]]) tensor([   1,  124, 1607,   36,   37,   64, 2286,    4,    2,    0,    0])\n",
      "tensor([[1, 2, 4, 2, 4, 2, 4, 2, 4, 2, 2, 2]]) tensor([   1,  303, 3163,    4,    7,  293,  117,  697,    4,    2,    0])\n",
      "tensor([[1, 2, 4, 2, 4, 2, 4, 2, 4, 2, 2, 2]]) tensor([  1, 167, 318,  16, 787,   4,   2,   0,   0,   0,   0])\n",
      "tensor([[1, 2, 4, 2, 4, 2, 4, 2, 4, 2, 2, 2]]) tensor([   1,   33,   45,  469, 2037, 5936,   83,   56,  319,    4,    2])\n",
      "tensor([[1, 2, 4, 2, 4, 2, 4, 2, 4, 2, 2, 2]]) tensor([  1,  60, 177, 518,  83,  66,   2,   0,   0,   0,   0])\n",
      "tensor([[1, 2, 4, 2, 4, 2, 4, 2, 4, 2, 2, 2]]) tensor([   1,   59,   76,  536,   98,   12, 1098,    4,    2,    0,    0])\n",
      "tensor([[1, 2, 4, 2, 4, 2, 4, 2, 4, 2, 2, 2]]) tensor([  1,  35,   7, 401,  21, 159,   4,   2,   0,   0,   0])\n",
      "tensor([[1, 2, 4, 2, 4, 2, 4, 2, 4, 2, 2, 2]]) tensor([  1,  60, 621,  56,  83,  53, 426,  56,   7,   4,   2])\n",
      "tensor([[1, 2, 4, 2, 4, 2, 4, 2, 4, 2, 2, 2]]) tensor([   1,  787, 4348,  147,   47,    7, 1026,    6,    2,    0,    0])\n",
      "tensor([[1, 2, 4, 2, 4, 2, 4, 2, 4, 2, 2, 2]]) tensor([   1, 1306,    4,    2,    0,    0,    0,    0,    0,    0,    0])\n",
      "tensor([[1, 2, 4, 2, 4, 2, 4, 2, 4, 2, 2, 2]]) tensor([  1, 100,  67,   6,   2,   0,   0,   0,   0,   0,   0])\n",
      "tensor([[1, 2, 4, 2, 4, 2, 4, 2, 4, 2, 2, 2]]) tensor([  1, 242, 637,  66,   2,   0,   0,   0,   0,   0,   0])\n",
      "tensor([[1, 2, 4, 2, 4, 2, 4, 2, 4, 2, 2, 2]]) tensor([ 1, 33,  6,  2,  0,  0,  0,  0,  0,  0,  0])\n",
      "tensor([[1, 2, 4, 2, 4, 2, 4, 2, 4, 2, 2, 2]]) tensor([   1, 1454,    2,    0,    0,    0,    0,    0,    0,    0,    0])\n",
      "tensor([[1, 2, 4, 2, 4, 2, 4, 2, 4, 2, 2, 2]]) tensor([   1,    3,   12, 1189,   96,   36, 3123,    6,    2,    0,    0])\n",
      "tensor([[1, 2, 4, 2, 4, 2, 4, 2, 4, 2, 2, 2]]) tensor([   1,   40,  154, 4467,    4,    2,    0,    0,    0,    0,    0])\n",
      "tensor([[1, 2, 4, 2, 4, 2, 4, 2, 4, 2, 2, 2]]) tensor([  1, 307,  45, 180,   4,   2,   0,   0,   0,   0,   0])\n",
      "tensor([[1, 2, 4, 2, 4, 2, 4, 2, 4, 2, 2, 2]]) tensor([   1, 1602,  177,    4, 1602,  177,    4,    2,    0,    0,    0])\n",
      "tensor([[1, 2, 4, 2, 4, 2, 4, 2, 4, 2, 2, 2]]) tensor([   1,    7,   14,   67,  123, 1312,    4,    2,    0,    0,    0])\n",
      "tensor([[1, 2, 4, 2, 4, 2, 4, 2, 4, 2, 2, 2]]) tensor([   1,   12, 1189,    4,    2,    0,    0,    0,    0,    0,    0])\n",
      "tensor([[1, 2, 4, 2, 4, 2, 4, 2, 4, 2, 2, 2]]) tensor([  1, 680,  66,   2,   0,   0,   0,   0,   0,   0,   0])\n",
      "tensor([[1, 2, 4, 2, 4, 2, 4, 2, 4, 2, 2, 2]]) tensor([   1,   51,  109, 1454,    4,    2,    0,    0,    0,    0,    0])\n",
      "tensor([[1, 2, 4, 2, 4, 2, 4, 2, 4, 2, 2, 2]]) tensor([  1,   7,  73, 518,  83,   6,   2,   0,   0,   0,   0])\n",
      "tensor([[1, 2, 4, 2, 4, 2, 4, 2, 4, 2, 2, 2]]) tensor([  1, 755,  96, 169,  83,   4,   2,   0,   0,   0,   0])\n",
      "tensor([[1, 2, 4, 2, 4, 2, 4, 2, 4, 2, 2, 2]]) tensor([ 1, 95, 27, 60,  2,  0,  0,  0,  0,  0,  0])\n",
      "tensor([[1, 2, 4, 2, 4, 2, 4, 2, 4, 2, 2, 2]]) tensor([   1,  272,    7,  290,   41, 7073,    4,    2,    0,    0,    0])\n",
      "tensor([[1, 2, 4, 2, 4, 2, 4, 2, 4, 2, 2, 2]]) tensor([   1,  124,    7,   89,   12,   79, 2007,   98, 4098,    4,    2])\n",
      "tensor([[1, 2, 4, 2, 4, 2, 4, 2, 4, 2, 2, 2]]) tensor([  1,  17,  18,   7,  14, 261,  12,  51, 609,   4,   2])\n",
      "tensor([[1, 2, 4, 2, 4, 2, 4, 2, 4, 2, 2, 2]]) tensor([   1,   77, 1138,   76,    6,    2,    0,    0,    0,    0,    0])\n",
      "tensor([[1, 2, 4, 2, 4, 2, 4, 2, 4, 2, 2, 2]]) tensor([  1,  25, 331, 117,   4,   2,   0,   0,   0,   0,   0])\n",
      "tensor([[1, 2, 4, 2, 4, 2, 4, 2, 4, 2, 2, 2]]) tensor([  1,  34, 170, 565,  66,   2,   0,   0,   0,   0,   0])\n",
      "tensor([[1, 2, 4, 2, 4, 2, 4, 2, 4, 2, 2, 2]]) tensor([  1, 147,   6,   2,   0,   0,   0,   0,   0,   0,   0])\n",
      "tensor([[1, 2, 4, 2, 4, 2, 4, 2, 4, 2, 2, 2]]) tensor([   1,  379,    7,  112,  561,   36, 3424,  479,  571,    6,    2])\n",
      "tensor([[1, 2, 4, 2, 4, 2, 4, 2, 4, 2, 2, 2]]) tensor([   1,  818, 2622,    4,    2,    0,    0,    0,    0,    0,    0])\n",
      "tensor([[1, 2, 4, 2, 4, 2, 4, 2, 4, 2, 2, 2]]) tensor([ 1, 50,  6,  2,  0,  0,  0,  0,  0,  0,  0])\n",
      "tensor([[1, 2, 4, 2, 4, 2, 4, 2, 4, 2, 2, 2]]) tensor([  1, 124, 242,  21,   4,   2,   0,   0,   0,   0,   0])\n",
      "tensor([[1, 2, 4, 2, 4, 2, 4, 2, 4, 2, 2, 2]]) tensor([  1, 401, 177,  66,   2,   0,   0,   0,   0,   0,   0])\n",
      "tensor([[1, 2, 4, 2, 4, 2, 4, 2, 4, 2, 2, 2]]) tensor([   1,  331,  117,   47,   50,  895, 5803,    6,    2,    0,    0])\n",
      "tensor([[1, 2, 4, 2, 4, 2, 4, 2, 4, 2, 2, 2]]) tensor([ 1, 50,  6,  2,  0,  0,  0,  0,  0,  0,  0])\n",
      "tensor([[1, 2, 4, 2, 4, 2, 4, 2, 4, 2, 2, 2]]) tensor([   1,   25,  148,    8,   44,   12,   79, 2382, 3329,    4,    2])\n"
     ]
    }
   ],
   "source": [
    "for batch in batches:\n",
    "    for i in range(5):\n",
    "        output=greedy_decode(model,batch.src[i].view(-1,12),batch.src_mask[i].view(1,-1,12),12,1)\n",
    "        print(str(output)+\" \"+str(batch.trg[i]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
