{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre-Processing\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "#Model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# For visualising metrics\n",
    "from visdom import Visdom\n",
    "\n",
    "# For visualising gradients plot\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "import copy\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The device found: cpu\n"
     ]
    }
   ],
   "source": [
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"The device found: \"+str(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisdomLinePlotter(object):\n",
    "    \"\"\"Plots to Visdom\"\"\"\n",
    "    \n",
    "    def __init__(self, env_name='main'):\n",
    "        self.viz = Visdom()\n",
    "        self.env = env_name\n",
    "        self.plots = {}\n",
    "    def plot(self, var_name, split_name, title_name, x, y):\n",
    "        if var_name not in self.plots:\n",
    "            self.plots[var_name] = self.viz.line(X=np.array([x,x]), Y=np.array([y,y]), env=self.env, opts=dict(\n",
    "                legend=[split_name],\n",
    "                title=title_name,\n",
    "                xlabel='Epochs',\n",
    "                ylabel=var_name\n",
    "            ))\n",
    "        else:\n",
    "            self.viz.line(X=np.array([x]), Y=np.array([y]), env=self.env, win=self.plots[var_name], name=split_name, update = 'append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_grad_flow(named_parameters):\n",
    "    \"\"\"\n",
    "        Plotting gradient flow across various layers\n",
    "        Thanks to: https://discuss.pytorch.org/t/check-gradient-flow-in-network/15063/2\n",
    "    \"\"\"   \n",
    "    ave_grads = []\n",
    "    layers = []\n",
    "    for n, p in named_parameters:\n",
    "        if(p.requires_grad) and (\"bias\" not in n):\n",
    "            layers.append(n)\n",
    "            ave_grads.append(p.grad.abs().mean())\n",
    "    plt.plot(ave_grads, alpha=0.3, color=\"b\")\n",
    "    plt.hlines(0, 0, len(ave_grads)+1, linewidth=1, color=\"k\" )\n",
    "    plt.xticks(range(0,len(ave_grads), 1), layers, rotation=\"vertical\")\n",
    "    plt.xlim(xmin=0, xmax=len(ave_grads))\n",
    "    plt.xlabel(\"Layers\")\n",
    "    plt.ylabel(\"average gradient\")\n",
    "    plt.title(\"Gradient flow\")\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The final data corpus folder: C:\\Users\\deepa\\Conversational Agents\\Datasets\\cornell movie-dialogs corpus\n"
     ]
    }
   ],
   "source": [
    "path='C:\\\\Users\\\\deepa\\\\Conversational Agents\\\\Datasets'\n",
    "dataset='cornell movie-dialogs corpus'\n",
    "\n",
    "data_folder=os.path.join(path,dataset)\n",
    "\n",
    "print(\"The final data corpus folder: \"+str(data_folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lines_conversations():\n",
    "    \"\"\"\n",
    "    Loads movie lines and conversations from the dataset.\n",
    "    \n",
    "    data_folder: Destination where conversations and lines are stored.\n",
    "    \n",
    "    movie_lines: Consist of movie lines as given by the dataset.\n",
    "    movie_conversations: Consist of movie conversations as given by the dataset.\n",
    "    \n",
    "    \"\"\"\n",
    "    f=open(os.path.join(data_folder,'movie_lines.txt'),'r')\n",
    "    movie_lines=f.read().splitlines()\n",
    "    f.close()\n",
    "    \n",
    "    f=open(os.path.join(data_folder,'movie_conversations.txt'),'r')\n",
    "    movie_conversations=f.read().splitlines()\n",
    "    f.close()\n",
    "    \n",
    "    return movie_lines,movie_conversations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting movie lines and movie conversations...\n",
      "Number of distinct lines: 304713\n",
      "Number of conversations: 83097\n",
      "Average Number of lines per conversations: 3.6669554857576085\n",
      "L1045 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ They do not!\n",
      "u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L194', 'L195', 'L196', 'L197']\n",
      "Extracting took place in: 0.2782161235809326\n"
     ]
    }
   ],
   "source": [
    "t1=time.time()\n",
    "print(\"Extracting movie lines and movie conversations...\")\n",
    "movie_lines,movie_conversations=get_lines_conversations()\n",
    "\n",
    "print(\"Number of distinct lines: \"+str(len(movie_lines)))\n",
    "print(\"Number of conversations: \"+str(len(movie_conversations)))\n",
    "print(\"Average Number of lines per conversations: \"+str(len(movie_lines)/len(movie_conversations)))\n",
    "\n",
    "print(movie_lines[0])\n",
    "print(movie_conversations[0])\n",
    "\n",
    "print(\"Extracting took place in: \"+str(time.time()-t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadLines(movie_lines,fields):\n",
    "    lines={}\n",
    "    for line in movie_lines:\n",
    "        values=line.split(\" +++$+++ \")\n",
    "        \n",
    "        lineVals={}\n",
    "        \n",
    "#         print(\"values\"+str(len(values)))\n",
    "#         print(\"fields\"+str(len(fields)))\n",
    "              \n",
    "        for i,field in enumerate(fields):\n",
    "            lineVals[field]=values[i]\n",
    "        \n",
    "        lines[lineVals['lineID']]=lineVals\n",
    "    \n",
    "    return lines\n",
    "\n",
    "def loadConversations(movie_conversations,lines,fields):\n",
    "    conversations=[]\n",
    "    \n",
    "    for convo in movie_conversations:\n",
    "        values=convo.split(\" +++$+++ \")\n",
    "        conVals={}\n",
    "       \n",
    "        for i,field in enumerate(fields):\n",
    "            conVals[field]=values[i]\n",
    "        \n",
    "        lineIDs=eval(conVals[\"utteranceIDs\"])\n",
    "        \n",
    "        conVals[\"lines\"]=[]\n",
    "        \n",
    "        for lineID in lineIDs:\n",
    "            conVals[\"lines\"].append(lines[lineID])\n",
    "        conversations.append(conVals)\n",
    "        \n",
    "    return conversations\n",
    "\n",
    "def sentencePairs(conversations):\n",
    "    qr_pairs=[]\n",
    "    \n",
    "    for conversation in conversations:\n",
    "        for i in range(len(conversation[\"lines\"])-1):\n",
    "            query=conversation[\"lines\"][i][\"text\"].strip()\n",
    "            response=conversation[\"lines\"][i+1][\"text\"].strip()\n",
    "            \n",
    "            if query and response:\n",
    "                qr_pairs.append([query,response])\n",
    "        \n",
    "    return qr_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Separating meaningfull information for our model...\n",
      "The number of query-response pairs are: 221282\n",
      "Separation took place in: 2.646280527114868\n"
     ]
    }
   ],
   "source": [
    "t1=time.time()\n",
    "print(\"Separating meaningfull information for our model...\")\n",
    "\n",
    "lines={}\n",
    "conversations=[]\n",
    "qr_pairs=[]\n",
    "\n",
    "movie_lines_fields=[\"lineID\",\"characterID\",\"movieID\",\"character\",\"text\"]\n",
    "movie_convo_fields=[\"charcaterID\",\"character2ID\",\"movieID\",\"utteranceIDs\"]\n",
    "\n",
    "lines=loadLines(movie_lines,movie_lines_fields)\n",
    "conversations=loadConversations(movie_conversations,lines,movie_convo_fields)\n",
    "qr_pairs=sentencePairs(conversations)\n",
    "\n",
    "print(\"The number of query-response pairs are: \"+str(len(qr_pairs)))\n",
    "print(\"Separation took place in: \"+str(time.time()-t1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_Token=0\n",
    "START_Token=1\n",
    "END_Token=2\n",
    "\n",
    "class Vocabulary:\n",
    "    def __init__(self):\n",
    "        self.trimmed=False\n",
    "        self.word2count={}\n",
    "        self.index2word={PAD_Token:\"PAD\",START_Token:\"SOS\",END_Token:\"EOS\"}\n",
    "        self.word2index={\"PAD\":PAD_Token,\"SOS\":START_Token,\"EOS\":END_Token}\n",
    "        self.num_words=3\n",
    "        \n",
    "    def addSentence(self,sentence):\n",
    "        for word in sentence.split(\" \"):\n",
    "            self.addWord(word)\n",
    "    def addWord(self,word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word]=self.num_words\n",
    "            self.index2word[self.num_words]=word\n",
    "            self.word2count[word]=1\n",
    "            self.num_words=self.num_words+1\n",
    "        else:\n",
    "            self.word2count[word]+=1\n",
    "            \n",
    "    def trim(self,min_count):\n",
    "        \n",
    "        if self.trimmed:\n",
    "            return\n",
    "        self.trimmed=True\n",
    "        \n",
    "        keep_words=[]\n",
    "        \n",
    "        for word,freq in self.word2count.items():\n",
    "            if freq>=min_count:\n",
    "                keep_words.append(word)\n",
    "        \n",
    "        self.word2count={}\n",
    "        self.index2word={PAD_Token:\"PAD\",START_Token:\"SOS\",END_Token:\"EOS\"}\n",
    "        self.word2index={\"PAD\":PAD_Token,\"SOS\":START_Token,\"EOS\":END_Token}\n",
    "        self.num_words=3\n",
    "        \n",
    "        for word in keep_words:\n",
    "            self.addWord(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataset and corresponding vocabulary...\n",
      "Preparation took place in: 9.411579608917236\n"
     ]
    }
   ],
   "source": [
    "Max_Length=10\n",
    "\n",
    "def normalizeString(s):\n",
    "    s=s.lower().strip()\n",
    "    s=re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s=re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    s=re.sub(r\"\\s+\", r\" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def readVocs(qr_pairs):\n",
    "    \n",
    "    for qr_pair in qr_pairs:\n",
    "        qr_pair[0]=normalizeString(qr_pair[0])\n",
    "        qr_pair[1]=normalizeString(qr_pair[1])\n",
    "    \n",
    "    voc=Vocabulary()\n",
    "    return voc,qr_pairs\n",
    "\n",
    "def filterPair(pair):\n",
    "    return len(pair[0].split(\" \"))<Max_Length and len(pair[1].split(\" \"))<Max_Length\n",
    "\n",
    "def filterPairs(qr_pairs):\n",
    "    return [pair for pair in qr_pairs if filterPair(pair)]\n",
    "\n",
    "def prepareDataset(qr_pairs):\n",
    "    voc, qr_pairs=readVocs(qr_pairs)\n",
    "    qr_pairs=filterPairs(qr_pairs)\n",
    "       \n",
    "    for pair in qr_pairs:\n",
    "        voc.addSentence(pair[0])\n",
    "        voc.addSentence(pair[1])\n",
    "#     print(\"Number\"+str(voc.num_words))\n",
    "    return voc,qr_pairs\n",
    "\n",
    "t1=time.time()\n",
    "print(\"Preparing dataset and corresponding vocabulary...\")\n",
    "voc, pairs=prepareDataset(qr_pairs)\n",
    "print(\"Preparation took place in: \"+str(time.time()-t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trimming rare words from vocabulary and dataset..\n",
      "Trimming took place in: 0.30527830123901367\n"
     ]
    }
   ],
   "source": [
    "Min_Count=3\n",
    "\n",
    "def trimRareWords(voc,qr_pairs):\n",
    "    \n",
    "    voc.trim(Min_Count)\n",
    "    keep_pairs=[]\n",
    "    \n",
    "    for pair in qr_pairs:\n",
    "        input_sentence=pair[0]\n",
    "        output_sentence=pair[1]\n",
    "        \n",
    "        keep_input=True\n",
    "        keep_output=True\n",
    "        \n",
    "        for word in input_sentence.split(\" \"):\n",
    "            if word not in voc.word2index:\n",
    "                keep_input=False\n",
    "                break\n",
    "        \n",
    "        for word in output_sentence.split(\" \"):\n",
    "            if word not in voc.word2index:\n",
    "                keep_output=False\n",
    "                break\n",
    "                \n",
    "        if keep_input and keep_output:\n",
    "            keep_pairs.append(pair)\n",
    "            \n",
    "    return keep_pairs\n",
    "\n",
    "t1=time.time()\n",
    "print(\"Trimming rare words from vocabulary and dataset..\")\n",
    "\n",
    "pairs=trimRareWords(voc,pairs)\n",
    "\n",
    "print(\"Trimming took place in: \"+str(time.time()-t1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(voc,sentence):\n",
    "    tokenised_sentence=[]\n",
    "    tokenised_sentence.append(START_Token)\n",
    "    \n",
    "    for word in sentence.split(\" \"):\n",
    "        tokenised_sentence.append(voc.word2index[word])\n",
    "        \n",
    "    tokenised_sentence.append(END_Token)\n",
    "    \n",
    "    assert len(tokenised_sentence)<=Max_Length+2\n",
    "    for _ in range(Max_Length+2-len(tokenised_sentence)):\n",
    "        tokenised_sentence.append(PAD_Token)\n",
    "        \n",
    "    return tokenised_sentence\n",
    "\n",
    "def binaryMatrix(l,value=PAD_Token):\n",
    "    m=[]\n",
    "    for i,seq in enumerate(l):\n",
    "        m.append([])\n",
    "        for token in seq:\n",
    "            if token==value:\n",
    "                m[i].append(0)\n",
    "            else:\n",
    "                m[i].append(1)\n",
    "        \n",
    "    return m\n",
    "\n",
    "def inputVar(voc,l):\n",
    "    \n",
    "    indexes_batch=[indexesFromSentence(voc,sentence) for sentence in l]\n",
    "    input_lengths=torch.tensor([len(index) for index in indexes_batch])\n",
    "    padVar=torch.LongTensor(indexes_batch)\n",
    "    return input_lengths,padVar\n",
    "\n",
    "def outputVar(voc,l):\n",
    "    indexes_batch=[indexesFromSentence(voc,sentence) for sentence in l]\n",
    "    max_target_len=torch.tensor([len(index) for index in indexes_batch])\n",
    "    mask=binaryMatrix(indexes_batch)\n",
    "    mask=torch.ByteTensor(mask)\n",
    "    padVar=torch.LongTensor(indexes_batch)\n",
    "    return max_target_len, mask, padVar\n",
    "\n",
    "def batch2TrainData(voc,pair_batch):\n",
    "    #sort function see \n",
    "    input_batch=[]\n",
    "    output_batch=[]\n",
    "\n",
    "    for pair in pair_batch:\n",
    "        input_batch.append(pair[0])\n",
    "        output_batch.append(pair[1])\n",
    "                                  \n",
    "    \n",
    "    input_lengths,tokenised_input=inputVar(voc,input_batch)\n",
    "    max_out_length,mask,tokenised_output=outputVar(voc,output_batch)\n",
    "    return input_lengths,tokenised_input,max_out_length,mask,tokenised_output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query-response pairs after all the preprocessing: 53113\n",
      "Input length: tensor([12, 12, 12, 12, 12]) Size: torch.Size([5])\n",
      "--------------------------------------------------------------------------------\n",
      "Tokenised Input: tensor([[   1, 2762,    4,   25,  200,   12, 2209,  140,    4,    2,    0,    0],\n",
      "        [   1,   76,   37,   36, 6525,   66,    2,    0,    0,    0,    0,    0],\n",
      "        [   1,  122,  101,  250,  518,    7,  129,    6,    2,    0,    0,    0],\n",
      "        [   1,  401,  177,  111,   66,    2,    0,    0,    0,    0,    0,    0],\n",
      "        [   1, 2324,    4,    4,    4,  167,   64,    4,    2,    0,    0,    0]]) Size: torch.Size([5, 12])\n",
      "--------------------------------------------------------------------------------\n",
      "Max out length: tensor([12, 12, 12, 12, 12]) Size: torch.Size([5])\n",
      "--------------------------------------------------------------------------------\n",
      "Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]], dtype=torch.uint8) Size: torch.Size([5, 12])\n",
      "--------------------------------------------------------------------------------\n",
      "Tokenised Output: tensor([[   1,    7,  153,   12, 1620,  153,    7,    6,    2,    0,    0,    0],\n",
      "        [   1,  318, 5710, 6526,    4,    2,    0,    0,    0,    0,    0,    0],\n",
      "        [   1,  250, 2582,   83,    4,    2,    0,    0,    0,    0,    0,    0],\n",
      "        [   1,  100,    6,    2,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [   1,   95,   61,   37,  123,   21,  169, 5008, 5004,    6,    2,    0]]) Size: torch.Size([5, 12])\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of query-response pairs after all the preprocessing: \"+str(len(pairs)))\n",
    "\n",
    "#Sample batch\n",
    "batch=[random.choice(pairs) for _ in range(5)]\n",
    "input_lengths,tokenised_input,max_out_length,mask,tokenised_output=batch2TrainData(voc,batch)\n",
    "\n",
    "print(\"Input length: \"+str(input_lengths)+\" Size: \"+str(input_lengths.shape))\n",
    "print(\"-\"*80)\n",
    "print(\"Tokenised Input: \"+str(tokenised_input)+\" Size: \"+str(tokenised_input.shape))\n",
    "print(\"-\"*80)\n",
    "print(\"Max out length: \"+str(max_out_length)+\" Size: \"+str(max_out_length.shape))\n",
    "print(\"-\"*80)\n",
    "print(\"Mask: \"+str(mask)+\" Size: \"+str(mask.shape))\n",
    "print(\"-\"*80)\n",
    "print(\"Tokenised Output: \"+str(tokenised_output)+\" Size: \"+str(tokenised_output.shape))\n",
    "print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A standard Encoder-Decoder architecture. Base for this and many \n",
    "    other models.\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.generator = generator\n",
    "        \n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        \"Take in and process masked src and target sequences.\"\n",
    "        return self.decode(self.encode(src, src_mask), src_mask,\n",
    "                            tgt, tgt_mask)\n",
    "    \n",
    "    def encode(self, src, src_mask):\n",
    "        return self.encoder(self.src_embed(src), src_mask)\n",
    "    \n",
    "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
    "        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"Define standard linear + softmax generation step.\"\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Generator, self).__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.log_softmax(self.proj(x), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clones(module, N):\n",
    "    \"Produce N identical layers.\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"Core encoder is a stack of N layers\"\n",
    "    def __init__(self, layer, N):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        \"Pass the input (and mask) through each layer in turn.\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"Construct a layernorm module (See citation for details).\"\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual connection followed by a layer norm.\n",
    "    Note for code simplicity the norm is first as opposed to last.\n",
    "    \"\"\"\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"Apply residual connection to any sublayer with the same size.\"\n",
    "        return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \"Encoder is made up of self-attn and feed forward (defined below)\"\n",
    "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"Follow Figure 1 (left) for connections.\"\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
    "        return self.sublayer[1](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"Generic N layer decoder with masking.\"\n",
    "    def __init__(self, layer, N):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "        \n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, memory, src_mask, tgt_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \"Decoder is made of self-attn, src-attn, and feed forward (defined below)\"\n",
    "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.size = size\n",
    "        self.self_attn = self_attn\n",
    "        self.src_attn = src_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n",
    " \n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        \"Follow Figure 1 (right) for connections.\"\n",
    "        m = memory\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
    "        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n",
    "        return self.sublayer[2](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsequent_mask(size):\n",
    "    \"Mask out subsequent positions.\"\n",
    "    attn_shape = (1, size, size)\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
    "    return torch.from_numpy(subsequent_mask) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    \"Compute 'Scaled Dot Product Attention'\"\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) \\\n",
    "             / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    p_attn = F.softmax(scores, dim = -1)\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    return torch.matmul(p_attn, value), p_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        \"Take in model size and number of heads.\"\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert d_model % h == 0\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"Implements Figure 2\"\n",
    "        if mask is not None:\n",
    "            # Same mask applied to all h heads.\n",
    "            mask = mask.unsqueeze(1)\n",
    "        nbatches = query.size(0)\n",
    "        \n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k \n",
    "        query, key, value = \\\n",
    "            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "             for l, x in zip(self.linears, (query, key, value))]\n",
    "        \n",
    "        # 2) Apply attention on all the projected vectors in batch. \n",
    "        x, self.attn = attention(query, key, value, mask=mask, \n",
    "                                 dropout=self.dropout)\n",
    "        \n",
    "        # 3) \"Concat\" using a view and apply a final linear. \n",
    "        x = x.transpose(1, 2).contiguous() \\\n",
    "             .view(nbatches, -1, self.h * self.d_k)\n",
    "        return self.linears[-1](x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"Implements FFN equation.\"\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(F.relu(self.w_1(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.lut = nn.Embedding(vocab, d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lut(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"Implement the PE function.\"\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) *\n",
    "                             -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + Variable(self.pe[:, :x.size(1)], \n",
    "                         requires_grad=False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(src_vocab, tgt_vocab, N=6, \n",
    "               d_model=512, d_ff=2048, h=8, dropout=0.1):\n",
    "    \"Helper: Construct a model from hyperparameters.\"\n",
    "    c = copy.deepcopy\n",
    "    attn = MultiHeadedAttention(h, d_model)\n",
    "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "    position = PositionalEncoding(d_model, dropout)\n",
    "    model = EncoderDecoder(\n",
    "        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
    "        Decoder(DecoderLayer(d_model, c(attn), c(attn), \n",
    "                             c(ff), dropout), N),\n",
    "        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),\n",
    "        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),\n",
    "        Generator(d_model, tgt_vocab))\n",
    "    \n",
    "    # This was important from their code. \n",
    "    # Initialize parameters with Glorot / fan_avg.\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform(p)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    \n",
    "    def __init__(self,encoder,decoder,source_embed,target_embed,generator):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder=encoder\n",
    "        self.decoder=decoder\n",
    "        \n",
    "        self.source_embed=source_embed\n",
    "        self.target_embed=target_embed\n",
    "        \n",
    "        self.generator=generator # Linear + Log_softmax\n",
    "        \n",
    "    def forward(self,source,target,source_mask,target_mask):\n",
    "        return self.decode(self.encode(source,source_mask),source_mask,target,target_mask)\n",
    "    \n",
    "    def encode(self,source,source_mask):\n",
    "        return self.encoder(self.source_embed(source),source_mask)\n",
    "    \n",
    "    def decode(self,memory, source_mask,target,target_mask):\n",
    "        return self.decoder(self.target_embed(target),memory,source_mask,target_mask)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \n",
    "    def __init__(self,d_model,vocab_size):\n",
    "        super().__init__()\n",
    "        self.projection=nn.Linear(d_model,vocab_size)\n",
    "        \n",
    "    def forward(self,decoder_output):\n",
    "        return F.log_softmax(self.projection(decoder_output),dim=-1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clones(module,N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self,layer,N):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layers=clones(layer,N)\n",
    "        self.norm=LayerNorm(layer.size)\n",
    "    \n",
    "    def forward(self,x,mask):\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x=layer(x,mask)\n",
    "        \n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self,size,self_attn,feed_forward,dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attn=self_attn\n",
    "        self.feed_forward=feed_forward\n",
    "        self.sublayer=clones(SublayerConnection(size,dropout),2)\n",
    "        self.size=size\n",
    "        \n",
    "    def forward(self,x,mask):\n",
    "        \n",
    "        x=self.sublayer[0](x,lambda x: self.attn(x,x,x,mask))\n",
    "        return self.sublayer[1](x,self.feed_forward)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \n",
    "    def __init__(self,features,eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.a_2=nn.Parameter(torch.ones(features))\n",
    "        self.b_2=nn.Parameter(torch.zeros(features))\n",
    "        self.eps=eps\n",
    "        \n",
    "    def forward(self,x):\n",
    "        mean=x.mean(-1,keepdim=True)\n",
    "        std=x.std(-1,keepdim=True)\n",
    "        return self.a_2*(x-mean)/(x+std)+self.b_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SublayerConnection(nn.Module):\n",
    "    \n",
    "    def __init__(self,size,dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "        self.norm=LayerNorm(size)\n",
    "        \n",
    "    def forward(self,x,sublayer):\n",
    "        return x+self.dropout(sublayer(self.norm(x)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self,layer,N):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layers=clones(layer,N)\n",
    "        self.norm=LayerNorm(layer.size)\n",
    "    \n",
    "    def forward(self,x,memory,curr_mask,tgt_mask):\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x=layer(x,memory,curr_mask,tgt_mask)\n",
    "            \n",
    "        return self.norm(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self,size,self_attn,src_attn,feed_forward,dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.size=size\n",
    "        self.self_attn=self_attn\n",
    "        self.src_attn=src_attn\n",
    "        self.feed_forward=feed_forward\n",
    "        \n",
    "        self.sublayer=clones(SublayerConnection(size,dropout),3)\n",
    "        \n",
    "    def forward(self,x,memory,src_mask,tgt_mask):\n",
    "        \n",
    "        m=memory\n",
    "        x=self.sublayer[0](x,lambda x:self.self_attn(x,x,x,tgt_mask))\n",
    "        x=self.sublayer[1](x,lambda x: self.src_attn(x,m,m,src_mask))\n",
    "        return self.sublayer[2](x,self.feed_forward)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self,h,d_model,dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert d_model%h==0\n",
    "        \n",
    "        self.d_k=d_model//h\n",
    "        self.h=h\n",
    "        self.linears=clones(nn.Linear(d_model,d_model),4)\n",
    "        self.attn=None\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self,query,key,values,mask=None):\n",
    "        \n",
    "        if mask is not None:\n",
    "            mask=mask.unsqueeze(1)\n",
    "            \n",
    "        nbatches=query.size(0)\n",
    "        \n",
    "        query,key,values=[l(x).view(nbatches,-1,self.h,self.d_k).transpose(1,2) for l, x in zip(self.linears,(query,key,values))]\n",
    "        \n",
    "        x,self.attn=attention(query,key,values,mask=mask,dropout=self.dropout)\n",
    "        \n",
    "        x=x.transpose(1,2).contiguous().view(nbatches,-1,self.h*self.d_k)\n",
    "        \n",
    "        return self.linears[-1](x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(query,key,value,mask=None,dropout=None):\n",
    "    \n",
    "    d_k=query.size(-1)\n",
    "\n",
    "    scores=torch.matmul(query,key.transpose(-2,-1))/math.sqrt(d_k)\n",
    "    \n",
    "    if mask is not None:\n",
    "        scores=scores.masked_fill(mask==0,-1e9)\n",
    "        \n",
    "    p_attn=F.softmax(scores,dim=-1)\n",
    "    \n",
    "    if dropout is not None:\n",
    "        p_attn=dropout(p_attn)\n",
    "        \n",
    "    return torch.matmul(p_attn,value),p_attn\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \n",
    "    def __init__(self,d_model,d_ff,dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.w_1=nn.Linear(d_model,d_ff)\n",
    "        self.w_2=nn.Linear(d_ff,d_model)\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.w_2(self.dropout(F.relu(self.w_1(x))))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    \n",
    "    def __init__(self,d_model,vocab):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embed=nn.Embedding(vocab,d_model)\n",
    "        self.d_model=d_model\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.embed(x)*math.sqrt(self.d_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \n",
    "    def __init__(self,d_model,dropout,max_len=5000):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "        pe=torch.zeros(max_len,d_model,dtype=torch.float)\n",
    "        position=torch.arange(0.,max_len).unsqueeze(1)\n",
    "        div_term=torch.exp(torch.arange(0.,d_model,2)*-(math.log(10000.0)/d_model))\n",
    "        \n",
    "        pe[:,0::2]=torch.sin(position*div_term)\n",
    "        pe[:,1::2]=torch.cos(position*div_term)\n",
    "        \n",
    "        pe=pe.unsqueeze(0)\n",
    "        self.register_buffer('pe',pe)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        x=x+Variable(self.pe[:,:x.size(1)],requires_grad=False)\n",
    "        return self.dropout(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model2(src_vocab,tgt_vocab,N=6,d_model=512,d_ff=2048,h=8,dropout=0.1):\n",
    "    \n",
    "    c=copy.deepcopy\n",
    "    attn=MultiHeadedAttention(h,d_model)\n",
    "    ff=PositionwiseFeedForward(d_model,d_ff,dropout)\n",
    "    position=PositionalEncoding(d_model,dropout)\n",
    "    model=EncoderDecoder(Encoder(EncoderLayer(d_model,c(attn),c(ff),dropout),N),\n",
    "                        Decoder(DecoderLayer(d_model,c(attn),c(attn),c(ff),dropout),N),\n",
    "                        nn.Sequential(Embeddings(d_model,src_vocab),c(position)),\n",
    "                        nn.Sequential(Embeddings(d_model,tgt_vocab),c(position)),\n",
    "                        Generator(d_model,tgt_vocab))\n",
    "    \n",
    "    for p in model.parameters():\n",
    "        if p.dim()>1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\deepa\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:20: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n"
     ]
    }
   ],
   "source": [
    "sample_model=make_model(voc.num_words,voc.num_words,1,512,2048,8,0.1)\n",
    "# print(sample_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Output size: torch.Size([5, 12, 512])\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#Sample Run\n",
    "source=torch.ones(5,12,dtype=torch.long)\n",
    "target=torch.ones(5,12,dtype=torch.long)\n",
    "source_mask=None\n",
    "target_mask=torch.ones(5,12,12,dtype=torch.long)\n",
    "out=sample_model(source,target,source_mask,target_mask)\n",
    "print(\"-\"*80)\n",
    "print(\"Output size: \"+str(out.shape))\n",
    "print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "triu function generates a copy of matrix with elemens below kth diagonal zeroed.\n",
    "The main diagonal is zeroeth diagonal above is first(k=1) and so on.\n",
    "\n",
    "Eg:\n",
    "A=[[1,2,3],[4,5,6],[7,8,9]]\n",
    "for above matrix:\n",
    "triu(A,k=1)\n",
    "will give [[0,2,3],[0,0,6],[0,0,0]]\n",
    "\"\"\"\n",
    "\n",
    "def subsequent_mask(size):\n",
    "    attn_shape=(1,size,size)\n",
    "    mask=np.triu(np.ones(attn_shape),k=1).astype('uint8')\n",
    "    \n",
    "    return torch.from_numpy(mask)==0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 5,  1,  3, 10,  6,  4,  5,  6,  5,  1],\n",
      "        [ 7,  7,  3,  3,  2,  2,  3,  3,  5, 10],\n",
      "        [ 2,  5,  1,  6,  6,  8,  2,  5,  6,  8],\n",
      "        [ 6,  1,  6,  2,  5,  9,  3,  9,  5, 10],\n",
      "        [ 3,  8,  9, 10,  4,  2,  7,  2,  1,  3]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "data=torch.from_numpy(np.random.randint(1,11,size=(5,10)))\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1,  1,  3, 10,  6,  4,  5,  6,  5,  1],\n",
      "        [ 1,  7,  3,  3,  2,  2,  3,  3,  5, 10],\n",
      "        [ 1,  5,  1,  6,  6,  8,  2,  5,  6,  8],\n",
      "        [ 1,  1,  6,  2,  5,  9,  3,  9,  5, 10],\n",
      "        [ 1,  8,  9, 10,  4,  2,  7,  2,  1,  3]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "data[:,0]=1\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generation(pairs,batch_size,n_batches):\n",
    "    \n",
    "    sample_batches=[batch2TrainData(voc,[random.choice(pairs) for _ in range(batch_size)]) for _ in range(n_batches)]\n",
    "    batches=[]\n",
    "    \n",
    "    for i in range(n_batches):\n",
    "        batches.append(Batch(sample_batches[i][1],sample_batches[i][-1]))\n",
    "#     batches=[]\n",
    "#     for i in range(n_batches):\n",
    "#         data = torch.from_numpy(np.random.randint(1, 11, size=(batch_size, 10)))\n",
    "#         data[:,0]=1\n",
    "        \n",
    "#         batches.append(Batch(data,data))\n",
    "    \n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Batch:\n",
    "    \n",
    "#     def __init__(self,sample_batch,pad):\n",
    "        \n",
    "#         self.src=sample_batch[1]\n",
    "#         self.src_mask=self.make_src_mask(self.src,pad)\n",
    "#         self.trg=sample_batch[-1][:,:-1]\n",
    "#         self.trg_mask=self.make_trg_mask(self.trg,pad)\n",
    "#         self.trg_y=sample_batch[-1][:,1:]\n",
    "#         self.ntokens=(self.trg_y!=pad).data.sum()\n",
    "        \n",
    "#     @staticmethod\n",
    "#     def make_src_mask(src,pad):\n",
    "#         return (src!=pad).unsqueeze(-2)\n",
    "#     @staticmethod    \n",
    "#     def make_trg_mask(trg,pad):\n",
    "#         trg_mask=(trg!=pad).unsqueeze(-2)\n",
    "# #         trg_mask=trg_mask&Variable(subsequent_mask(trg.size(-1)).type_as(trg_mask.data))\n",
    "#         return trg_mask\n",
    "class Batch:\n",
    "    \"Object for holding a batch of data with mask during training.\"\n",
    "    def __init__(self, src, trg=None, pad=0):\n",
    "        src=torch.tensor(src).to(torch.int64)\n",
    "        trg=torch.tensor(trg).to(torch.int64)\n",
    "        self.src = src\n",
    "        self.src_mask = (src != pad).unsqueeze(-2)\n",
    "        if trg is not None:\n",
    "            self.trg = trg[:, :-1]\n",
    "            self.trg_y = trg[:, 1:]\n",
    "            self.trg_mask = \\\n",
    "                self.make_std_mask(self.trg, pad)\n",
    "            self.ntokens = (self.trg_y != pad).data.sum()\n",
    "    \n",
    "    @staticmethod\n",
    "    def make_std_mask(tgt, pad):\n",
    "        \"Create a mask to hide padding and future words.\"\n",
    "        tgt_mask = (tgt != pad).unsqueeze(-2)\n",
    "        tgt_mask = tgt_mask & Variable(\n",
    "            subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data))\n",
    "        return tgt_mask        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(data,model,loss_compute):\n",
    "    \n",
    "    start_time=time.time()\n",
    "    total_tokens=0\n",
    "    total_loss=0\n",
    "    tokens=0\n",
    "    \n",
    "    out=model(data.src,data.trg,data.src_mask,data.trg_mask)\n",
    "#     print(\"Model output: \"+str(out.size()))\n",
    "    loss=loss_compute(out,data.trg_y,data.ntokens)\n",
    "    \n",
    "    return loss\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def customLossFunction(outputs,target):\n",
    "    batch_size=outputs.size()[0]\n",
    "    numberOfWords=outputs.size()[1]\n",
    "    outputs=F.softmax(outputs,dim=-1)\n",
    "    loss=0\n",
    "    normalisingVal=0\n",
    "#     print(outputs)\n",
    "#     print(target)\n",
    "    for i in range(batch_size):\n",
    "        for j in range(numberOfWords):\n",
    "            trg=target[i][j]\n",
    "            if trg!=0:\n",
    "                \n",
    "                currLoss=-(outputs[i][j][trg]+5)\n",
    "                loss+=currLoss\n",
    "                normalisingVal+=1\n",
    "    return loss/normalisingVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class LabelSmoothing(nn.Module):\n",
    "    \n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.criteria=customLossFunction()\n",
    "#     def forward(self,x,target):\n",
    "#         return self.criteria(x,target)\n",
    "class LabelSmoothing(nn.Module):\n",
    "    \"Implement label smoothing.\"\n",
    "    def __init__(self, size, padding_idx, smoothing=0.0):\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "        self.criterion = nn.KLDivLoss(size_average=False)\n",
    "        self.padding_idx = padding_idx\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.size = size\n",
    "        self.true_dist = None\n",
    "        \n",
    "    def forward(self, x, target):\n",
    "#         print(\"Before assertion: \"+str(x.size())+str(self.size))\n",
    "        assert x.size(1) == self.size\n",
    "        true_dist = x.data.clone()\n",
    "        true_dist.fill_(self.smoothing / (self.size - 2))\n",
    "        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        true_dist[:, self.padding_idx] = 0\n",
    "        mask = torch.nonzero(target.data == self.padding_idx)\n",
    "        if mask.dim() > 0:\n",
    "            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
    "        self.true_dist = true_dist\n",
    "        return self.criterion(x, Variable(true_dist, requires_grad=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class LossCompute:\n",
    "    \n",
    "#     def __init__(self,model,opt):\n",
    "        \n",
    "#         self.opt=opt\n",
    "#         self.model=model\n",
    "    \n",
    "#     def __call__(self,x,y,norm):\n",
    "        \n",
    "#         x=self.model.generator(x)\n",
    "#         loss=customLossFunction(x,y)\n",
    "        \n",
    "    \n",
    "       \n",
    "\n",
    "#         loss.backward()\n",
    "        \n",
    "# #         _=nn.utils.clip_grad_norm_(model.parameters(),50.0)\n",
    "        \n",
    "#         plot_grad_flow(self.model.named_parameters())\n",
    "        \n",
    "#         self.opt.step()\n",
    "#         self.opt.optimizer.zero_grad()\n",
    "        \n",
    "#         return loss.item()\n",
    "\n",
    "class SimpleLossCompute:\n",
    "    \"A simple loss compute and train function.\"\n",
    "    def __init__(self, generator, criterion, opt=None):\n",
    "        self.generator = generator\n",
    "        self.criterion = criterion\n",
    "        self.opt = opt\n",
    "        \n",
    "    def __call__(self, x, y, norm):\n",
    "        x = self.generator(x)\n",
    "#         print(str(x.size())+\" \"+str(y.size()))\n",
    "        loss = self.criterion(x.contiguous().view(-1, x.size(-1)), \n",
    "                              y.contiguous().view(-1)) / norm\n",
    "        loss.backward()\n",
    "        if self.opt is not None:\n",
    "            self.opt.step()\n",
    "            self.opt.optimizer.zero_grad()\n",
    "        return loss.item()* norm\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoamOpt:\n",
    "    \"Optim wrapper that implements rate.\"\n",
    "    def __init__(self, model_size, factor, warmup, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        self._step = 0\n",
    "        self.warmup = warmup\n",
    "        self.factor = factor\n",
    "        self.model_size = model_size\n",
    "        self._rate = 0\n",
    "        \n",
    "    def step(self):\n",
    "        \"Update parameters and rate\"\n",
    "        self._step += 1\n",
    "        rate = self.rate()\n",
    "        for p in self.optimizer.param_groups:\n",
    "            p['lr'] = rate\n",
    "        self._rate = rate\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def rate(self, step = None):\n",
    "        \"Implement `lrate` above\"\n",
    "        if step is None:\n",
    "            step = self._step\n",
    "        return self.factor * \\\n",
    "            (self.model_size ** (-0.5) *\n",
    "            min(step ** (-0.5), step * self.warmup ** (-1.5)))\n",
    "        \n",
    "def get_std_opt(model):\n",
    "    return NoamOpt(model.src_embed[0].d_model, 2, 4000,\n",
    "            torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\deepa\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "C:\\Users\\deepa\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    }
   ],
   "source": [
    "batches=data_generation(pairs,30,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialising and creating models....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\deepa\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:20: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Creating Models took: 0.8907039165496826\n",
      "Epoch: 0 Loss Value: tensor(1657.2264)\n",
      "Epoch: 1 Loss Value: tensor(1586.2345)\n",
      "Epoch: 2 Loss Value: tensor(1378.1316)\n",
      "Epoch: 3 Loss Value: tensor(1636.3027)\n",
      "Epoch: 4 Loss Value: tensor(1531.3615)\n",
      "Epoch: 5 Loss Value: tensor(1579.5085)\n",
      "Epoch: 6 Loss Value: tensor(1443.1082)\n",
      "Epoch: 7 Loss Value: tensor(1482.9202)\n",
      "Epoch: 8 Loss Value: tensor(1160.6945)\n",
      "Epoch: 9 Loss Value: tensor(1452.5197)\n",
      "Epoch: 10 Loss Value: tensor(1364.9233)\n",
      "Epoch: 11 Loss Value: tensor(1296.8413)\n",
      "Epoch: 12 Loss Value: tensor(1127.3286)\n",
      "Epoch: 13 Loss Value: tensor(1391.3564)\n",
      "Epoch: 14 Loss Value: tensor(1297.3831)\n",
      "Epoch: 15 Loss Value: tensor(1372.6509)\n",
      "Epoch: 16 Loss Value: tensor(1269.8928)\n",
      "Epoch: 17 Loss Value: tensor(1311.6996)\n",
      "Epoch: 18 Loss Value: tensor(1023.3458)\n",
      "Epoch: 19 Loss Value: tensor(1314.2087)\n",
      "Epoch: 20 Loss Value: tensor(1223.4802)\n",
      "Epoch: 21 Loss Value: tensor(1161.8348)\n",
      "Epoch: 22 Loss Value: tensor(1012.9011)\n",
      "Epoch: 23 Loss Value: tensor(1258.1986)\n",
      "Epoch: 24 Loss Value: tensor(1162.3579)\n",
      "Epoch: 25 Loss Value: tensor(1235.8893)\n",
      "Epoch: 26 Loss Value: tensor(1136.8291)\n",
      "Epoch: 27 Loss Value: tensor(1170.1824)\n",
      "Epoch: 28 Loss Value: tensor(904.0647)\n",
      "Epoch: 29 Loss Value: tensor(1157.6055)\n",
      "Epoch: 30 Loss Value: tensor(1072.2244)\n",
      "Epoch: 31 Loss Value: tensor(1006.2238)\n",
      "Epoch: 32 Loss Value: tensor(876.4068)\n",
      "Epoch: 33 Loss Value: tensor(1095.5630)\n",
      "Epoch: 34 Loss Value: tensor(999.5365)\n",
      "Epoch: 35 Loss Value: tensor(1069.5413)\n",
      "Epoch: 36 Loss Value: tensor(974.7996)\n",
      "Epoch: 37 Loss Value: tensor(999.8918)\n",
      "Epoch: 38 Loss Value: tensor(773.1822)\n",
      "Epoch: 39 Loss Value: tensor(985.1885)\n",
      "Epoch: 40 Loss Value: tensor(915.1704)\n",
      "Epoch: 41 Loss Value: tensor(849.4941)\n",
      "Epoch: 42 Loss Value: tensor(739.1456)\n",
      "Epoch: 43 Loss Value: tensor(937.8759)\n",
      "Epoch: 44 Loss Value: tensor(848.1702)\n",
      "Epoch: 45 Loss Value: tensor(919.7125)\n",
      "Epoch: 46 Loss Value: tensor(834.9576)\n",
      "Epoch: 47 Loss Value: tensor(854.6983)\n",
      "Epoch: 48 Loss Value: tensor(665.1894)\n",
      "Epoch: 49 Loss Value: tensor(860.8171)\n",
      "Epoch: 50 Loss Value: tensor(818.4408)\n",
      "Epoch: 51 Loss Value: tensor(738.1246)\n",
      "Epoch: 52 Loss Value: tensor(645.9249)\n",
      "Epoch: 53 Loss Value: tensor(840.3709)\n",
      "Epoch: 54 Loss Value: tensor(766.4519)\n",
      "Epoch: 55 Loss Value: tensor(847.5755)\n",
      "Epoch: 56 Loss Value: tensor(770.4510)\n",
      "Epoch: 57 Loss Value: tensor(793.1479)\n",
      "Epoch: 58 Loss Value: tensor(625.2965)\n",
      "Epoch: 59 Loss Value: tensor(811.7842)\n",
      "Epoch: 60 Loss Value: tensor(777.9280)\n",
      "Epoch: 61 Loss Value: tensor(698.3815)\n",
      "Epoch: 62 Loss Value: tensor(614.4357)\n",
      "Epoch: 63 Loss Value: tensor(806.9230)\n",
      "Epoch: 64 Loss Value: tensor(735.9477)\n",
      "Epoch: 65 Loss Value: tensor(822.6773)\n",
      "Epoch: 66 Loss Value: tensor(739.9306)\n",
      "Epoch: 67 Loss Value: tensor(775.0670)\n",
      "Epoch: 68 Loss Value: tensor(608.7178)\n",
      "Epoch: 69 Loss Value: tensor(793.2363)\n",
      "Epoch: 70 Loss Value: tensor(756.8273)\n",
      "Epoch: 71 Loss Value: tensor(679.8534)\n",
      "Epoch: 72 Loss Value: tensor(594.1995)\n",
      "Epoch: 73 Loss Value: tensor(789.4110)\n",
      "Epoch: 74 Loss Value: tensor(719.9259)\n",
      "Epoch: 75 Loss Value: tensor(796.9690)\n",
      "Epoch: 76 Loss Value: tensor(716.0411)\n",
      "Epoch: 77 Loss Value: tensor(753.0396)\n",
      "Epoch: 78 Loss Value: tensor(583.2648)\n",
      "Epoch: 79 Loss Value: tensor(771.8655)\n",
      "Epoch: 80 Loss Value: tensor(723.2242)\n",
      "Epoch: 81 Loss Value: tensor(660.6233)\n",
      "Epoch: 82 Loss Value: tensor(568.7683)\n",
      "Epoch: 83 Loss Value: tensor(772.6178)\n",
      "Epoch: 84 Loss Value: tensor(697.5414)\n",
      "Epoch: 85 Loss Value: tensor(772.7250)\n",
      "Epoch: 86 Loss Value: tensor(692.6972)\n",
      "Epoch: 87 Loss Value: tensor(721.2714)\n",
      "Epoch: 88 Loss Value: tensor(560.5253)\n",
      "Epoch: 89 Loss Value: tensor(746.7214)\n",
      "Epoch: 90 Loss Value: tensor(709.4689)\n",
      "Epoch: 91 Loss Value: tensor(645.7108)\n",
      "Epoch: 92 Loss Value: tensor(547.8052)\n",
      "Epoch: 93 Loss Value: tensor(736.7264)\n",
      "Epoch: 94 Loss Value: tensor(668.5059)\n",
      "Epoch: 95 Loss Value: tensor(739.3301)\n",
      "Epoch: 96 Loss Value: tensor(662.5372)\n",
      "Epoch: 97 Loss Value: tensor(688.2899)\n",
      "Epoch: 98 Loss Value: tensor(540.5969)\n",
      "Epoch: 99 Loss Value: tensor(713.6450)\n"
     ]
    }
   ],
   "source": [
    "print(\"Initialising and creating models....\")\n",
    "V=voc.num_words\n",
    "t1=time.time()\n",
    "# criterion=LabelSmoothing()\n",
    "criterion = LabelSmoothing(size=V, padding_idx=0, smoothing=0.0)\n",
    "\n",
    "model=make_model(V,V)\n",
    "# model_opt=torch.optim.Adam(model.parameters(),lr=0.0001,betas=(0.9,0.988),eps=1e-9)\n",
    "model_opt = NoamOpt(model.src_embed[0].d_model, 1, 400,\n",
    "        torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))\n",
    "print(\"=\"*100)\n",
    "print(\"Creating Models took: \"+str(time.time()-t1))\n",
    "\n",
    "\n",
    "\n",
    "model.train()\n",
    "for epoch in range(100):\n",
    "    \n",
    "    current_batch=batches[epoch%10]\n",
    "    loss_val=run_epoch(current_batch,model,SimpleLossCompute(model.generator, criterion, model_opt))\n",
    "    print(\"Epoch: \"+str(epoch)+\" Loss Value: \"+str(loss_val))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    memory = model.encode(src, src_mask)\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type_as(src.data)\n",
    "    for i in range(max_len-1):\n",
    "        out = model.decode(memory, src_mask, \n",
    "                           Variable(ys), \n",
    "                           Variable(subsequent_mask(ys.size(1))\n",
    "                                    .type_as(src.data)))\n",
    "        prob = model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim = 1)\n",
    "        next_word = next_word.data[0]\n",
    "        ys = torch.cat([ys, \n",
    "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1)\n",
    "    return ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderDecoder(\n",
       "  (encoder): Encoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): EncoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): EncoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): EncoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): EncoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): EncoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): EncoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm()\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): DecoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (src_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): DecoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (src_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): DecoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (src_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): DecoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (src_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): DecoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (src_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): DecoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (src_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm()\n",
       "  )\n",
       "  (src_embed): Sequential(\n",
       "    (0): Embeddings(\n",
       "      (lut): Embedding(7816, 512)\n",
       "    )\n",
       "    (1): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (tgt_embed): Sequential(\n",
       "    (0): Embeddings(\n",
       "      (lut): Embedding(7816, 512)\n",
       "    )\n",
       "    (1): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (generator): Generator(\n",
       "    (proj): Linear(in_features=512, out_features=7816, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12])\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "tensor(1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-209-d2fc48964690>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mpred\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0msrc_sentence\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex2word\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mid\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msrc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[0mtrg_sentence\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex2word\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mid\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrg\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mpred_sentence\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex2word\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mid\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-209-d2fc48964690>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mpred\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0msrc_sentence\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex2word\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mid\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msrc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[0mtrg_sentence\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex2word\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mid\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrg\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mpred_sentence\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex2word\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mid\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: tensor(1)"
     ]
    }
   ],
   "source": [
    "for batch in batches:\n",
    "    for i in range(5):\n",
    "        output=greedy_decode(model,batch.src[i].view(-1,12),batch.src_mask[i].view(1,-1,12),10,1)\n",
    "        src=batch.src[i].view(-1)\n",
    "        trg=batch.trg[i].view(-1)\n",
    "        pred=output.view(-1)\n",
    "        print(src.size())\n",
    "        src_sentence=[voc.index2word[id] for id in src]\n",
    "        trg_sentence=[voc.index2word[id] for id in trg]\n",
    "        pred_sentence=[voc.index2word[id] for id in pred]\n",
    "        print(src_sentence)\n",
    "        print(trg_sentence)\n",
    "        print(pred_sentence)\n",
    "        print(\"-\"*80)\n",
    "#         print(\"-\"*80)\n",
    "#         print(str(output)+\" \"+str(batch.src[i])+\" \"+str(batch.trg[i]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1,  2, 10, 10,  9,  5,  3,  2,  9,  5]])\n"
     ]
    }
   ],
   "source": [
    "src = Variable(torch.LongTensor([[1,2,3,4,5,6,7,8,9,10]]) )\n",
    "src_mask = Variable(torch.ones(1, 1, 10) )\n",
    "print(greedy_decode(model, src, src_mask, max_len=10, start_symbol=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
