{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre-Processing\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "#Model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# For visualising metrics\n",
    "from visdom import Visdom\n",
    "\n",
    "# For visualising gradients plot\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The device found: cuda\n"
     ]
    }
   ],
   "source": [
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device=torch.device(\"cpu\")\n",
    "print(\"The device found: \"+str(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class VisdomLinePlotter(object):\n",
    "    \"\"\"Plots to Visdom\"\"\"\n",
    "    \n",
    "    def __init__(self, env_name='main'):\n",
    "        self.viz = Visdom()\n",
    "        self.env = env_name\n",
    "        self.plots = {}\n",
    "    def plot(self, var_name, split_name, title_name, x, y):\n",
    "        if var_name not in self.plots:\n",
    "            self.plots[var_name] = self.viz.line(X=np.array([x,x]), Y=np.array([y,y]), env=self.env, opts=dict(\n",
    "                legend=[split_name],\n",
    "                title=title_name,\n",
    "                xlabel='Epochs',\n",
    "                ylabel=var_name\n",
    "            ))\n",
    "        else:\n",
    "            self.viz.line(X=np.array([x]), Y=np.array([y]), env=self.env, win=self.plots[var_name], name=split_name, update = 'append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The final data corpus folder: C:\\Users\\deepa\\Conversational Agents\\Datasets\\cornell movie-dialogs corpus\n"
     ]
    }
   ],
   "source": [
    "path='C:\\\\Users\\\\deepa\\\\Conversational Agents\\\\Datasets'\n",
    "dataset='cornell movie-dialogs corpus'\n",
    "\n",
    "data_folder=os.path.join(path,dataset)\n",
    "\n",
    "print(\"The final data corpus folder: \"+str(data_folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lines_conversations():\n",
    "    \"\"\"\n",
    "    Loads movie lines and conversations from the dataset.\n",
    "    \n",
    "    data_folder: Destination where conversations and lines are stored.\n",
    "    \n",
    "    movie_lines: Consist of movie lines as given by the dataset.\n",
    "    movie_conversations: Consist of movie conversations as given by the dataset.\n",
    "    \n",
    "    \"\"\"\n",
    "    f=open(os.path.join(data_folder,'movie_lines.txt'),'r')\n",
    "    movie_lines=f.read().splitlines()\n",
    "    f.close()\n",
    "    \n",
    "    f=open(os.path.join(data_folder,'movie_conversations.txt'),'r')\n",
    "    movie_conversations=f.read().splitlines()\n",
    "    f.close()\n",
    "    \n",
    "    return movie_lines,movie_conversations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of distinct lines: 304713\n",
      "Number of conversations: 83097\n",
      "Average Number of lines per conversations: 3.6669554857576085\n",
      "L1045 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ They do not!\n",
      "u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L194', 'L195', 'L196', 'L197']\n"
     ]
    }
   ],
   "source": [
    "movie_lines,movie_conversations=get_lines_conversations()\n",
    "\n",
    "print(\"Number of distinct lines: \"+str(len(movie_lines)))\n",
    "print(\"Number of conversations: \"+str(len(movie_conversations)))\n",
    "print(\"Average Number of lines per conversations: \"+str(len(movie_lines)/len(movie_conversations)))\n",
    "\n",
    "print(movie_lines[0])\n",
    "print(movie_conversations[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadLines(movie_lines,fields):\n",
    "    lines={}\n",
    "    for line in movie_lines:\n",
    "        values=line.split(\" +++$+++ \")\n",
    "        \n",
    "        lineVals={}\n",
    "        \n",
    "#         print(\"values\"+str(len(values)))\n",
    "#         print(\"fields\"+str(len(fields)))\n",
    "              \n",
    "        for i,field in enumerate(fields):\n",
    "            lineVals[field]=values[i]\n",
    "        \n",
    "        lines[lineVals['lineID']]=lineVals\n",
    "    \n",
    "    return lines\n",
    "\n",
    "def loadConversations(movie_conversations,lines,fields):\n",
    "    conversations=[]\n",
    "    \n",
    "    for convo in movie_conversations:\n",
    "        values=convo.split(\" +++$+++ \")\n",
    "        conVals={}\n",
    "       \n",
    "        for i,field in enumerate(fields):\n",
    "            conVals[field]=values[i]\n",
    "        \n",
    "        lineIDs=eval(conVals[\"utteranceIDs\"])\n",
    "        \n",
    "        conVals[\"lines\"]=[]\n",
    "        \n",
    "        for lineID in lineIDs:\n",
    "            conVals[\"lines\"].append(lines[lineID])\n",
    "        conversations.append(conVals)\n",
    "        \n",
    "    return conversations\n",
    "\n",
    "def sentencePairs(conversations):\n",
    "    qr_pairs=[]\n",
    "    \n",
    "    for conversation in conversations:\n",
    "        for i in range(len(conversation[\"lines\"])-1):\n",
    "            query=conversation[\"lines\"][i][\"text\"].strip()\n",
    "            response=conversation[\"lines\"][i+1][\"text\"].strip()\n",
    "            \n",
    "            if query and response:\n",
    "                qr_pairs.append([query,response])\n",
    "        \n",
    "    return qr_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of query-response pairs are: 221282\n"
     ]
    }
   ],
   "source": [
    "lines={}\n",
    "conversations=[]\n",
    "qr_pairs=[]\n",
    "\n",
    "movie_lines_fields=[\"lineID\",\"characterID\",\"movieID\",\"character\",\"text\"]\n",
    "movie_convo_fields=[\"charcaterID\",\"character2ID\",\"movieID\",\"utteranceIDs\"]\n",
    "\n",
    "lines=loadLines(movie_lines,movie_lines_fields)\n",
    "conversations=loadConversations(movie_conversations,lines,movie_convo_fields)\n",
    "qr_pairs=sentencePairs(conversations)\n",
    "\n",
    "print(\"The number of query-response pairs are: \"+str(len(qr_pairs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_Token=0\n",
    "START_Token=1\n",
    "END_Token=2\n",
    "\n",
    "class Vocabulary:\n",
    "    def __init__(self):\n",
    "        self.trimmed=False\n",
    "        self.word2count={}\n",
    "        self.index2word={PAD_Token:\"PAD\",START_Token:\"SOS\",END_Token:\"EOS\"}\n",
    "        self.word2index={\"PAD\":PAD_Token,\"SOS\":START_Token,\"EOS\":END_Token}\n",
    "        self.num_words=3\n",
    "        \n",
    "    def addSentence(self,sentence):\n",
    "        for word in sentence.split(\" \"):\n",
    "            self.addWord(word)\n",
    "    def addWord(self,word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word]=self.num_words\n",
    "            self.index2word[self.num_words]=word\n",
    "            self.word2count[word]=1\n",
    "            self.num_words=self.num_words+1\n",
    "        else:\n",
    "            self.word2count[word]+=1\n",
    "            \n",
    "    def trim(self,min_count):\n",
    "        \n",
    "        if self.trimmed:\n",
    "            return\n",
    "        self.trimmed=True\n",
    "        \n",
    "        keep_words=[]\n",
    "        \n",
    "        for word,freq in self.word2count.items():\n",
    "            if freq>=min_count:\n",
    "                keep_words.append(word)\n",
    "        \n",
    "        self.word2count={}\n",
    "        self.index2word={PAD_Token:\"PAD\",START_Token:\"SOS\",END_Token:\"EOS\"}\n",
    "        self.word2index={\"PAD\":PAD_Token,\"SOS\":START_Token,\"EOS\":END_Token}\n",
    "        self.num_words=3\n",
    "        \n",
    "        for word in keep_words:\n",
    "            self.addWord(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Max_Length=10\n",
    "\n",
    "def normalizeString(s):\n",
    "    s=s.lower().strip()\n",
    "    s=re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s=re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    s=re.sub(r\"\\s+\", r\" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def readVocs(qr_pairs):\n",
    "    \n",
    "    for qr_pair in qr_pairs:\n",
    "        qr_pair[0]=normalizeString(qr_pair[0])\n",
    "        qr_pair[1]=normalizeString(qr_pair[1])\n",
    "    \n",
    "    voc=Vocabulary()\n",
    "    return voc,qr_pairs\n",
    "\n",
    "def filterPair(pair):\n",
    "    return len(pair[0].split(\" \"))<Max_Length and len(pair[1].split(\" \"))<Max_Length\n",
    "\n",
    "def filterPairs(qr_pairs):\n",
    "    return [pair for pair in qr_pairs if filterPair(pair)]\n",
    "\n",
    "def prepareDataset(qr_pairs):\n",
    "    voc, qr_pairs=readVocs(qr_pairs)\n",
    "    qr_pairs=filterPairs(qr_pairs)\n",
    "       \n",
    "    for pair in qr_pairs:\n",
    "        voc.addSentence(pair[0])\n",
    "        voc.addSentence(pair[1])\n",
    "#     print(\"Number\"+str(voc.num_words))\n",
    "    return voc,qr_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc, pairs=prepareDataset(qr_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Min_Count=3\n",
    "\n",
    "def trimRareWords(voc,qr_pairs):\n",
    "    \n",
    "    voc.trim(Min_Count)\n",
    "    keep_pairs=[]\n",
    "    \n",
    "    for pair in qr_pairs:\n",
    "        input_sentence=pair[0]\n",
    "        output_sentence=pair[1]\n",
    "        \n",
    "        keep_input=True\n",
    "        keep_output=True\n",
    "        \n",
    "        for word in input_sentence.split(\" \"):\n",
    "            if word not in voc.word2index:\n",
    "                keep_input=False\n",
    "                break\n",
    "        \n",
    "        for word in output_sentence.split(\" \"):\n",
    "            if word not in voc.word2index:\n",
    "                keep_output=False\n",
    "                break\n",
    "                \n",
    "        if keep_input and keep_output:\n",
    "            keep_pairs.append(pair)\n",
    "            \n",
    "    return keep_pairs\n",
    "\n",
    "pairs=trimRareWords(voc,pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([8, 8, 7, 7, 7, 5, 4, 3, 3, 3])\n",
      "tensor([8, 8, 7, 7, 7, 5, 4, 3, 3, 3])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 1],\n",
      "        [0, 0, 1, 1, 1, 0, 1, 0, 0, 1],\n",
      "        [0, 0, 1, 0, 1, 0, 1, 0, 0, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0]], dtype=torch.uint8)\n",
      "torch.Size([8, 10])\n",
      "tensor([[  34,   25,    7,  318,  197,  188,   92,   50,  266, 1245],\n",
      "        [6326,  200,  424,   42,  117,    4,    7,  881,   95, 4008],\n",
      "        [   7, 1525,   83,   53,  669,    4,  123,    6,    4,   56],\n",
      "        [   6,    4,   63, 3739,   75,    4,   40,    2,    2,   53],\n",
      "        [   2,    2, 1562,    4,   76,    2,   47,    0,    0,  317],\n",
      "        [   0,    0,    4,    2,    4,    0,   45,    0,    0,    4],\n",
      "        [   0,    0,    2,    0,    2,    0,  329,    0,    0,    2],\n",
      "        [   0,    0,    0,    0,    0,    0,   50,    0,    0,    0],\n",
      "        [   0,    0,    0,    0,    0,    0,    6,    0,    0,    0],\n",
      "        [   0,    0,    0,    0,    0,    0,    2,    0,    0,    0]])\n"
     ]
    }
   ],
   "source": [
    "def indexesFromSentence(voc,sentence):\n",
    "    return [voc.word2index[word] for word in sentence.split(\" \")]+[END_Token]\n",
    "\n",
    "def zeroPadding(l,fillValue=PAD_Token):\n",
    "    return list(itertools.zip_longest(*l,fillvalue=fillValue))\n",
    "\n",
    "def binaryMatrix(l,value=PAD_Token):\n",
    "    m=[]\n",
    "    for i,seq in enumerate(l):\n",
    "        m.append([])\n",
    "        for token in seq:\n",
    "            if token==value:\n",
    "                m[i].append(0)\n",
    "            else:\n",
    "                m[i].append(1)\n",
    "        \n",
    "    return m\n",
    "\n",
    "def inputVar(voc,l):\n",
    "    indexes_batch=[indexesFromSentence(voc,sentence) for sentence in l]\n",
    "    lengths=torch.tensor([len(index) for index in indexes_batch])\n",
    "    padList=zeroPadding(indexes_batch)\n",
    "    padVar=torch.LongTensor(padList)\n",
    "    return lengths,padVar\n",
    "\n",
    "def outputVar(voc,l):\n",
    "    indexes_batch=[indexesFromSentence(voc,sentence) for sentence in l]\n",
    "    max_target_len=max([len(index) for index in indexes_batch])\n",
    "    padList=zeroPadding(indexes_batch)\n",
    "    mask=binaryMatrix(padList)\n",
    "    mask=torch.ByteTensor(mask)\n",
    "    padVar=torch.LongTensor(padList)\n",
    "    return max_target_len, mask, padVar\n",
    "\n",
    "def batch2TrainData(voc,pair_batch):\n",
    "    #sort function see \n",
    "    pair_batch.sort(key=lambda x: len(x[0].split(\" \")), reverse=True)\n",
    "\n",
    "    input_batch=[]\n",
    "    output_batch=[]\n",
    "    \n",
    "    for pair in pair_batch:\n",
    "        input_batch.append(pair[0])\n",
    "        output_batch.append(pair[1])\n",
    "    \n",
    "    input_lengths,tokenised_input=inputVar(voc,input_batch)\n",
    "    max_out_length,mask,tokenised_output=outputVar(voc,output_batch)\n",
    "    return tokenised_input,input_lengths,tokenised_output,mask,max_out_length\n",
    "\n",
    "batch_size=10\n",
    "batches=[batch2TrainData(voc,[random.choice(pairs) for _ in range(batch_size)]) for _ in range(10)]\n",
    "input_lengths, tokenised_input, max_out_length, mask, tokenised_output=batches[0]\n",
    "\n",
    "print(tokenised_input)\n",
    "print(tokenised_input)\n",
    "print(mask)\n",
    "print(input_lengths.size())\n",
    "print(max_out_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self,hidden_size,embedding,n_layers=1,dropout=0):\n",
    "        \"\"\"\n",
    "        Encoder module for seq2seq architechture.\n",
    "        \"\"\"\n",
    "    \n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_layers=n_layers\n",
    "        self.hidden_size=hidden_size\n",
    "        \n",
    "        self.embedding=embedding\n",
    "        self.gru=nn.GRU(hidden_size,hidden_size,n_layers,dropout=(0 if n_layers==1 else dropout),bidirectional=True)\n",
    "        \n",
    "    def forward(self,input_seq,input_lengths,hidden=None):\n",
    "        \n",
    "        embedded_input=self.embedding(input_seq)\n",
    "        packed=nn.utils.rnn.pack_padded_sequence(embedded_input,input_lengths)\n",
    "        outputs,hidden=self.gru(packed,hidden)\n",
    "        outputs,_=nn.utils.rnn.pad_packed_sequence(outputs)\n",
    "        \n",
    "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:]\n",
    "        \n",
    "        return outputs,hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Luong attention layer\n",
    "class Attn(torch.nn.Module):\n",
    "    def __init__(self, method, hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        self.method = method\n",
    "        if self.method not in ['dot', 'general', 'concat']:\n",
    "            raise ValueError(self.method, \"is not an appropriate attention method.\")\n",
    "        self.hidden_size = hidden_size\n",
    "        if self.method == 'general':\n",
    "            self.attn = torch.nn.Linear(self.hidden_size, hidden_size)\n",
    "        elif self.method == 'concat':\n",
    "            self.attn = torch.nn.Linear(self.hidden_size * 2, hidden_size)\n",
    "            self.v = torch.nn.Parameter(torch.FloatTensor(hidden_size))\n",
    "\n",
    "    def dot_score(self, hidden, encoder_output):\n",
    "        return torch.sum(hidden * encoder_output, dim=2)\n",
    "\n",
    "    def general_score(self, hidden, encoder_output):\n",
    "        energy = self.attn(encoder_output)\n",
    "        return torch.sum(hidden * energy, dim=2)\n",
    "\n",
    "    def concat_score(self, hidden, encoder_output):\n",
    "        energy = self.attn(torch.cat((hidden.expand(encoder_output.size(0), -1, -1), encoder_output), 2)).tanh()\n",
    "        return torch.sum(self.v * energy, dim=2)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        # Calculate the attention weights (energies) based on the given method\n",
    "        if self.method == 'general':\n",
    "            attn_energies = self.general_score(hidden, encoder_outputs)\n",
    "        elif self.method == 'concat':\n",
    "            attn_energies = self.concat_score(hidden, encoder_outputs)\n",
    "        elif self.method == 'dot':\n",
    "            attn_energies = self.dot_score(hidden, encoder_outputs)\n",
    "\n",
    "        # Transpose max_length and batch_size dimensions\n",
    "        attn_energies = attn_energies.t()\n",
    "\n",
    "        # Return the softmax normalized probability scores (with added dimension)\n",
    "        return F.softmax(attn_energies, dim=1).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LuongAttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, attn_model, embedding, hidden_size, output_size, n_layers=1, dropout=0.1):\n",
    "        super(LuongAttnDecoderRNN, self).__init__()\n",
    "\n",
    "        # Keep for reference\n",
    "        self.attn_model = attn_model\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Define layers\n",
    "        self.embedding = embedding\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers == 1 else dropout))\n",
    "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "        self.attn = Attn(attn_model, hidden_size)\n",
    "\n",
    "    def forward(self, input_step, last_hidden, encoder_outputs):\n",
    "        # Note: we run this one step (word) at a time\n",
    "        # Get embedding of current input word\n",
    "        embedded = self.embedding(input_step)\n",
    "        embedded = self.embedding_dropout(embedded)\n",
    "        # Forward through unidirectional GRU\n",
    "        rnn_output, hidden = self.gru(embedded, last_hidden)\n",
    "        # Calculate attention weights from the current GRU output\n",
    "        attn_weights = self.attn(rnn_output, encoder_outputs)\n",
    "        # Multiply attention weights to encoder outputs to get new \"weighted sum\" context vector\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))\n",
    "        # Concatenate weighted context vector and GRU output using Luong eq. 5\n",
    "        rnn_output = rnn_output.squeeze(0)\n",
    "        context = context.squeeze(1)\n",
    "        concat_input = torch.cat((rnn_output, context), 1)\n",
    "        concat_output = torch.tanh(self.concat(concat_input))\n",
    "        # Predict next word using Luong eq. 6\n",
    "        output = self.out(concat_output)\n",
    "        output = F.softmax(output, dim=1)\n",
    "        # Return output and final hidden state\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maskNLLLoss(inp, target, mask):\n",
    "    nTotal = mask.sum()\n",
    "#     scores,decoder_res=torch.max(inp,dim=1)\n",
    "#     for i in range(64):\n",
    "#         print(\"Target: \"+str(target[i])+\"predicted: \"+str(decoder_res[i]))\n",
    "    crossEntropy = -torch.log(torch.gather(inp, 1, target.view(-1, 1)).squeeze(1))\n",
    "    loss = crossEntropy.masked_select(mask).mean()\n",
    "    loss = loss.to(device)\n",
    "    return loss, nTotal.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_grad_flow(named_parameters):\n",
    "    \"\"\"\n",
    "        Plotting gradient flow across various layers\n",
    "        Thanks to: https://discuss.pytorch.org/t/check-gradient-flow-in-network/15063/2\n",
    "    \"\"\"   \n",
    "    ave_grads = []\n",
    "    layers = []\n",
    "    for n, p in named_parameters:\n",
    "        if(p.requires_grad) and (\"bias\" not in n):\n",
    "            layers.append(n)\n",
    "            ave_grads.append(p.grad.abs().mean())\n",
    "    plt.plot(ave_grads, alpha=0.3, color=\"b\")\n",
    "    plt.hlines(0, 0, len(ave_grads)+1, linewidth=1, color=\"k\" )\n",
    "    plt.xticks(range(0,len(ave_grads), 1), layers, rotation=\"vertical\")\n",
    "    plt.xlim(xmin=0, xmax=len(ave_grads))\n",
    "    plt.xlabel(\"Layers\")\n",
    "    plt.ylabel(\"average gradient\")\n",
    "    plt.title(\"Gradient flow\")\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH=10\n",
    "def train(input_variable,lengths,target_variable,mask,max_target_len,encoder,decoder,embedding,\n",
    "          encoder_optimizer,decoder_optimizer,batch_size,clip,max_length=MAX_LENGTH):\n",
    "    \n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    \n",
    "    input_variable=torch.tensor(input_variable).to(device)\n",
    "    lengths=lengths.to(device)\n",
    "    target_variable=torch.tensor(target_variable).to(device)\n",
    "    mask=mask.to(device)\n",
    "    \n",
    "    loss=0\n",
    "    print_losses=[]\n",
    "    n_totals=0\n",
    "    \n",
    "    encoder_outputs, encoder_hidden=encoder(input_variable,lengths)\n",
    "    \n",
    "    decoder_input=torch.LongTensor([[START_Token for _ in range(batch_size)]])\n",
    "    decoder_input=decoder_input.to(device)\n",
    "    use_teacher_forcing=True #if random.random()<teacher_forcing_ratio else False\n",
    "    \n",
    "    decoder_hidden=encoder_hidden[:decoder.n_layers]\n",
    "    \n",
    "    if use_teacher_forcing:\n",
    "        \n",
    "        for t in range(max_target_len):\n",
    "            decoder_output,decoder_hidden=decoder(decoder_input,decoder_hidden,encoder_outputs)\n",
    "            \n",
    "            decoder_input=target_variable[t].view(1,-1)\n",
    "            \n",
    "            mask_loss,nTotal=maskNLLLoss(decoder_output,target_variable[t],mask[t])\n",
    "            loss+=mask_loss\n",
    "            print_losses.append(mask_loss.item()*nTotal)\n",
    "            n_totals+=nTotal\n",
    "            \n",
    "    else:\n",
    "        \n",
    "        for t in range(max_target_len):\n",
    "            decoder_output,decoder_hidden=decoder(decoder_input,decoder_hidden,encoder_outputs)\n",
    "            \n",
    "            _,topi=decoder_output.topk(1)\n",
    "            decoder_input=torch.LongTensor([[topi[i][0] for i in range(batch_size)]])\n",
    "            decoder_input=decoder_input.to(device)\n",
    "            \n",
    "            mask_loss,nTotal=maskNLLLoss(decoder_output,target_variable[t],mask[t])\n",
    "            loss+=mask_loss\n",
    "            print_losses.append(mask_loss.item()*nTotal)\n",
    "            n_totals+=nTotal\n",
    "            \n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    _=nn.utils.clip_grad_norm(encoder.parameters(),clip)\n",
    "    _=nn.utils.clip_grad_norm(decoder.parameters(),clip)\n",
    "    \n",
    "    \n",
    "    plot_grad_flow(encoder.named_parameters())\n",
    "    plot_grad_flow(decoder.named_parameters())\n",
    "    \n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    return sum(print_losses)/n_totals\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(model_name,voc,pairs,encoder,decoder,encoder_optimizer,decoder_optimizer,\n",
    "               embedding,encoder_n_layers,decoder_n_layers,save_dir,n_iteration,batch_size,print_every,\n",
    "               save_every,clip,corpus_name,loadFileName):\n",
    "    \n",
    "    training_batches=[batch2TrainData(voc,[random.choice(pairs) for _ in range(batch_size)]) for _ in range(n_iteration)]\n",
    "    \n",
    "    start_iteration=1\n",
    "    print_loss=0\n",
    "    \n",
    "    if loadFileName:\n",
    "        start_iteration=checkpoint['iteration']+1\n",
    "        \n",
    "    for iteration in range(start_iteration,n_iteration):\n",
    "        training_batch=training_batches[iteration-1]\n",
    "        \n",
    "        input_variable,lengths,target_variable,mask,max_target_len=training_batch\n",
    "#         print(\"Length of lengths: \"+str(lengths.size()))\n",
    "        loss=train(input_variable,lengths,target_variable,mask,max_target_len,encoder,decoder,embedding,\n",
    "                   encoder_optimizer,decoder_optimizer,batch_size,clip)\n",
    "        \n",
    "        print_loss+=loss\n",
    "        \n",
    "        if iteration%print_every==0:\n",
    "            print_loss_avg=print_loss/print_every\n",
    "            plotter.plot('loss','train','epoch vs loss',iteration,print_loss_avg)\n",
    "            print(\"Iteration: \"+str(iteration)+\"Loss: \"+str(print_loss_avg))\n",
    "            print_loss=0\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreedySearchDecoder(nn.Module):\n",
    "    \n",
    "    def __init__(self,encoder,decoder):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder=encoder\n",
    "        self.decoder=decoder\n",
    "        \n",
    "    def forward(self,input_seq,input_length,max_length):\n",
    "        \n",
    "        encoder_outputs,encoder_hidden=self.encoder(input_seq,input_length)\n",
    "        \n",
    "        decoder_hidden=encoder_hidden[:decoder.n_layers]\n",
    "        print(\"Decoder hidden state: \"+str(decoder_hidden))\n",
    "        decoder_input=torch.ones(1,1,device=device,dtype=torch.long)*START_Token\n",
    "        \n",
    "        \n",
    "        print(\"Decoder's Input: \"+str(decoder_input))\n",
    "        all_tokens=torch.zeros([0],device=device,dtype=torch.long)\n",
    "        all_scores=torch.zeros([0],device=device)\n",
    "        \n",
    "        for _ in range(max_length):\n",
    "            \n",
    "            decoder_output,decoder_hidden=self.decoder(decoder_input,decoder_hidden,encoder_outputs)\n",
    "            print(\"Decoder Output: \"+str(decoder_output))\n",
    "            decoder_scores,decoder_input=torch.max(decoder_output,dim=1)\n",
    "            all_scores=torch.cat((all_scores,decoder_scores),dim=0)\n",
    "            all_tokens=torch.cat((all_tokens,decoder_input),dim=0)\n",
    "            \n",
    "            decoder_input=torch.unsqueeze(decoder_input,0)\n",
    "            \n",
    "        return all_tokens, all_scores\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, searcher,voc,sentence,max_length=MAX_LENGTH):\n",
    "    \n",
    "    index_batch=[indexesFromSentence(voc,sentence)]\n",
    "    print(\"Indexed sentence: \"+str(index_batch))\n",
    "    lengths=torch.tensor([len(index) for index in index_batch])\n",
    "    print(\"The Lengths tensor: \"+str(lengths))\n",
    "    input_batch=torch.LongTensor(index_batch).transpose(0,1)\n",
    "    \n",
    "    input_batch=input_batch.to(device)\n",
    "    lengths=lengths.to(device)\n",
    "    \n",
    "    tokens, scores=searcher(input_batch,lengths,max_length)\n",
    "    print(\"The tokens: \"+str(tokens))\n",
    "    decoded_words=[voc.index2word[token.item()] for token in tokens]\n",
    "    return decoded_words\n",
    "\n",
    "def evaluateInput(encoder,decoder,searcher,voc):\n",
    "    input_sentence=''\n",
    "    while True:\n",
    "        try:\n",
    "            input_sentence=input('Human> ')\n",
    "            \n",
    "            if input_sentence=='q' or input_sentence=='quit':\n",
    "                break\n",
    "            input_sentence=normalizeString(input_sentence)\n",
    "            print(\"The Normalized Input Sentence: \"+str(input_sentence))\n",
    "            output_words=evaluate(encoder,decoder,searcher,voc,input_sentence)\n",
    "            \n",
    "            output_words[:]=[x for x in output_words if not(x==\"PAD\" or x==\"EOS\")]\n",
    "            print(\"Bot:\",\" \".join(output_words))\n",
    "            \n",
    "        except KeyError:\n",
    "            print(\"Unknown Word\")\n",
    "            \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='seq2seq'\n",
    "corpus_name='movies'\n",
    "attn_model='dot'\n",
    "encoder_n_layers=2\n",
    "decoder_n_layers=2\n",
    "hidden_size=500\n",
    "dropout=0.1\n",
    "batch_size=64\n",
    "\n",
    "checkpoint_iter=4000\n",
    "loadFilename=None\n",
    "\n",
    "embedding=nn.Embedding(voc.num_words,hidden_size)\n",
    "encoder=EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)\n",
    "decoder = LuongAttnDecoderRNN(attn_model, embedding, hidden_size, voc.num_words, decoder_n_layers, dropout)\n",
    "\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n",
      "C:\\Users\\deepa\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n",
      "C:\\Users\\deepa\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "C:\\Users\\deepa\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:54: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "C:\\Users\\deepa\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:55: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1Loss: 3.5997518906225414\n",
      "Iteration: 2Loss: 3.3554110587118884\n",
      "Iteration: 3Loss: 3.5820662217926227\n",
      "Iteration: 4Loss: 3.5452680810356973\n",
      "Iteration: 5Loss: 3.648831555188583\n",
      "Iteration: 6Loss: 3.1055710202001117\n",
      "Iteration: 7Loss: 3.198326212817162\n",
      "Iteration: 8Loss: 3.280131420792966\n",
      "Iteration: 9Loss: 3.599402393454029\n",
      "Iteration: 10Loss: 3.5350134205746944\n",
      "Iteration: 11Loss: 3.2174046454106633\n",
      "Iteration: 12Loss: 3.1067036025087775\n",
      "Iteration: 13Loss: 3.2800029861829914\n",
      "Iteration: 14Loss: 3.700256017478639\n",
      "Iteration: 15Loss: 3.3486608095871384\n",
      "Iteration: 16Loss: 3.454937042584913\n",
      "Iteration: 17Loss: 3.602193617985325\n",
      "Iteration: 18Loss: 3.553971380069852\n",
      "Iteration: 19Loss: 3.5663109702031934\n",
      "Iteration: 20Loss: 3.4387375965376643\n",
      "Iteration: 21Loss: 3.5565052405389816\n",
      "Iteration: 22Loss: 3.4135396705411467\n",
      "Iteration: 23Loss: 3.4127587005297344\n",
      "Iteration: 24Loss: 3.7140865906561134\n",
      "Iteration: 25Loss: 3.353567130182467\n",
      "Iteration: 26Loss: 3.656408807486654\n",
      "Iteration: 27Loss: 3.3080096068081457\n",
      "Iteration: 28Loss: 3.5713203727948266\n",
      "Iteration: 29Loss: 3.4477922213125494\n",
      "Iteration: 30Loss: 3.5710229422683484\n",
      "Iteration: 31Loss: 3.4493779294830276\n",
      "Iteration: 32Loss: 3.31836118570011\n",
      "Iteration: 33Loss: 3.497017725771647\n",
      "Iteration: 34Loss: 3.7022709160878056\n",
      "Iteration: 35Loss: 3.224010131572588\n",
      "Iteration: 36Loss: 3.1592687817897587\n",
      "Iteration: 37Loss: 3.178402817018685\n",
      "Iteration: 38Loss: 3.659188702426575\n",
      "Iteration: 39Loss: 3.6605959529966188\n",
      "Iteration: 40Loss: 3.5039825648358414\n",
      "Iteration: 41Loss: 3.440269315840535\n",
      "Iteration: 42Loss: 3.7648117888772314\n",
      "Iteration: 43Loss: 3.8020698034423\n",
      "Iteration: 44Loss: 3.512476624940727\n",
      "Iteration: 45Loss: 3.4541357071729513\n",
      "Iteration: 46Loss: 3.6382063821517963\n",
      "Iteration: 47Loss: 3.633440431028227\n",
      "Iteration: 48Loss: 3.612359778746794\n",
      "Iteration: 49Loss: 3.3658728301525116\n",
      "Iteration: 50Loss: 3.4884465059460137\n",
      "Iteration: 51Loss: 3.7216162679366547\n",
      "Iteration: 52Loss: 3.731683817962159\n",
      "Iteration: 53Loss: 3.613249521341259\n",
      "Iteration: 54Loss: 3.509696360399528\n",
      "Iteration: 55Loss: 3.2753196345723192\n",
      "Iteration: 56Loss: 3.6428724105256456\n",
      "Iteration: 57Loss: 3.372684921175775\n",
      "Iteration: 58Loss: 3.168786430225262\n",
      "Iteration: 59Loss: 3.3212603783155026\n",
      "Iteration: 60Loss: 3.4135458054516135\n",
      "Iteration: 61Loss: 3.6481854150723665\n",
      "Iteration: 62Loss: 3.6805932064913214\n",
      "Iteration: 63Loss: 3.3460708131757757\n",
      "Iteration: 64Loss: 3.4436683889690767\n",
      "Iteration: 65Loss: 3.705272382378616\n",
      "Iteration: 66Loss: 3.306338445095868\n",
      "Iteration: 67Loss: 3.65507633533718\n",
      "Iteration: 68Loss: 3.5330248241880327\n",
      "Iteration: 69Loss: 3.5208422191897215\n",
      "Iteration: 70Loss: 3.437133829823233\n",
      "Iteration: 71Loss: 3.235217207208065\n",
      "Iteration: 72Loss: 3.386225116897892\n",
      "Iteration: 73Loss: 3.22651896540386\n",
      "Iteration: 74Loss: 3.395372520289139\n",
      "Iteration: 75Loss: 3.54279861039489\n",
      "Iteration: 76Loss: 3.569556834682488\n",
      "Iteration: 77Loss: 3.5437068633995756\n",
      "Iteration: 78Loss: 3.5199801905446075\n",
      "Iteration: 79Loss: 3.774794258306056\n",
      "Iteration: 80Loss: 3.366707980986207\n",
      "Iteration: 81Loss: 3.501026334780573\n",
      "Iteration: 82Loss: 3.425812186477706\n",
      "Iteration: 83Loss: 3.6116688981405667\n",
      "Iteration: 84Loss: 3.3788504654156832\n",
      "Iteration: 85Loss: 3.574100653672727\n",
      "Iteration: 86Loss: 3.4408438973107924\n",
      "Iteration: 87Loss: 3.39677709155518\n",
      "Iteration: 88Loss: 3.3645476747122136\n",
      "Iteration: 89Loss: 3.4318775966814465\n",
      "Iteration: 90Loss: 3.536443101729269\n",
      "Iteration: 91Loss: 3.50323820430055\n",
      "Iteration: 92Loss: 3.599610377202311\n",
      "Iteration: 93Loss: 3.8120323086176255\n",
      "Iteration: 94Loss: 3.7363104475758124\n",
      "Iteration: 95Loss: 3.3404605824342393\n",
      "Iteration: 96Loss: 3.7353053646325374\n",
      "Iteration: 97Loss: 3.2238488556267932\n",
      "Iteration: 98Loss: 3.3547065294220157\n",
      "Iteration: 99Loss: 3.4373642406901537\n",
      "Iteration: 100Loss: 3.4143807661241374\n",
      "Iteration: 101Loss: 3.642147021895403\n",
      "Iteration: 102Loss: 3.6940001075647753\n",
      "Iteration: 103Loss: 3.543547014628777\n",
      "Iteration: 104Loss: 3.355206999856846\n",
      "Iteration: 105Loss: 3.579624015800976\n",
      "Iteration: 106Loss: 3.2583525233104993\n",
      "Iteration: 107Loss: 3.3386121665179314\n",
      "Iteration: 108Loss: 3.473602340034576\n",
      "Iteration: 109Loss: 3.505134003343148\n",
      "Iteration: 110Loss: 3.3591920558106283\n",
      "Iteration: 111Loss: 3.3349827873542552\n",
      "Iteration: 112Loss: 3.3175856032265494\n",
      "Iteration: 113Loss: 3.713139448592912\n",
      "Iteration: 114Loss: 3.2471688208822163\n",
      "Iteration: 115Loss: 3.457295147986348\n",
      "Iteration: 116Loss: 3.497533611976709\n",
      "Iteration: 117Loss: 3.623718170552073\n",
      "Iteration: 118Loss: 3.4584850839756687\n",
      "Iteration: 119Loss: 3.5443641747786807\n",
      "Iteration: 120Loss: 3.330186960498281\n",
      "Iteration: 121Loss: 3.2356642457523868\n",
      "Iteration: 122Loss: 3.157422307871848\n",
      "Iteration: 123Loss: 3.447325668086468\n",
      "Iteration: 124Loss: 3.3250058684743804\n",
      "Iteration: 125Loss: 3.6002424120467724\n",
      "Iteration: 126Loss: 3.53364872522245\n",
      "Iteration: 127Loss: 3.5595462635936563\n",
      "Iteration: 128Loss: 3.5214493146029913\n",
      "Iteration: 129Loss: 3.5328240746521518\n",
      "Iteration: 130Loss: 3.491111132403973\n",
      "Iteration: 131Loss: 3.572603135983089\n",
      "Iteration: 132Loss: 3.5933406532669676\n",
      "Iteration: 133Loss: 3.1722562730854817\n",
      "Iteration: 134Loss: 3.7597160492580777\n",
      "Iteration: 135Loss: 3.270825239895892\n",
      "Iteration: 136Loss: 3.492320940533273\n",
      "Iteration: 137Loss: 3.4566914053913873\n",
      "Iteration: 138Loss: 3.4800334497409575\n",
      "Iteration: 139Loss: 3.2079273037240874\n",
      "Iteration: 140Loss: 3.3097041218272625\n",
      "Iteration: 141Loss: 3.617856005844697\n",
      "Iteration: 142Loss: 3.105150430016996\n",
      "Iteration: 143Loss: 3.5046068615823365\n",
      "Iteration: 144Loss: 3.6938897233891796\n",
      "Iteration: 145Loss: 3.339311699263257\n",
      "Iteration: 146Loss: 3.4519349133273884\n",
      "Iteration: 147Loss: 3.458775562052268\n",
      "Iteration: 148Loss: 3.233852255005891\n",
      "Iteration: 149Loss: 3.443706287957331\n",
      "Iteration: 150Loss: 3.4912379021934896\n",
      "Iteration: 151Loss: 3.622237751906055\n",
      "Iteration: 152Loss: 3.510523742315483\n",
      "Iteration: 153Loss: 3.626696442945224\n",
      "Iteration: 154Loss: 3.4443830457503597\n",
      "Iteration: 155Loss: 3.299682781731993\n",
      "Iteration: 156Loss: 3.40420351880554\n",
      "Iteration: 157Loss: 3.5407472621349982\n",
      "Iteration: 158Loss: 3.556791218814298\n",
      "Iteration: 159Loss: 3.6326602620347113\n",
      "Iteration: 160Loss: 3.337968316815433\n",
      "Iteration: 161Loss: 3.341802904387508\n",
      "Iteration: 162Loss: 3.3888346836427035\n",
      "Iteration: 163Loss: 3.4687747697742797\n",
      "Iteration: 164Loss: 3.3124338503389117\n",
      "Iteration: 165Loss: 3.6036089076957802\n",
      "Iteration: 166Loss: 3.5001455509513235\n",
      "Iteration: 167Loss: 3.5416290331444635\n",
      "Iteration: 168Loss: 3.19605052464568\n",
      "Iteration: 169Loss: 3.647921636869739\n",
      "Iteration: 170Loss: 3.3345757933697784\n",
      "Iteration: 171Loss: 3.3361962858891245\n",
      "Iteration: 172Loss: 3.7790161502768544\n",
      "Iteration: 173Loss: 3.534173150073717\n",
      "Iteration: 174Loss: 3.4071298174478195\n",
      "Iteration: 175Loss: 3.665195509589013\n",
      "Iteration: 176Loss: 3.230993954292158\n",
      "Iteration: 177Loss: 3.6728140501932582\n",
      "Iteration: 178Loss: 3.6253717659555904\n",
      "Iteration: 179Loss: 3.4685425919043014\n",
      "Iteration: 180Loss: 3.5193122842120323\n",
      "Iteration: 181Loss: 3.410710316066928\n",
      "Iteration: 182Loss: 3.3650147126058565\n",
      "Iteration: 183Loss: 3.1497849138771614\n",
      "Iteration: 184Loss: 3.0769068984013135\n",
      "Iteration: 185Loss: 3.453187411484522\n",
      "Iteration: 186Loss: 3.5779220561155736\n",
      "Iteration: 187Loss: 3.33308167478547\n",
      "Iteration: 188Loss: 3.6325100634657024\n",
      "Iteration: 189Loss: 3.3057087448799067\n",
      "Iteration: 190Loss: 2.9305421107986445\n",
      "Iteration: 191Loss: 3.233415257109865\n",
      "Iteration: 192Loss: 3.282901943193217\n",
      "Iteration: 193Loss: 3.4966354476789254\n",
      "Iteration: 194Loss: 3.500716871321577\n",
      "Iteration: 195Loss: 3.2589849149369443\n",
      "Iteration: 196Loss: 3.618181056127564\n",
      "Iteration: 197Loss: 3.2377454039545506\n",
      "Iteration: 198Loss: 3.2427620574516434\n",
      "Iteration: 199Loss: 3.481349097717052\n",
      "Iteration: 200Loss: 3.3274575120749628\n",
      "Iteration: 201Loss: 3.257331876932336\n",
      "Iteration: 202Loss: 3.284651328425918\n",
      "Iteration: 203Loss: 3.5824933233105134\n",
      "Iteration: 204Loss: 3.3531888010826756\n",
      "Iteration: 205Loss: 3.2970174909663874\n",
      "Iteration: 206Loss: 3.648778258696636\n",
      "Iteration: 207Loss: 3.527181431235269\n",
      "Iteration: 208Loss: 3.560752886926403\n",
      "Iteration: 209Loss: 3.532588760368526\n",
      "Iteration: 210Loss: 3.3091921462045364\n",
      "Iteration: 211Loss: 3.4913695565100467\n",
      "Iteration: 212Loss: 3.339577915345101\n",
      "Iteration: 213Loss: 3.307441517563876\n",
      "Iteration: 214Loss: 3.437854471412443\n",
      "Iteration: 215Loss: 3.5927577894318308\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 216Loss: 3.2762979443841136\n",
      "Iteration: 217Loss: 3.1090352033288817\n",
      "Iteration: 218Loss: 3.470305718184217\n",
      "Iteration: 219Loss: 3.6443908693766516\n",
      "Iteration: 220Loss: 3.3847555812974552\n",
      "Iteration: 221Loss: 3.426909918095962\n",
      "Iteration: 222Loss: 3.3180753216632946\n",
      "Iteration: 223Loss: 3.801438786258325\n",
      "Iteration: 224Loss: 3.335604577835721\n",
      "Iteration: 225Loss: 3.2605798370740793\n",
      "Iteration: 226Loss: 3.4099585817142057\n",
      "Iteration: 227Loss: 3.507942126155263\n",
      "Iteration: 228Loss: 3.662973144738223\n",
      "Iteration: 229Loss: 3.180850521882588\n",
      "Iteration: 230Loss: 3.529234427889922\n",
      "Iteration: 231Loss: 3.2496800042879888\n",
      "Iteration: 232Loss: 3.599809295978744\n",
      "Iteration: 233Loss: 3.2456708918135053\n",
      "Iteration: 234Loss: 3.456047342493061\n",
      "Iteration: 235Loss: 3.498331991287127\n",
      "Iteration: 236Loss: 3.5530464936143313\n",
      "Iteration: 237Loss: 3.6432375796613194\n",
      "Iteration: 238Loss: 3.459261662670626\n",
      "Iteration: 239Loss: 3.4964381885365583\n",
      "Iteration: 240Loss: 3.726737765808437\n",
      "Iteration: 241Loss: 3.7527980515148474\n",
      "Iteration: 242Loss: 3.5110493759507113\n",
      "Iteration: 243Loss: 3.592499214106148\n",
      "Iteration: 244Loss: 3.413622407334866\n",
      "Iteration: 245Loss: 3.469546963617665\n",
      "Iteration: 246Loss: 3.5065883233620068\n",
      "Iteration: 247Loss: 3.429811115386559\n",
      "Iteration: 248Loss: 3.46652700304422\n",
      "Iteration: 249Loss: 3.539197787129301\n",
      "Iteration: 250Loss: 3.37554832714532\n",
      "Iteration: 251Loss: 3.4767372008385276\n",
      "Iteration: 252Loss: 3.2310375405507457\n",
      "Iteration: 253Loss: 3.3346303953394214\n",
      "Iteration: 254Loss: 3.2993929219851994\n",
      "Iteration: 255Loss: 3.5443598172299704\n",
      "Iteration: 256Loss: 3.3886302816343585\n",
      "Iteration: 257Loss: 3.2505402341561265\n",
      "Iteration: 258Loss: 3.38063768658514\n",
      "Iteration: 259Loss: 3.25239053375802\n",
      "Iteration: 260Loss: 3.3645525941971557\n",
      "Iteration: 261Loss: 3.5028848696553534\n",
      "Iteration: 262Loss: 3.236163236487879\n",
      "Iteration: 263Loss: 3.4426407142706528\n",
      "Iteration: 264Loss: 3.3134902158505\n",
      "Iteration: 265Loss: 3.4192992113712357\n",
      "Iteration: 266Loss: 3.377054513106501\n",
      "Iteration: 267Loss: 3.3189885707474067\n",
      "Iteration: 268Loss: 3.600907914717998\n",
      "Iteration: 269Loss: 3.4728986235916035\n",
      "Iteration: 270Loss: 3.4262945693210547\n",
      "Iteration: 271Loss: 3.394133086826469\n",
      "Iteration: 272Loss: 3.3437009426831668\n",
      "Iteration: 273Loss: 3.184958234620891\n",
      "Iteration: 274Loss: 3.2299759338414558\n",
      "Iteration: 275Loss: 3.3309590518527497\n",
      "Iteration: 276Loss: 3.3691076216396767\n",
      "Iteration: 277Loss: 3.373870076894147\n",
      "Iteration: 278Loss: 3.5778033163783234\n",
      "Iteration: 279Loss: 3.2126536752302153\n",
      "Iteration: 280Loss: 3.574983180101429\n",
      "Iteration: 281Loss: 3.175444840016163\n",
      "Iteration: 282Loss: 3.4379784666021047\n",
      "Iteration: 283Loss: 3.2925681474898467\n",
      "Iteration: 284Loss: 3.3613837205049695\n",
      "Iteration: 285Loss: 3.3196911233768778\n",
      "Iteration: 286Loss: 3.5083926432740817\n",
      "Iteration: 287Loss: 3.4276062592944245\n",
      "Iteration: 288Loss: 3.245737676775411\n",
      "Iteration: 289Loss: 3.2779824587656976\n",
      "Iteration: 290Loss: 3.4153751140559905\n",
      "Iteration: 291Loss: 3.234584407098591\n",
      "Iteration: 292Loss: 3.399532335078448\n",
      "Iteration: 293Loss: 3.5966438177552593\n",
      "Iteration: 294Loss: 3.731170459802855\n",
      "Iteration: 295Loss: 3.5131140099587896\n",
      "Iteration: 296Loss: 3.278244134165685\n",
      "Iteration: 297Loss: 3.3898450112543426\n",
      "Iteration: 298Loss: 3.321563543059731\n",
      "Iteration: 299Loss: 3.591278802303876\n",
      "Iteration: 300Loss: 3.2289715438961664\n",
      "Iteration: 301Loss: 3.4030325836673634\n",
      "Iteration: 302Loss: 3.5134468350261066\n",
      "Iteration: 303Loss: 3.3395807524018153\n",
      "Iteration: 304Loss: 3.311810240335298\n",
      "Iteration: 305Loss: 3.2618592945095535\n",
      "Iteration: 306Loss: 3.429039590580829\n",
      "Iteration: 307Loss: 3.3504049166813696\n",
      "Iteration: 308Loss: 3.2240612061238303\n",
      "Iteration: 309Loss: 3.159557041874575\n",
      "Iteration: 310Loss: 3.6015764983091265\n",
      "Iteration: 311Loss: 3.2410330736751076\n",
      "Iteration: 312Loss: 3.3694170041877216\n",
      "Iteration: 313Loss: 3.2923820520439113\n",
      "Iteration: 314Loss: 3.4303667933723023\n",
      "Iteration: 315Loss: 3.707587632159871\n",
      "Iteration: 316Loss: 3.5204365628002656\n",
      "Iteration: 317Loss: 3.352001070603728\n",
      "Iteration: 318Loss: 3.434624227738937\n",
      "Iteration: 319Loss: 3.2235616305698978\n",
      "Iteration: 320Loss: 3.0943134621113795\n",
      "Iteration: 321Loss: 3.3636806692662686\n",
      "Iteration: 322Loss: 3.210159979250074\n",
      "Iteration: 323Loss: 3.539441337905881\n",
      "Iteration: 324Loss: 3.4807170011754174\n",
      "Iteration: 325Loss: 3.4984552895203436\n",
      "Iteration: 326Loss: 3.2780022186598754\n",
      "Iteration: 327Loss: 3.3544147332385643\n",
      "Iteration: 328Loss: 3.541602436228718\n",
      "Iteration: 329Loss: 3.2703235209723442\n",
      "Iteration: 330Loss: 3.4649966492949758\n",
      "Iteration: 331Loss: 3.3778761259571333\n",
      "Iteration: 332Loss: 3.3748572336574743\n",
      "Iteration: 333Loss: 3.4654503258586327\n",
      "Iteration: 334Loss: 3.310324280131921\n",
      "Iteration: 335Loss: 3.2139924940774622\n",
      "Iteration: 336Loss: 3.221091376975216\n",
      "Iteration: 337Loss: 3.4612052512218856\n",
      "Iteration: 338Loss: 3.2859808553469576\n",
      "Iteration: 339Loss: 3.4493932531229654\n",
      "Iteration: 340Loss: 3.1388610606585505\n",
      "Iteration: 341Loss: 3.3571573108238137\n",
      "Iteration: 342Loss: 3.337299623174371\n",
      "Iteration: 343Loss: 3.470970744430681\n",
      "Iteration: 344Loss: 3.1497053262357255\n",
      "Iteration: 345Loss: 3.3761834025534547\n",
      "Iteration: 346Loss: 3.053349927584769\n",
      "Iteration: 347Loss: 3.033278931186759\n",
      "Iteration: 348Loss: 3.246779418103397\n",
      "Iteration: 349Loss: 3.4380390670983445\n",
      "Iteration: 350Loss: 3.337973917351205\n",
      "Iteration: 351Loss: 3.1949593279588724\n",
      "Iteration: 352Loss: 3.3855821473176397\n",
      "Iteration: 353Loss: 3.254550074448671\n",
      "Iteration: 354Loss: 3.198246707563563\n",
      "Iteration: 355Loss: 3.2801779048026676\n",
      "Iteration: 356Loss: 3.462650446863976\n",
      "Iteration: 357Loss: 3.5066135063408894\n",
      "Iteration: 358Loss: 3.292452029379192\n",
      "Iteration: 359Loss: 3.4209949501975454\n",
      "Iteration: 360Loss: 3.4402730851926933\n",
      "Iteration: 361Loss: 3.2975512594701004\n",
      "Iteration: 362Loss: 3.370616450383552\n",
      "Iteration: 363Loss: 3.1086627376946407\n",
      "Iteration: 364Loss: 3.334840859277334\n",
      "Iteration: 365Loss: 3.291479324304301\n",
      "Iteration: 366Loss: 3.382125896202296\n",
      "Iteration: 367Loss: 3.1457441181112227\n",
      "Iteration: 368Loss: 3.330010911483211\n",
      "Iteration: 369Loss: 3.441248793002051\n",
      "Iteration: 370Loss: 3.2479761509012572\n",
      "Iteration: 371Loss: 3.5408126425881155\n",
      "Iteration: 372Loss: 3.373608936164105\n",
      "Iteration: 373Loss: 3.712792621095408\n",
      "Iteration: 374Loss: 3.546760495845674\n",
      "Iteration: 375Loss: 3.5220861450685486\n",
      "Iteration: 376Loss: 3.654606114068598\n",
      "Iteration: 377Loss: 3.2431838698201365\n",
      "Iteration: 378Loss: 3.115795163219755\n",
      "Iteration: 379Loss: 3.4718423157593397\n",
      "Iteration: 380Loss: 3.375221648486909\n",
      "Iteration: 381Loss: 3.587379772331929\n",
      "Iteration: 382Loss: 3.3386597041545847\n",
      "Iteration: 383Loss: 3.3626026900175487\n",
      "Iteration: 384Loss: 3.585025159209624\n",
      "Iteration: 385Loss: 3.5935085633888186\n",
      "Iteration: 386Loss: 3.211338460415916\n",
      "Iteration: 387Loss: 3.5267618762561463\n",
      "Iteration: 388Loss: 3.4516506628885084\n",
      "Iteration: 389Loss: 3.30436348032197\n",
      "Iteration: 390Loss: 3.3862891059262856\n",
      "Iteration: 391Loss: 3.302264840335067\n",
      "Iteration: 392Loss: 3.252803459192458\n",
      "Iteration: 393Loss: 3.4387102350969063\n",
      "Iteration: 394Loss: 3.489947641295015\n",
      "Iteration: 395Loss: 3.622951755511392\n",
      "Iteration: 396Loss: 3.410865477523837\n",
      "Iteration: 397Loss: 3.1029491379554326\n",
      "Iteration: 398Loss: 3.303857801642489\n",
      "Iteration: 399Loss: 3.13861356925841\n",
      "Iteration: 400Loss: 3.545961652234726\n",
      "Iteration: 401Loss: 3.6315152089708276\n",
      "Iteration: 402Loss: 3.125863720000618\n",
      "Iteration: 403Loss: 3.336700333434676\n",
      "Iteration: 404Loss: 3.516480970942943\n",
      "Iteration: 405Loss: 3.3926432891514815\n",
      "Iteration: 406Loss: 3.284204746784788\n",
      "Iteration: 407Loss: 3.5270889859399963\n",
      "Iteration: 408Loss: 3.6591736691629637\n",
      "Iteration: 409Loss: 3.241506870208814\n",
      "Iteration: 410Loss: 3.418197297153624\n",
      "Iteration: 411Loss: 3.2571461240398665\n",
      "Iteration: 412Loss: 3.369701387975318\n",
      "Iteration: 413Loss: 3.4767102866313038\n",
      "Iteration: 414Loss: 3.559592477718642\n",
      "Iteration: 415Loss: 3.1107957669025796\n",
      "Iteration: 416Loss: 3.3392886572414966\n",
      "Iteration: 417Loss: 3.195334036359633\n",
      "Iteration: 418Loss: 3.649924492865579\n",
      "Iteration: 419Loss: 3.4417389637375453\n",
      "Iteration: 420Loss: 3.2637431399050922\n",
      "Iteration: 421Loss: 3.295797694555886\n",
      "Iteration: 422Loss: 3.405448349669137\n",
      "Iteration: 423Loss: 3.6685900700092318\n",
      "Iteration: 424Loss: 3.4646835131799523\n",
      "Iteration: 425Loss: 3.2852826101142307\n",
      "Iteration: 426Loss: 3.251713082636245\n",
      "Iteration: 427Loss: 3.410896789768733\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 428Loss: 3.2667516490354718\n",
      "Iteration: 429Loss: 3.321953184613413\n",
      "Iteration: 430Loss: 2.96905720541578\n",
      "Iteration: 431Loss: 3.2853915389260333\n",
      "Iteration: 432Loss: 3.167449611715353\n",
      "Iteration: 433Loss: 3.2526345727776613\n",
      "Iteration: 434Loss: 3.0292463971211054\n",
      "Iteration: 435Loss: 3.353384075415405\n",
      "Iteration: 436Loss: 3.4192588049000854\n",
      "Iteration: 437Loss: 3.282109815467158\n",
      "Iteration: 438Loss: 3.3974056986158208\n",
      "Iteration: 439Loss: 3.374204766941234\n",
      "Iteration: 440Loss: 3.291687551060746\n",
      "Iteration: 441Loss: 3.4956144281446115\n",
      "Iteration: 442Loss: 3.214851891065573\n",
      "Iteration: 443Loss: 3.3385450718069345\n",
      "Iteration: 444Loss: 3.347493797484065\n",
      "Iteration: 445Loss: 3.5078943276354537\n",
      "Iteration: 446Loss: 3.443752174688138\n",
      "Iteration: 447Loss: 3.470044442953715\n",
      "Iteration: 448Loss: 3.0896125021236425\n",
      "Iteration: 449Loss: 3.23761657740882\n",
      "Iteration: 450Loss: 3.189031674393924\n",
      "Iteration: 451Loss: 3.2142899043247235\n",
      "Iteration: 452Loss: 3.2092019258923226\n",
      "Iteration: 453Loss: 3.2499685795008735\n",
      "Iteration: 454Loss: 3.5341461789132356\n",
      "Iteration: 455Loss: 3.0684328686184754\n",
      "Iteration: 456Loss: 3.456234727582431\n",
      "Iteration: 457Loss: 3.089182556349757\n",
      "Iteration: 458Loss: 3.187604055670034\n",
      "Iteration: 459Loss: 3.286405315932352\n",
      "Iteration: 460Loss: 3.26925689616459\n",
      "Iteration: 461Loss: 3.184813594176892\n",
      "Iteration: 462Loss: 3.371463255227073\n",
      "Iteration: 463Loss: 3.378112204739305\n",
      "Iteration: 464Loss: 3.374506961839879\n",
      "Iteration: 465Loss: 3.2040973094767056\n",
      "Iteration: 466Loss: 3.5378593880874223\n",
      "Iteration: 467Loss: 3.2128588123578985\n",
      "Iteration: 468Loss: 3.2105091697419694\n",
      "Iteration: 469Loss: 3.3145932641322005\n",
      "Iteration: 470Loss: 3.3401025203009596\n",
      "Iteration: 471Loss: 3.051270843051267\n",
      "Iteration: 472Loss: 3.030863658349052\n",
      "Iteration: 473Loss: 3.2893161239905004\n",
      "Iteration: 474Loss: 3.3816273384080433\n",
      "Iteration: 475Loss: 3.379035242503307\n",
      "Iteration: 476Loss: 3.3348495750775875\n",
      "Iteration: 477Loss: 3.340039351467331\n",
      "Iteration: 478Loss: 3.36307526221136\n",
      "Iteration: 479Loss: 3.463413372475422\n",
      "Iteration: 480Loss: 3.4492215294479616\n",
      "Iteration: 481Loss: 3.057114114043795\n",
      "Iteration: 482Loss: 3.45655902324721\n",
      "Iteration: 483Loss: 3.282732320463834\n",
      "Iteration: 484Loss: 3.379150672680841\n",
      "Iteration: 485Loss: 3.4329201261087965\n",
      "Iteration: 486Loss: 3.215639498328118\n",
      "Iteration: 487Loss: 3.285385433715695\n",
      "Iteration: 488Loss: 3.3434081961022457\n",
      "Iteration: 489Loss: 3.3256185945205874\n",
      "Iteration: 490Loss: 3.40498387700487\n",
      "Iteration: 491Loss: 3.3689881719730264\n",
      "Iteration: 492Loss: 3.2510172991537827\n",
      "Iteration: 493Loss: 3.250307178116591\n",
      "Iteration: 494Loss: 3.1715854728190696\n",
      "Iteration: 495Loss: 3.1888366678460356\n",
      "Iteration: 496Loss: 3.5250463406445007\n",
      "Iteration: 497Loss: 3.2829232944605806\n",
      "Iteration: 498Loss: 3.2130267169621898\n",
      "Iteration: 499Loss: 3.1766234891088327\n",
      "Iteration: 500Loss: 3.4542850116392128\n",
      "Iteration: 501Loss: 3.3178536623126007\n",
      "Iteration: 502Loss: 3.340175431595262\n",
      "Iteration: 503Loss: 3.564941655988203\n",
      "Iteration: 504Loss: 3.4713847441173296\n",
      "Iteration: 505Loss: 3.4722043829247915\n",
      "Iteration: 506Loss: 3.28723119526209\n",
      "Iteration: 507Loss: 3.409075867036602\n",
      "Iteration: 508Loss: 3.311256310351172\n",
      "Iteration: 509Loss: 3.403633816841063\n",
      "Iteration: 510Loss: 3.1002999872216264\n",
      "Iteration: 511Loss: 3.23765974027038\n",
      "Iteration: 512Loss: 3.2102879718934165\n",
      "Iteration: 513Loss: 3.4358081823854887\n",
      "Iteration: 514Loss: 3.3072101620620296\n",
      "Iteration: 515Loss: 3.3733514400251727\n",
      "Iteration: 516Loss: 3.179694904033283\n",
      "Iteration: 517Loss: 3.471165219898324\n",
      "Iteration: 518Loss: 3.547135592002017\n",
      "Iteration: 519Loss: 3.257509362713636\n",
      "Iteration: 520Loss: 3.155872707647257\n",
      "Iteration: 521Loss: 3.343313694881801\n",
      "Iteration: 522Loss: 3.2463272338975826\n",
      "Iteration: 523Loss: 3.4931066241292728\n",
      "Iteration: 524Loss: 3.3396183096607657\n",
      "Iteration: 525Loss: 2.903226940746319\n",
      "Iteration: 526Loss: 3.157600126483224\n",
      "Iteration: 527Loss: 3.277348060343396\n",
      "Iteration: 528Loss: 3.228954061059727\n",
      "Iteration: 529Loss: 3.1968838427924275\n",
      "Iteration: 530Loss: 3.226660062821804\n",
      "Iteration: 531Loss: 3.7256734753701544\n",
      "Iteration: 532Loss: 3.3246021254550318\n",
      "Iteration: 533Loss: 3.1310136110077464\n",
      "Iteration: 534Loss: 3.2299587679820947\n",
      "Iteration: 535Loss: 3.1793144744025965\n",
      "Iteration: 536Loss: 3.43270364844927\n",
      "Iteration: 537Loss: 3.228354995011115\n",
      "Iteration: 538Loss: 3.1398700276945433\n",
      "Iteration: 539Loss: 3.2478936115270796\n",
      "Iteration: 540Loss: 3.2717078728932374\n",
      "Iteration: 541Loss: 3.1927400558260413\n",
      "Iteration: 542Loss: 3.2840586150361895\n",
      "Iteration: 543Loss: 2.9796871527742326\n",
      "Iteration: 544Loss: 3.44669824553005\n",
      "Iteration: 545Loss: 3.037009850384851\n",
      "Iteration: 546Loss: 3.4801902646662226\n",
      "Iteration: 547Loss: 2.9661837765715906\n",
      "Iteration: 548Loss: 3.1486367443488597\n",
      "Iteration: 549Loss: 3.05304932823239\n",
      "Iteration: 550Loss: 3.495831787232031\n",
      "Iteration: 551Loss: 3.391408741640303\n",
      "Iteration: 552Loss: 3.249499883896471\n",
      "Iteration: 553Loss: 3.448154973987825\n",
      "Iteration: 554Loss: 3.4394214739106173\n",
      "Iteration: 555Loss: 3.527461691212881\n",
      "Iteration: 556Loss: 3.313454904812801\n",
      "Iteration: 557Loss: 3.431140429906618\n",
      "Iteration: 558Loss: 3.417822284157197\n",
      "Iteration: 559Loss: 3.465021908143755\n",
      "Iteration: 560Loss: 3.4367978750169277\n",
      "Iteration: 561Loss: 2.959061979022606\n",
      "Iteration: 562Loss: 3.176950034690209\n",
      "Iteration: 563Loss: 3.261312188073773\n",
      "Iteration: 564Loss: 3.1513540098383985\n",
      "Iteration: 565Loss: 3.281960087133993\n",
      "Iteration: 566Loss: 3.471668900939669\n",
      "Iteration: 567Loss: 3.333743560782028\n",
      "Iteration: 568Loss: 3.3492750546863777\n",
      "Iteration: 569Loss: 3.3623739770648102\n",
      "Iteration: 570Loss: 3.4790518826290886\n",
      "Iteration: 571Loss: 3.194067178001428\n",
      "Iteration: 572Loss: 3.3063447761826983\n",
      "Iteration: 573Loss: 3.04617321106203\n",
      "Iteration: 574Loss: 3.229610579541928\n",
      "Iteration: 575Loss: 3.3486544507312828\n",
      "Iteration: 576Loss: 3.276480738656518\n",
      "Iteration: 577Loss: 3.089855757318852\n",
      "Iteration: 578Loss: 3.1379887277127376\n",
      "Iteration: 579Loss: 3.340214928503929\n",
      "Iteration: 580Loss: 3.1984264753470857\n",
      "Iteration: 581Loss: 3.3503343842979243\n",
      "Iteration: 582Loss: 3.2781781295670953\n",
      "Iteration: 583Loss: 3.653913599282208\n",
      "Iteration: 584Loss: 3.258282150093224\n",
      "Iteration: 585Loss: 3.435448511691692\n",
      "Iteration: 586Loss: 3.3700588338895154\n",
      "Iteration: 587Loss: 3.325078586680871\n",
      "Iteration: 588Loss: 3.1423046340722194\n",
      "Iteration: 589Loss: 3.429456506568822\n",
      "Iteration: 590Loss: 3.3783701154604944\n",
      "Iteration: 591Loss: 3.047838260270999\n",
      "Iteration: 592Loss: 3.286639337505907\n",
      "Iteration: 593Loss: 3.2016033490253397\n",
      "Iteration: 594Loss: 3.4418604104977213\n",
      "Iteration: 595Loss: 3.2281619239871095\n",
      "Iteration: 596Loss: 3.411104558647415\n",
      "Iteration: 597Loss: 3.2868639345123167\n",
      "Iteration: 598Loss: 3.1610752514011606\n",
      "Iteration: 599Loss: 3.511866739474165\n",
      "Iteration: 600Loss: 3.351989767429451\n",
      "Iteration: 601Loss: 3.525429138881715\n",
      "Iteration: 602Loss: 3.0748453556334425\n",
      "Iteration: 603Loss: 3.416039347889363\n",
      "Iteration: 604Loss: 3.282942833924333\n",
      "Iteration: 605Loss: 2.998032705297748\n",
      "Iteration: 606Loss: 3.177401760894719\n",
      "Iteration: 607Loss: 3.1847594488421316\n",
      "Iteration: 608Loss: 3.2598043575722877\n",
      "Iteration: 609Loss: 3.2429470003630296\n",
      "Iteration: 610Loss: 3.2376388303816\n",
      "Iteration: 611Loss: 3.1150112337511002\n",
      "Iteration: 612Loss: 3.2538128882247745\n",
      "Iteration: 613Loss: 3.167918974050597\n",
      "Iteration: 614Loss: 3.3397644352544\n",
      "Iteration: 615Loss: 3.2241174930164718\n",
      "Iteration: 616Loss: 3.3309415272540517\n",
      "Iteration: 617Loss: 3.1844250224530697\n",
      "Iteration: 618Loss: 3.491123770050068\n",
      "Iteration: 619Loss: 3.361031451747456\n",
      "Iteration: 620Loss: 3.1990721142187457\n",
      "Iteration: 621Loss: 3.2954556815921494\n",
      "Iteration: 622Loss: 3.1999809627197875\n",
      "Iteration: 623Loss: 3.4420852145612844\n",
      "Iteration: 624Loss: 3.398439725660102\n",
      "Iteration: 625Loss: 3.346675099973501\n",
      "Iteration: 626Loss: 3.3750607178404053\n",
      "Iteration: 627Loss: 3.2963529900610937\n",
      "Iteration: 628Loss: 3.349681021486144\n",
      "Iteration: 629Loss: 3.3075194337668465\n",
      "Iteration: 630Loss: 3.335540994768373\n",
      "Iteration: 631Loss: 3.192258191541643\n",
      "Iteration: 632Loss: 3.2573908798523696\n",
      "Iteration: 633Loss: 3.470156199101328\n",
      "Iteration: 634Loss: 3.240199039307292\n",
      "Iteration: 635Loss: 3.2327698462300916\n",
      "Iteration: 636Loss: 3.5403572054549115\n",
      "Iteration: 637Loss: 3.1574355519704693\n",
      "Iteration: 638Loss: 3.427472575650185\n",
      "Iteration: 639Loss: 3.1255860478922495\n",
      "Iteration: 640Loss: 3.3591585428464215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 641Loss: 3.2578994376202806\n",
      "Iteration: 642Loss: 3.326600747729712\n",
      "Iteration: 643Loss: 3.2949781562874025\n",
      "Iteration: 644Loss: 3.0470838438795536\n",
      "Iteration: 645Loss: 3.2457890545052512\n",
      "Iteration: 646Loss: 3.1736537128705864\n",
      "Iteration: 647Loss: 3.1832615120967973\n",
      "Iteration: 648Loss: 3.14387310399669\n",
      "Iteration: 649Loss: 3.1724460359137288\n",
      "Iteration: 650Loss: 3.1918773792491604\n",
      "Iteration: 651Loss: 3.2302687624665833\n",
      "Iteration: 652Loss: 3.412539224647725\n",
      "Iteration: 653Loss: 3.5322174476895465\n",
      "Iteration: 654Loss: 3.4063924726412957\n",
      "Iteration: 655Loss: 3.26517080430417\n",
      "Iteration: 656Loss: 3.0201591702637782\n",
      "Iteration: 657Loss: 3.242268804442413\n",
      "Iteration: 658Loss: 3.191122396121645\n",
      "Iteration: 659Loss: 3.144832958112515\n",
      "Iteration: 660Loss: 3.1655040744609564\n",
      "Iteration: 661Loss: 3.116649031438427\n",
      "Iteration: 662Loss: 3.386316074768518\n",
      "Iteration: 663Loss: 3.175547311871778\n",
      "Iteration: 664Loss: 3.25193711604933\n",
      "Iteration: 665Loss: 3.3417183501021257\n",
      "Iteration: 666Loss: 3.19704532072201\n",
      "Iteration: 667Loss: 3.3210768091404224\n",
      "Iteration: 668Loss: 3.1739897857959334\n",
      "Iteration: 669Loss: 3.1413771536157284\n",
      "Iteration: 670Loss: 3.41317729137793\n",
      "Iteration: 671Loss: 3.191008343686087\n",
      "Iteration: 672Loss: 3.2027703064044393\n",
      "Iteration: 673Loss: 3.5349903333877988\n",
      "Iteration: 674Loss: 3.078735277238028\n",
      "Iteration: 675Loss: 3.1658940402425593\n",
      "Iteration: 676Loss: 3.154983661312916\n",
      "Iteration: 677Loss: 3.2468345911181142\n",
      "Iteration: 678Loss: 3.175046441683141\n",
      "Iteration: 679Loss: 3.286798284615187\n",
      "Iteration: 680Loss: 3.134875518237897\n",
      "Iteration: 681Loss: 3.0624388442413464\n",
      "Iteration: 682Loss: 3.068111603130518\n",
      "Iteration: 683Loss: 3.4047629737738268\n",
      "Iteration: 684Loss: 3.3697676390516462\n",
      "Iteration: 685Loss: 3.326081706568448\n",
      "Iteration: 686Loss: 3.5749616713793757\n",
      "Iteration: 687Loss: 3.275658159213721\n",
      "Iteration: 688Loss: 3.2569162361051913\n",
      "Iteration: 689Loss: 3.119649709132083\n",
      "Iteration: 690Loss: 3.3873837416458157\n",
      "Iteration: 691Loss: 3.1204073801874888\n",
      "Iteration: 692Loss: 3.256391065535401\n",
      "Iteration: 693Loss: 3.0802201428141\n",
      "Iteration: 694Loss: 3.5455312268377988\n",
      "Iteration: 695Loss: 3.3782924880120326\n",
      "Iteration: 696Loss: 3.211341960666891\n",
      "Iteration: 697Loss: 3.25315860006258\n",
      "Iteration: 698Loss: 3.2160174620051407\n",
      "Iteration: 699Loss: 3.288110650710862\n",
      "Iteration: 700Loss: 3.105909947789227\n",
      "Iteration: 701Loss: 3.3911697567331682\n",
      "Iteration: 702Loss: 3.256463265355258\n",
      "Iteration: 703Loss: 3.361619926693628\n",
      "Iteration: 704Loss: 3.1474211604671742\n",
      "Iteration: 705Loss: 3.0428438953359076\n",
      "Iteration: 706Loss: 3.444089430284827\n",
      "Iteration: 707Loss: 3.0592829378573856\n",
      "Iteration: 708Loss: 3.149046096126087\n",
      "Iteration: 709Loss: 3.240409778889699\n",
      "Iteration: 710Loss: 2.918552261945923\n",
      "Iteration: 711Loss: 3.3306722103267687\n",
      "Iteration: 712Loss: 3.115422004994787\n",
      "Iteration: 713Loss: 3.184831273924786\n",
      "Iteration: 714Loss: 3.316903817080348\n",
      "Iteration: 715Loss: 3.179703813489704\n",
      "Iteration: 716Loss: 3.174771265003222\n",
      "Iteration: 717Loss: 3.416612695955427\n",
      "Iteration: 718Loss: 3.4551294469622738\n",
      "Iteration: 719Loss: 3.15793445018475\n",
      "Iteration: 720Loss: 3.249998158817371\n",
      "Iteration: 721Loss: 2.837474993111876\n",
      "Iteration: 722Loss: 3.0696374441559438\n",
      "Iteration: 723Loss: 3.005471032744242\n",
      "Iteration: 724Loss: 3.12607497468996\n",
      "Iteration: 725Loss: 3.176265262769089\n",
      "Iteration: 726Loss: 3.4785696394098897\n",
      "Iteration: 727Loss: 3.254242047787674\n",
      "Iteration: 728Loss: 3.127808861315059\n",
      "Iteration: 729Loss: 3.2297940209804885\n",
      "Iteration: 730Loss: 3.214399380016949\n",
      "Iteration: 731Loss: 3.36196657144633\n",
      "Iteration: 732Loss: 3.3592841089034673\n",
      "Iteration: 733Loss: 3.215815309596801\n",
      "Iteration: 734Loss: 3.3052904323875296\n",
      "Iteration: 735Loss: 3.363892732315934\n",
      "Iteration: 736Loss: 2.993254151797773\n",
      "Iteration: 737Loss: 3.3285759285430165\n",
      "Iteration: 738Loss: 3.3365619530259107\n",
      "Iteration: 739Loss: 3.3378454967307825\n",
      "Iteration: 740Loss: 3.2561946067514023\n",
      "Iteration: 741Loss: 3.2074184247302178\n",
      "Iteration: 742Loss: 3.3528167325667386\n",
      "Iteration: 743Loss: 3.162458355801798\n",
      "Iteration: 744Loss: 3.1812561025249333\n",
      "Iteration: 745Loss: 3.198428214149951\n",
      "Iteration: 746Loss: 2.9619577478849033\n",
      "Iteration: 747Loss: 3.3136778411354677\n",
      "Iteration: 748Loss: 3.08138151818692\n",
      "Iteration: 749Loss: 3.2422115078507945\n",
      "Iteration: 750Loss: 3.1256719510204696\n",
      "Iteration: 751Loss: 3.5715516773655938\n",
      "Iteration: 752Loss: 3.0928278596358214\n",
      "Iteration: 753Loss: 3.354360638656061\n",
      "Iteration: 754Loss: 3.074352540831635\n",
      "Iteration: 755Loss: 3.1941040246228662\n",
      "Iteration: 756Loss: 3.339399323766038\n",
      "Iteration: 757Loss: 3.1025005722317496\n",
      "Iteration: 758Loss: 3.15394269824028\n",
      "Iteration: 759Loss: 3.2049313470346035\n",
      "Iteration: 760Loss: 3.2599900996456492\n",
      "Iteration: 761Loss: 3.566903537229161\n",
      "Iteration: 762Loss: 3.3090137091565164\n",
      "Iteration: 763Loss: 3.0317083756692744\n",
      "Iteration: 764Loss: 2.9891236825807237\n",
      "Iteration: 765Loss: 3.2193603274327787\n",
      "Iteration: 766Loss: 3.4542922184831486\n",
      "Iteration: 767Loss: 3.0285345873883864\n",
      "Iteration: 768Loss: 3.2064324099259296\n",
      "Iteration: 769Loss: 3.1633400669709073\n",
      "Iteration: 770Loss: 3.283378391148816\n",
      "Iteration: 771Loss: 3.1302429601989763\n",
      "Iteration: 772Loss: 3.0845700350597482\n",
      "Iteration: 773Loss: 3.2352072982926954\n",
      "Iteration: 774Loss: 3.150744864977314\n",
      "Iteration: 775Loss: 3.3492475676630598\n",
      "Iteration: 776Loss: 3.1441216820720443\n",
      "Iteration: 777Loss: 3.302697503863728\n",
      "Iteration: 778Loss: 3.1692828418697676\n",
      "Iteration: 779Loss: 3.2579296694256556\n",
      "Iteration: 780Loss: 3.277133446342001\n",
      "Iteration: 781Loss: 3.4217331992524085\n",
      "Iteration: 782Loss: 3.2023136386478956\n",
      "Iteration: 783Loss: 3.2316011810178122\n",
      "Iteration: 784Loss: 3.3169365663980215\n",
      "Iteration: 785Loss: 2.995097770535779\n",
      "Iteration: 786Loss: 3.3250823569063854\n",
      "Iteration: 787Loss: 3.155605950491036\n",
      "Iteration: 788Loss: 3.1159241317653392\n",
      "Iteration: 789Loss: 3.1878328801490574\n",
      "Iteration: 790Loss: 3.032019765419902\n",
      "Iteration: 791Loss: 3.0794062623229457\n",
      "Iteration: 792Loss: 3.4321588280785513\n",
      "Iteration: 793Loss: 3.0977391020165594\n",
      "Iteration: 794Loss: 3.3356715367318075\n",
      "Iteration: 795Loss: 3.4574192719723795\n",
      "Iteration: 796Loss: 3.1837832405631197\n",
      "Iteration: 797Loss: 2.9701799250336207\n",
      "Iteration: 798Loss: 3.5153578400243948\n",
      "Iteration: 799Loss: 3.200725013530111\n",
      "Iteration: 800Loss: 2.9838670481341825\n",
      "Iteration: 801Loss: 2.9823822380647935\n",
      "Iteration: 802Loss: 3.0636412545792617\n",
      "Iteration: 803Loss: 3.253896684510175\n",
      "Iteration: 804Loss: 3.2321834416733575\n",
      "Iteration: 805Loss: 3.1971387849854573\n",
      "Iteration: 806Loss: 3.4519085388758026\n",
      "Iteration: 807Loss: 3.344710251005164\n",
      "Iteration: 808Loss: 3.2995455811421075\n",
      "Iteration: 809Loss: 3.150209585691051\n",
      "Iteration: 810Loss: 3.02663194226449\n",
      "Iteration: 811Loss: 3.2987508005054114\n",
      "Iteration: 812Loss: 3.328835214917871\n",
      "Iteration: 813Loss: 2.9278721308951536\n",
      "Iteration: 814Loss: 3.003205339362396\n",
      "Iteration: 815Loss: 3.120084889210397\n",
      "Iteration: 816Loss: 3.1004885216045026\n",
      "Iteration: 817Loss: 3.2168903219713627\n",
      "Iteration: 818Loss: 3.180611844337604\n",
      "Iteration: 819Loss: 3.2055195995634396\n",
      "Iteration: 820Loss: 3.1480672594845154\n",
      "Iteration: 821Loss: 3.041620530462038\n",
      "Iteration: 822Loss: 3.447271365842974\n",
      "Iteration: 823Loss: 3.291109709289962\n",
      "Iteration: 824Loss: 3.2462961020377086\n",
      "Iteration: 825Loss: 3.148452370802244\n",
      "Iteration: 826Loss: 3.3780574739816034\n",
      "Iteration: 827Loss: 3.5320115960182634\n",
      "Iteration: 828Loss: 3.1922192583588185\n",
      "Iteration: 829Loss: 3.16010395783931\n",
      "Iteration: 830Loss: 3.3187389689438205\n",
      "Iteration: 831Loss: 3.1611625365849205\n",
      "Iteration: 832Loss: 3.2316355477575818\n",
      "Iteration: 833Loss: 3.023859776664215\n",
      "Iteration: 834Loss: 3.1671061276611896\n",
      "Iteration: 835Loss: 3.2690683770859303\n",
      "Iteration: 836Loss: 3.100322476110195\n",
      "Iteration: 837Loss: 3.100664992979916\n",
      "Iteration: 838Loss: 3.1221039490370943\n",
      "Iteration: 839Loss: 2.962045158576834\n",
      "Iteration: 840Loss: 3.4086417287331203\n",
      "Iteration: 841Loss: 3.2509089695308035\n",
      "Iteration: 842Loss: 3.368354790421732\n",
      "Iteration: 843Loss: 3.120332651976644\n",
      "Iteration: 844Loss: 3.280743574639339\n",
      "Iteration: 845Loss: 3.2819300119496546\n",
      "Iteration: 846Loss: 2.9749048624178323\n",
      "Iteration: 847Loss: 3.1707736042441512\n",
      "Iteration: 848Loss: 3.3819352155756253\n",
      "Iteration: 849Loss: 3.2677885203897685\n",
      "Iteration: 850Loss: 3.326088779118525\n",
      "Iteration: 851Loss: 3.1429367905160794\n",
      "Iteration: 852Loss: 3.0203911506766032\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 853Loss: 3.117065071294535\n",
      "Iteration: 854Loss: 3.1272295713911014\n",
      "Iteration: 855Loss: 2.9307237248679603\n",
      "Iteration: 856Loss: 3.2662959486052587\n",
      "Iteration: 857Loss: 2.741626502650157\n",
      "Iteration: 858Loss: 3.13506667333916\n",
      "Iteration: 859Loss: 3.3332705712472825\n",
      "Iteration: 860Loss: 3.0860767369963114\n",
      "Iteration: 861Loss: 3.2341737544414606\n",
      "Iteration: 862Loss: 3.3871892384701106\n",
      "Iteration: 863Loss: 3.340737044573059\n",
      "Iteration: 864Loss: 2.9946450876507593\n",
      "Iteration: 865Loss: 3.375183342343598\n",
      "Iteration: 866Loss: 3.2961240900373183\n",
      "Iteration: 867Loss: 3.2750442667168977\n",
      "Iteration: 868Loss: 3.2493001857735946\n",
      "Iteration: 869Loss: 3.1596457835544554\n",
      "Iteration: 870Loss: 3.245792491737939\n",
      "Iteration: 871Loss: 3.1054794113000765\n",
      "Iteration: 872Loss: 3.290770804107411\n",
      "Iteration: 873Loss: 3.3262137677908563\n",
      "Iteration: 874Loss: 3.18967217802694\n",
      "Iteration: 875Loss: 3.370450826081377\n",
      "Iteration: 876Loss: 3.101298845673521\n",
      "Iteration: 877Loss: 3.286445123852646\n",
      "Iteration: 878Loss: 3.1953862012283856\n",
      "Iteration: 879Loss: 3.0429474416914886\n",
      "Iteration: 880Loss: 2.903248587361075\n",
      "Iteration: 881Loss: 3.3299112953463528\n",
      "Iteration: 882Loss: 3.1573523385561915\n",
      "Iteration: 883Loss: 2.8974621232706013\n",
      "Iteration: 884Loss: 3.349459746120596\n",
      "Iteration: 885Loss: 3.0883413949665033\n",
      "Iteration: 886Loss: 3.1030124042335214\n",
      "Iteration: 887Loss: 3.295736185021429\n",
      "Iteration: 888Loss: 3.0668049953214784\n",
      "Iteration: 889Loss: 3.2315771605414434\n",
      "Iteration: 890Loss: 3.162758295540119\n",
      "Iteration: 891Loss: 2.971799745556058\n",
      "Iteration: 892Loss: 3.1499005551635024\n",
      "Iteration: 893Loss: 3.2146573248330284\n",
      "Iteration: 894Loss: 3.1482598131266664\n",
      "Iteration: 895Loss: 3.124790935090247\n",
      "Iteration: 896Loss: 3.260575377376014\n",
      "Iteration: 897Loss: 3.3611554345835333\n",
      "Iteration: 898Loss: 2.9728682245966045\n",
      "Iteration: 899Loss: 3.40937028252277\n",
      "Iteration: 900Loss: 3.021935904376521\n",
      "Iteration: 901Loss: 3.1290255500029245\n",
      "Iteration: 902Loss: 3.245703510580884\n",
      "Iteration: 903Loss: 3.17929750668663\n",
      "Iteration: 904Loss: 3.1307457592738133\n",
      "Iteration: 905Loss: 3.251368388436914\n",
      "Iteration: 906Loss: 3.108388727530837\n",
      "Iteration: 907Loss: 3.1352269066854226\n",
      "Iteration: 908Loss: 3.1432124531604875\n",
      "Iteration: 909Loss: 3.2574187716397134\n",
      "Iteration: 910Loss: 3.189675840543257\n",
      "Iteration: 911Loss: 3.3604610793488714\n",
      "Iteration: 912Loss: 3.2849220314995566\n",
      "Iteration: 913Loss: 3.141708795026561\n",
      "Iteration: 914Loss: 2.921328215371995\n",
      "Iteration: 915Loss: 3.143346078261984\n",
      "Iteration: 916Loss: 3.275256130706381\n",
      "Iteration: 917Loss: 3.270247204645141\n",
      "Iteration: 918Loss: 3.2270161854593375\n",
      "Iteration: 919Loss: 3.2451108975679546\n",
      "Iteration: 920Loss: 3.1942813638773226\n",
      "Iteration: 921Loss: 2.9831153534638184\n",
      "Iteration: 922Loss: 2.9510077015981078\n",
      "Iteration: 923Loss: 3.1074884239606035\n",
      "Iteration: 924Loss: 3.192863538554592\n",
      "Iteration: 925Loss: 3.0972764920493776\n",
      "Iteration: 926Loss: 3.306596026303352\n",
      "Iteration: 927Loss: 3.5324055658170033\n",
      "Iteration: 928Loss: 3.1739712140552\n",
      "Iteration: 929Loss: 2.997389265362364\n",
      "Iteration: 930Loss: 3.1686432493154904\n",
      "Iteration: 931Loss: 2.9040184004774576\n",
      "Iteration: 932Loss: 3.241091243664607\n",
      "Iteration: 933Loss: 2.8178446418797423\n",
      "Iteration: 934Loss: 3.044879975923701\n",
      "Iteration: 935Loss: 3.086843349346527\n",
      "Iteration: 936Loss: 3.383987899271794\n",
      "Iteration: 937Loss: 3.2251314744353294\n",
      "Iteration: 938Loss: 3.3561443483892077\n",
      "Iteration: 939Loss: 3.237320380357679\n",
      "Iteration: 940Loss: 3.381369427184337\n",
      "Iteration: 941Loss: 2.9317454831376066\n",
      "Iteration: 942Loss: 3.2050775195338894\n",
      "Iteration: 943Loss: 3.346521711394113\n",
      "Iteration: 944Loss: 3.122529311314699\n",
      "Iteration: 945Loss: 3.2150628413737317\n",
      "Iteration: 946Loss: 3.12944572310786\n",
      "Iteration: 947Loss: 3.177341251178101\n",
      "Iteration: 948Loss: 3.273338773027802\n",
      "Iteration: 949Loss: 3.1000470956463126\n",
      "Iteration: 950Loss: 3.1500430419543193\n",
      "Iteration: 951Loss: 3.39922012683675\n",
      "Iteration: 952Loss: 3.344398094648349\n",
      "Iteration: 953Loss: 2.979936271369776\n",
      "Iteration: 954Loss: 3.2112660664634833\n",
      "Iteration: 955Loss: 3.120399092933114\n",
      "Iteration: 956Loss: 3.0452070860997904\n",
      "Iteration: 957Loss: 3.2920307513892273\n",
      "Iteration: 958Loss: 3.227566918193011\n",
      "Iteration: 959Loss: 3.1412658253537202\n",
      "Iteration: 960Loss: 2.975551283324784\n",
      "Iteration: 961Loss: 3.1884243299732136\n",
      "Iteration: 962Loss: 3.3885725180777473\n",
      "Iteration: 963Loss: 2.9639405377691883\n",
      "Iteration: 964Loss: 3.1825429686474256\n",
      "Iteration: 965Loss: 3.131451513754049\n",
      "Iteration: 966Loss: 3.48755279725508\n",
      "Iteration: 967Loss: 3.2208447742939073\n",
      "Iteration: 968Loss: 3.1650581866294702\n",
      "Iteration: 969Loss: 3.1983673603521012\n",
      "Iteration: 970Loss: 3.0759996032635266\n",
      "Iteration: 971Loss: 2.9151366480493786\n",
      "Iteration: 972Loss: 3.394374126787319\n",
      "Iteration: 973Loss: 3.4134724185601146\n",
      "Iteration: 974Loss: 3.0506333339000395\n",
      "Iteration: 975Loss: 3.257526131510043\n",
      "Iteration: 976Loss: 3.1751821290086495\n",
      "Iteration: 977Loss: 3.106796882278526\n",
      "Iteration: 978Loss: 3.2135153510403227\n",
      "Iteration: 979Loss: 3.188120770909831\n",
      "Iteration: 980Loss: 2.896150532844125\n",
      "Iteration: 981Loss: 2.7878090164176825\n",
      "Iteration: 982Loss: 3.21372013453772\n",
      "Iteration: 983Loss: 3.1791973234127546\n",
      "Iteration: 984Loss: 2.9714865786157154\n",
      "Iteration: 985Loss: 2.912094163211272\n",
      "Iteration: 986Loss: 3.056505976831449\n",
      "Iteration: 987Loss: 2.9435373887272296\n",
      "Iteration: 988Loss: 3.287372576897855\n",
      "Iteration: 989Loss: 3.277210090296146\n",
      "Iteration: 990Loss: 2.9361939480348997\n",
      "Iteration: 991Loss: 3.1582385222383893\n",
      "Iteration: 992Loss: 3.3130843298646457\n",
      "Iteration: 993Loss: 3.119760785857594\n",
      "Iteration: 994Loss: 3.1776097269259247\n",
      "Iteration: 995Loss: 3.2581176323070182\n",
      "Iteration: 996Loss: 3.2388569824515154\n",
      "Iteration: 997Loss: 3.066052676335447\n",
      "Iteration: 998Loss: 3.192080931953905\n",
      "Iteration: 999Loss: 3.2667362144868313\n",
      "Iteration: 1000Loss: 3.3715078405709598\n",
      "Iteration: 1001Loss: 3.3790803225526567\n",
      "Iteration: 1002Loss: 3.3436308385340996\n",
      "Iteration: 1003Loss: 3.341841350118784\n",
      "Iteration: 1004Loss: 3.336900888934172\n",
      "Iteration: 1005Loss: 3.0208314648079035\n",
      "Iteration: 1006Loss: 2.9844200560214498\n",
      "Iteration: 1007Loss: 3.0614576911753955\n",
      "Iteration: 1008Loss: 2.9995614542351925\n",
      "Iteration: 1009Loss: 3.068025351931555\n",
      "Iteration: 1010Loss: 2.8766712708083886\n",
      "Iteration: 1011Loss: 3.0536549075115276\n",
      "Iteration: 1012Loss: 3.204252479034801\n",
      "Iteration: 1013Loss: 3.1993690774523285\n",
      "Iteration: 1014Loss: 3.258820179522262\n",
      "Iteration: 1015Loss: 3.135087185676477\n",
      "Iteration: 1016Loss: 3.272669411309849\n",
      "Iteration: 1017Loss: 3.242115523831304\n",
      "Iteration: 1018Loss: 3.184001658260081\n",
      "Iteration: 1019Loss: 3.07045442831706\n",
      "Iteration: 1020Loss: 3.17811631893758\n",
      "Iteration: 1021Loss: 3.477749651197125\n",
      "Iteration: 1022Loss: 2.9348120609530004\n",
      "Iteration: 1023Loss: 3.2520904296272715\n",
      "Iteration: 1024Loss: 3.192688775014253\n",
      "Iteration: 1025Loss: 3.267165445599803\n",
      "Iteration: 1026Loss: 3.1829494507213534\n",
      "Iteration: 1027Loss: 2.9794811188496118\n",
      "Iteration: 1028Loss: 3.1760769882955118\n",
      "Iteration: 1029Loss: 3.2941451342652632\n",
      "Iteration: 1030Loss: 3.1180958181107292\n",
      "Iteration: 1031Loss: 3.193599593759115\n",
      "Iteration: 1032Loss: 3.475008242537041\n",
      "Iteration: 1033Loss: 3.262442213450833\n",
      "Iteration: 1034Loss: 3.092594052518138\n",
      "Iteration: 1035Loss: 2.9608371713206014\n",
      "Iteration: 1036Loss: 3.1544897586639356\n",
      "Iteration: 1037Loss: 3.221720943766348\n",
      "Iteration: 1038Loss: 2.948283584434775\n",
      "Iteration: 1039Loss: 3.1569146553483156\n",
      "Iteration: 1040Loss: 3.3474388668981256\n",
      "Iteration: 1041Loss: 2.916672379272419\n",
      "Iteration: 1042Loss: 3.2043448244176735\n",
      "Iteration: 1043Loss: 2.9241633952068264\n",
      "Iteration: 1044Loss: 3.168087697912497\n",
      "Iteration: 1045Loss: 3.181261856413185\n",
      "Iteration: 1046Loss: 2.9898636705472024\n",
      "Iteration: 1047Loss: 3.234211159123192\n",
      "Iteration: 1048Loss: 3.3839596938509047\n",
      "Iteration: 1049Loss: 3.438091933011644\n",
      "Iteration: 1050Loss: 2.973103790272829\n",
      "Iteration: 1051Loss: 3.1048416341614007\n",
      "Iteration: 1052Loss: 3.2251317246869053\n",
      "Iteration: 1053Loss: 3.254711994936975\n",
      "Iteration: 1054Loss: 2.8801871836136947\n",
      "Iteration: 1055Loss: 3.1263923590871405\n",
      "Iteration: 1056Loss: 3.249271344236145\n",
      "Iteration: 1057Loss: 3.1072627245007562\n",
      "Iteration: 1058Loss: 3.2073061163276275\n",
      "Iteration: 1059Loss: 3.0978966420491405\n",
      "Iteration: 1060Loss: 3.0248828904137395\n",
      "Iteration: 1061Loss: 3.0652074854635547\n",
      "Iteration: 1062Loss: 3.1520675566485528\n",
      "Iteration: 1063Loss: 3.0880014719441533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1064Loss: 3.4027479775040472\n",
      "Iteration: 1065Loss: 2.9717160474245534\n",
      "Iteration: 1066Loss: 3.285272604012862\n",
      "Iteration: 1067Loss: 3.0847594060846633\n",
      "Iteration: 1068Loss: 3.132393141641197\n",
      "Iteration: 1069Loss: 3.3426980268539865\n",
      "Iteration: 1070Loss: 2.898265714001713\n",
      "Iteration: 1071Loss: 2.8835078601112687\n",
      "Iteration: 1072Loss: 3.3252263981873273\n",
      "Iteration: 1073Loss: 3.0272468076776082\n",
      "Iteration: 1074Loss: 3.255648014895292\n",
      "Iteration: 1075Loss: 3.014262077534558\n",
      "Iteration: 1076Loss: 3.193120734028736\n",
      "Iteration: 1077Loss: 3.0885032098130405\n",
      "Iteration: 1078Loss: 3.0961633326025098\n",
      "Iteration: 1079Loss: 3.2386913708531626\n",
      "Iteration: 1080Loss: 3.106143610427859\n",
      "Iteration: 1081Loss: 3.406842295738813\n",
      "Iteration: 1082Loss: 3.11667737390233\n",
      "Iteration: 1083Loss: 3.3110580616115812\n",
      "Iteration: 1084Loss: 3.477295949228692\n",
      "Iteration: 1085Loss: 3.378481954022925\n",
      "Iteration: 1086Loss: 3.191451940624878\n",
      "Iteration: 1087Loss: 3.0235710410754892\n",
      "Iteration: 1088Loss: 3.194888601137299\n",
      "Iteration: 1089Loss: 3.235175348120662\n",
      "Iteration: 1090Loss: 3.1959966850804657\n",
      "Iteration: 1091Loss: 3.253757174680326\n",
      "Iteration: 1092Loss: 3.0770367189324497\n",
      "Iteration: 1093Loss: 3.120640802395767\n",
      "Iteration: 1094Loss: 3.406286521779208\n",
      "Iteration: 1095Loss: 3.1478598967547367\n",
      "Iteration: 1096Loss: 3.3119604926726938\n",
      "Iteration: 1097Loss: 2.8701092626761793\n",
      "Iteration: 1098Loss: 2.855498123245726\n",
      "Iteration: 1099Loss: 3.2195110125529913\n",
      "Iteration: 1100Loss: 2.888298113828094\n",
      "Iteration: 1101Loss: 3.0031379609097004\n",
      "Iteration: 1102Loss: 3.025985704925309\n",
      "Iteration: 1103Loss: 3.237623415273023\n",
      "Iteration: 1104Loss: 3.176995049212187\n",
      "Iteration: 1105Loss: 3.268207877703632\n",
      "Iteration: 1106Loss: 3.443207213184122\n",
      "Iteration: 1107Loss: 3.366178131669994\n",
      "Iteration: 1108Loss: 3.1012909667808928\n",
      "Iteration: 1109Loss: 3.1697495300162593\n",
      "Iteration: 1110Loss: 3.088706979792104\n",
      "Iteration: 1111Loss: 3.1996120704590516\n",
      "Iteration: 1112Loss: 2.9153655657205966\n",
      "Iteration: 1113Loss: 2.8692417082521313\n",
      "Iteration: 1114Loss: 3.272325831129657\n",
      "Iteration: 1115Loss: 3.014203416871855\n",
      "Iteration: 1116Loss: 2.9221125775745453\n",
      "Iteration: 1117Loss: 3.0597680959065645\n",
      "Iteration: 1118Loss: 2.9817861313372913\n",
      "Iteration: 1119Loss: 3.234906189566113\n",
      "Iteration: 1120Loss: 3.2501963748393323\n",
      "Iteration: 1121Loss: 3.1713234936922627\n",
      "Iteration: 1122Loss: 3.0199770123343486\n",
      "Iteration: 1123Loss: 3.2726363612676757\n",
      "Iteration: 1124Loss: 2.97238349424118\n",
      "Iteration: 1125Loss: 3.0296519548751246\n",
      "Iteration: 1126Loss: 3.304808903164065\n",
      "Iteration: 1127Loss: 3.0707234736198856\n",
      "Iteration: 1128Loss: 3.1660285293191825\n",
      "Iteration: 1129Loss: 3.236363478268031\n",
      "Iteration: 1130Loss: 3.532444306406314\n",
      "Iteration: 1131Loss: 3.039041521637626\n",
      "Iteration: 1132Loss: 2.8630891454279723\n",
      "Iteration: 1133Loss: 3.07520272753001\n",
      "Iteration: 1134Loss: 2.8840711444597877\n",
      "Iteration: 1135Loss: 3.0438132298598886\n",
      "Iteration: 1136Loss: 3.399508053531908\n",
      "Iteration: 1137Loss: 3.0238584361779384\n",
      "Iteration: 1138Loss: 3.2100874158587125\n",
      "Iteration: 1139Loss: 3.1534898649745933\n",
      "Iteration: 1140Loss: 3.2384941833044745\n",
      "Iteration: 1141Loss: 3.490563096742812\n",
      "Iteration: 1142Loss: 3.2340804422456846\n",
      "Iteration: 1143Loss: 3.2243021730081742\n",
      "Iteration: 1144Loss: 3.051787588288742\n",
      "Iteration: 1145Loss: 3.134294658733964\n",
      "Iteration: 1146Loss: 3.1114023942738163\n",
      "Iteration: 1147Loss: 2.9624046995519056\n",
      "Iteration: 1148Loss: 3.264392243444778\n",
      "Iteration: 1149Loss: 3.196461576366802\n",
      "Iteration: 1150Loss: 2.997003529770844\n",
      "Iteration: 1151Loss: 3.121621638420038\n",
      "Iteration: 1152Loss: 2.877756551872045\n",
      "Iteration: 1153Loss: 2.956486260391655\n",
      "Iteration: 1154Loss: 3.030836699743572\n",
      "Iteration: 1155Loss: 3.2526481152868434\n",
      "Iteration: 1156Loss: 3.29677181445259\n",
      "Iteration: 1157Loss: 3.2360562537686217\n",
      "Iteration: 1158Loss: 3.0616521763792144\n",
      "Iteration: 1159Loss: 3.2396100643090904\n",
      "Iteration: 1160Loss: 3.076519854762482\n",
      "Iteration: 1161Loss: 3.2521465087821246\n",
      "Iteration: 1162Loss: 3.2160739117821007\n",
      "Iteration: 1163Loss: 3.3873971938678156\n",
      "Iteration: 1164Loss: 3.303349700308574\n",
      "Iteration: 1165Loss: 2.9143896796347866\n",
      "Iteration: 1166Loss: 3.194331125559376\n",
      "Iteration: 1167Loss: 2.965776661701218\n",
      "Iteration: 1168Loss: 3.279093745018845\n",
      "Iteration: 1169Loss: 3.3400415607729816\n",
      "Iteration: 1170Loss: 3.0840503082280235\n",
      "Iteration: 1171Loss: 3.0770011231990395\n",
      "Iteration: 1172Loss: 3.153895833017894\n",
      "Iteration: 1173Loss: 3.346785442479886\n",
      "Iteration: 1174Loss: 3.2268544285031413\n",
      "Iteration: 1175Loss: 3.0833582369275296\n",
      "Iteration: 1176Loss: 3.3216747074996236\n",
      "Iteration: 1177Loss: 2.9578264965637553\n",
      "Iteration: 1178Loss: 2.9457570511622952\n",
      "Iteration: 1179Loss: 3.1514369952663075\n",
      "Iteration: 1180Loss: 3.2331814493486135\n",
      "Iteration: 1181Loss: 3.412483621921498\n",
      "Iteration: 1182Loss: 3.0445091531798245\n",
      "Iteration: 1183Loss: 3.105398194793312\n",
      "Iteration: 1184Loss: 3.294752811556477\n",
      "Iteration: 1185Loss: 3.076014129671616\n",
      "Iteration: 1186Loss: 2.8378970963747374\n",
      "Iteration: 1187Loss: 3.2582317590244183\n",
      "Iteration: 1188Loss: 3.1578737561950128\n",
      "Iteration: 1189Loss: 3.345729066772544\n",
      "Iteration: 1190Loss: 3.090142411810472\n",
      "Iteration: 1191Loss: 3.274160495036662\n",
      "Iteration: 1192Loss: 3.0860664832929907\n",
      "Iteration: 1193Loss: 3.1059295702091765\n",
      "Iteration: 1194Loss: 3.4353383367003274\n",
      "Iteration: 1195Loss: 3.2543110782057174\n",
      "Iteration: 1196Loss: 3.0768501308024323\n",
      "Iteration: 1197Loss: 3.12526381570017\n",
      "Iteration: 1198Loss: 3.0043918953944657\n",
      "Iteration: 1199Loss: 3.1599624585323465\n",
      "Iteration: 1200Loss: 3.070504390178295\n",
      "Iteration: 1201Loss: 2.97986637303241\n",
      "Iteration: 1202Loss: 3.1083324655364013\n",
      "Iteration: 1203Loss: 2.936615991706833\n",
      "Iteration: 1204Loss: 3.1123725742360633\n",
      "Iteration: 1205Loss: 3.0125665257021432\n",
      "Iteration: 1206Loss: 2.973554221680388\n",
      "Iteration: 1207Loss: 2.8605809990191697\n",
      "Iteration: 1208Loss: 2.9867537706630762\n",
      "Iteration: 1209Loss: 3.192698602068965\n",
      "Iteration: 1210Loss: 3.3579890014976264\n",
      "Iteration: 1211Loss: 3.0258314774570207\n",
      "Iteration: 1212Loss: 3.065020965254756\n",
      "Iteration: 1213Loss: 3.151842653485363\n",
      "Iteration: 1214Loss: 3.0449463006130113\n",
      "Iteration: 1215Loss: 3.080485426183825\n",
      "Iteration: 1216Loss: 3.1482281600049395\n",
      "Iteration: 1217Loss: 3.022420299598852\n",
      "Iteration: 1218Loss: 2.978427584234037\n",
      "Iteration: 1219Loss: 3.1219942429673204\n",
      "Iteration: 1220Loss: 2.9179694174763053\n",
      "Iteration: 1221Loss: 3.003248811606467\n",
      "Iteration: 1222Loss: 3.1818978126265636\n",
      "Iteration: 1223Loss: 2.9189794590929523\n",
      "Iteration: 1224Loss: 3.0493904261842486\n",
      "Iteration: 1225Loss: 3.001327694782949\n",
      "Iteration: 1226Loss: 3.024925408764922\n",
      "Iteration: 1227Loss: 3.0071675696329754\n",
      "Iteration: 1228Loss: 3.177022250450796\n",
      "Iteration: 1229Loss: 2.883372148182466\n",
      "Iteration: 1230Loss: 3.2419176749241205\n",
      "Iteration: 1231Loss: 2.9748301259217427\n",
      "Iteration: 1232Loss: 3.1312250495660074\n",
      "Iteration: 1233Loss: 3.199575282862946\n",
      "Iteration: 1234Loss: 2.964513566267366\n",
      "Iteration: 1235Loss: 3.1463931678274237\n",
      "Iteration: 1236Loss: 3.2529802509339447\n",
      "Iteration: 1237Loss: 3.2861288606179353\n",
      "Iteration: 1238Loss: 3.113585985139508\n",
      "Iteration: 1239Loss: 3.2416908075298108\n",
      "Iteration: 1240Loss: 3.309573496717457\n",
      "Iteration: 1241Loss: 3.0990760857928703\n",
      "Iteration: 1242Loss: 2.8982455291909783\n",
      "Iteration: 1243Loss: 3.0026181145108204\n",
      "Iteration: 1244Loss: 2.9797455972220463\n",
      "Iteration: 1245Loss: 3.0510504028871512\n",
      "Iteration: 1246Loss: 2.7312803707295163\n",
      "Iteration: 1247Loss: 2.9022169025498155\n",
      "Iteration: 1248Loss: 3.1687320638504177\n",
      "Iteration: 1249Loss: 3.0777715652144138\n",
      "Iteration: 1250Loss: 2.824784132734507\n",
      "Iteration: 1251Loss: 3.26595568117052\n",
      "Iteration: 1252Loss: 3.1957744027790556\n",
      "Iteration: 1253Loss: 3.0338629101034997\n",
      "Iteration: 1254Loss: 3.0250829486034267\n",
      "Iteration: 1255Loss: 2.8930889939653284\n",
      "Iteration: 1256Loss: 3.26162843458085\n",
      "Iteration: 1257Loss: 3.077557470139299\n",
      "Iteration: 1258Loss: 3.1477415455770017\n",
      "Iteration: 1259Loss: 3.0493999342098066\n",
      "Iteration: 1260Loss: 3.279892105383549\n",
      "Iteration: 1261Loss: 3.093756540985553\n",
      "Iteration: 1262Loss: 3.207454123548517\n",
      "Iteration: 1263Loss: 2.8850129589539453\n",
      "Iteration: 1264Loss: 2.9055263025505815\n",
      "Iteration: 1265Loss: 2.9954051572615095\n",
      "Iteration: 1266Loss: 2.8742661060624117\n",
      "Iteration: 1267Loss: 2.8217683149003863\n",
      "Iteration: 1268Loss: 3.1739769107472076\n",
      "Iteration: 1269Loss: 3.030572941603263\n",
      "Iteration: 1270Loss: 3.270462362078933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1271Loss: 3.0801032770387415\n",
      "Iteration: 1272Loss: 3.271282327922682\n",
      "Iteration: 1273Loss: 3.110083214335316\n",
      "Iteration: 1274Loss: 3.2891044038236865\n",
      "Iteration: 1275Loss: 3.0145786712187403\n",
      "Iteration: 1276Loss: 3.0604495438524806\n",
      "Iteration: 1277Loss: 3.2445847877130216\n",
      "Iteration: 1278Loss: 3.110743843441917\n",
      "Iteration: 1279Loss: 2.9392987964503723\n",
      "Iteration: 1280Loss: 3.0749064479275607\n",
      "Iteration: 1281Loss: 3.1305149646817245\n",
      "Iteration: 1282Loss: 2.887635132258663\n",
      "Iteration: 1283Loss: 3.2161118629670655\n",
      "Iteration: 1284Loss: 3.1424794837197565\n",
      "Iteration: 1285Loss: 3.27252342833836\n",
      "Iteration: 1286Loss: 3.4272157996599573\n",
      "Iteration: 1287Loss: 3.008365724264953\n",
      "Iteration: 1288Loss: 2.9604160027651045\n",
      "Iteration: 1289Loss: 3.34737498177633\n",
      "Iteration: 1290Loss: 2.7790253848292443\n",
      "Iteration: 1291Loss: 3.175533876647505\n",
      "Iteration: 1292Loss: 3.161441784580143\n",
      "Iteration: 1293Loss: 2.9859702753666704\n",
      "Iteration: 1294Loss: 2.935855145010547\n",
      "Iteration: 1295Loss: 3.3399246243727956\n",
      "Iteration: 1296Loss: 3.198156735240284\n",
      "Iteration: 1297Loss: 3.09931720956402\n",
      "Iteration: 1298Loss: 3.0939419946468356\n",
      "Iteration: 1299Loss: 2.904982273904883\n",
      "Iteration: 1300Loss: 3.1170566326543794\n",
      "Iteration: 1301Loss: 3.1840423803083477\n",
      "Iteration: 1302Loss: 3.0142693361044643\n",
      "Iteration: 1303Loss: 3.1266007103007425\n",
      "Iteration: 1304Loss: 2.9062906662267607\n",
      "Iteration: 1305Loss: 3.026226420004768\n",
      "Iteration: 1306Loss: 3.0363625218474923\n",
      "Iteration: 1307Loss: 3.3501623075417415\n",
      "Iteration: 1308Loss: 3.200623175948174\n",
      "Iteration: 1309Loss: 2.852648721052668\n",
      "Iteration: 1310Loss: 3.0169966947032956\n",
      "Iteration: 1311Loss: 3.0196053135310277\n",
      "Iteration: 1312Loss: 3.0118420311313776\n",
      "Iteration: 1313Loss: 3.041520942544665\n",
      "Iteration: 1314Loss: 2.9616313829995513\n",
      "Iteration: 1315Loss: 3.1995315164255262\n",
      "Iteration: 1316Loss: 2.924589003137842\n",
      "Iteration: 1317Loss: 2.8476246680190536\n",
      "Iteration: 1318Loss: 2.9963754732555605\n",
      "Iteration: 1319Loss: 3.0667909430923497\n",
      "Iteration: 1320Loss: 3.039466555873604\n",
      "Iteration: 1321Loss: 2.937353128714389\n",
      "Iteration: 1322Loss: 2.957304570181698\n",
      "Iteration: 1323Loss: 3.018655819792686\n",
      "Iteration: 1324Loss: 2.934245687501681\n",
      "Iteration: 1325Loss: 3.3763522157849755\n",
      "Iteration: 1326Loss: 2.932722262859655\n",
      "Iteration: 1327Loss: 3.0552381908362682\n",
      "Iteration: 1328Loss: 2.8344609321938696\n",
      "Iteration: 1329Loss: 3.2140893174137424\n",
      "Iteration: 1330Loss: 2.8289760575323797\n",
      "Iteration: 1331Loss: 3.0431654062081157\n",
      "Iteration: 1332Loss: 2.974264333620019\n",
      "Iteration: 1333Loss: 3.121317326679496\n",
      "Iteration: 1334Loss: 3.039253394674472\n",
      "Iteration: 1335Loss: 3.0343356842833353\n",
      "Iteration: 1336Loss: 3.4053551293519466\n",
      "Iteration: 1337Loss: 3.290188764960372\n",
      "Iteration: 1338Loss: 3.05059761828004\n",
      "Iteration: 1339Loss: 2.935831297470936\n",
      "Iteration: 1340Loss: 3.0182382917813717\n",
      "Iteration: 1341Loss: 3.0243668487908533\n",
      "Iteration: 1342Loss: 2.7792695994135137\n",
      "Iteration: 1343Loss: 3.283160220130349\n",
      "Iteration: 1344Loss: 2.9522671302240684\n",
      "Iteration: 1345Loss: 3.0393183434485813\n",
      "Iteration: 1346Loss: 3.021539722675977\n",
      "Iteration: 1347Loss: 3.1469451779186137\n",
      "Iteration: 1348Loss: 2.9273839919641613\n",
      "Iteration: 1349Loss: 2.790168133352946\n",
      "Iteration: 1350Loss: 3.2061828083177946\n",
      "Iteration: 1351Loss: 3.0663728790368348\n",
      "Iteration: 1352Loss: 3.2094639234552136\n",
      "Iteration: 1353Loss: 3.302760974067922\n",
      "Iteration: 1354Loss: 2.9296270914918856\n",
      "Iteration: 1355Loss: 3.060107506439917\n",
      "Iteration: 1356Loss: 3.071178976059704\n",
      "Iteration: 1357Loss: 3.120949005565813\n",
      "Iteration: 1358Loss: 2.8890464746481364\n",
      "Iteration: 1359Loss: 2.978623319904634\n",
      "Iteration: 1360Loss: 2.95262308746678\n",
      "Iteration: 1361Loss: 2.970266233290066\n",
      "Iteration: 1362Loss: 3.029231189362084\n",
      "Iteration: 1363Loss: 3.0163855764655825\n",
      "Iteration: 1364Loss: 3.333692339923361\n",
      "Iteration: 1365Loss: 2.6168708493130635\n",
      "Iteration: 1366Loss: 3.092475444245068\n",
      "Iteration: 1367Loss: 3.064438519320439\n",
      "Iteration: 1368Loss: 2.942402972399488\n",
      "Iteration: 1369Loss: 2.881047039893456\n",
      "Iteration: 1370Loss: 3.116692852926701\n",
      "Iteration: 1371Loss: 3.1421842688478314\n",
      "Iteration: 1372Loss: 2.914348600264731\n",
      "Iteration: 1373Loss: 3.149392758253344\n",
      "Iteration: 1374Loss: 3.103648280400331\n",
      "Iteration: 1375Loss: 3.3006102432091975\n",
      "Iteration: 1376Loss: 3.098067845927767\n",
      "Iteration: 1377Loss: 3.0223806052452127\n",
      "Iteration: 1378Loss: 3.245895692269796\n",
      "Iteration: 1379Loss: 2.974633943992667\n",
      "Iteration: 1380Loss: 2.7466358658780585\n",
      "Iteration: 1381Loss: 2.8689750980923128\n",
      "Iteration: 1382Loss: 2.922166076128796\n",
      "Iteration: 1383Loss: 3.062607792368493\n",
      "Iteration: 1384Loss: 3.054592269566632\n",
      "Iteration: 1385Loss: 3.0515259398037897\n",
      "Iteration: 1386Loss: 2.762412467730669\n",
      "Iteration: 1387Loss: 2.9529220459194025\n",
      "Iteration: 1388Loss: 3.3184180686281226\n",
      "Iteration: 1389Loss: 2.8547615655232814\n",
      "Iteration: 1390Loss: 3.0098185440943226\n",
      "Iteration: 1391Loss: 3.126199781417217\n",
      "Iteration: 1392Loss: 3.3173716946439717\n",
      "Iteration: 1393Loss: 3.0631358667382416\n",
      "Iteration: 1394Loss: 2.9832037312068764\n",
      "Iteration: 1395Loss: 3.2180819364735878\n",
      "Iteration: 1396Loss: 3.1089759886437727\n",
      "Iteration: 1397Loss: 3.058163003954417\n",
      "Iteration: 1398Loss: 2.9947685874064214\n",
      "Iteration: 1399Loss: 3.330023392387776\n",
      "Iteration: 1400Loss: 2.8558418266521537\n",
      "Iteration: 1401Loss: 3.182866302976192\n",
      "Iteration: 1402Loss: 3.1941393508146017\n",
      "Iteration: 1403Loss: 2.6871503508526167\n",
      "Iteration: 1404Loss: 3.1342842092106826\n",
      "Iteration: 1405Loss: 3.3772399089331415\n",
      "Iteration: 1406Loss: 3.1528271711300437\n",
      "Iteration: 1407Loss: 2.8798863702535193\n",
      "Iteration: 1408Loss: 3.0515954858605183\n",
      "Iteration: 1409Loss: 2.798818476694726\n",
      "Iteration: 1410Loss: 3.105583441858231\n",
      "Iteration: 1411Loss: 2.9012233512669106\n",
      "Iteration: 1412Loss: 3.18527673391945\n",
      "Iteration: 1413Loss: 2.831638237366932\n",
      "Iteration: 1414Loss: 2.856759217936152\n",
      "Iteration: 1415Loss: 3.278621401729541\n",
      "Iteration: 1416Loss: 3.25715082766963\n",
      "Iteration: 1417Loss: 3.2745114264007706\n",
      "Iteration: 1418Loss: 3.3446452373912408\n",
      "Iteration: 1419Loss: 2.8417991317402747\n",
      "Iteration: 1420Loss: 3.1178202792726006\n",
      "Iteration: 1421Loss: 2.8665430364119286\n",
      "Iteration: 1422Loss: 2.9023643658781904\n",
      "Iteration: 1423Loss: 2.887997446530628\n",
      "Iteration: 1424Loss: 3.127115590425573\n",
      "Iteration: 1425Loss: 2.881375525204182\n",
      "Iteration: 1426Loss: 2.888406324104683\n",
      "Iteration: 1427Loss: 3.149385521577559\n",
      "Iteration: 1428Loss: 2.697139101498131\n",
      "Iteration: 1429Loss: 2.938403562200737\n",
      "Iteration: 1430Loss: 3.004507707085361\n",
      "Iteration: 1431Loss: 3.024718423048598\n",
      "Iteration: 1432Loss: 2.9483928055802244\n",
      "Iteration: 1433Loss: 3.039916329625724\n",
      "Iteration: 1434Loss: 3.3051583491932086\n",
      "Iteration: 1435Loss: 3.019876551566401\n",
      "Iteration: 1436Loss: 2.968117584567048\n",
      "Iteration: 1437Loss: 2.8942117336179805\n",
      "Iteration: 1438Loss: 2.7998429654357144\n",
      "Iteration: 1439Loss: 3.027225245266274\n",
      "Iteration: 1440Loss: 2.8503243405640424\n",
      "Iteration: 1441Loss: 2.9367626201309545\n",
      "Iteration: 1442Loss: 3.1954340664427456\n",
      "Iteration: 1443Loss: 2.9530347250082523\n",
      "Iteration: 1444Loss: 2.885235536083669\n",
      "Iteration: 1445Loss: 3.2616564758028344\n",
      "Iteration: 1446Loss: 2.8697878066495677\n",
      "Iteration: 1447Loss: 3.091101764617876\n",
      "Iteration: 1448Loss: 3.097653828508062\n",
      "Iteration: 1449Loss: 2.833117737090469\n",
      "Iteration: 1450Loss: 2.9017586533871844\n",
      "Iteration: 1451Loss: 2.91680654094946\n",
      "Iteration: 1452Loss: 3.091486605912236\n",
      "Iteration: 1453Loss: 2.9797311276128102\n",
      "Iteration: 1454Loss: 2.861349016954896\n",
      "Iteration: 1455Loss: 3.0915274488409086\n",
      "Iteration: 1456Loss: 3.0651234978263697\n",
      "Iteration: 1457Loss: 3.114191792163005\n",
      "Iteration: 1458Loss: 2.954692336366508\n",
      "Iteration: 1459Loss: 3.277334040687712\n",
      "Iteration: 1460Loss: 2.993156459948255\n",
      "Iteration: 1461Loss: 2.9327317720653454\n",
      "Iteration: 1462Loss: 2.973134385997328\n",
      "Iteration: 1463Loss: 2.7644193273517885\n",
      "Iteration: 1464Loss: 3.272269391215377\n",
      "Iteration: 1465Loss: 3.1500377483947024\n",
      "Iteration: 1466Loss: 3.024146207955246\n",
      "Iteration: 1467Loss: 3.0407450995839413\n",
      "Iteration: 1468Loss: 3.0590934535285506\n",
      "Iteration: 1469Loss: 3.067458746917745\n",
      "Iteration: 1470Loss: 2.9033622611910332\n",
      "Iteration: 1471Loss: 3.0532055467731163\n",
      "Iteration: 1472Loss: 2.957411302371978\n",
      "Iteration: 1473Loss: 3.1003585915861187\n",
      "Iteration: 1474Loss: 3.1300068068153717\n",
      "Iteration: 1475Loss: 2.8194819689294786\n",
      "Iteration: 1476Loss: 2.814232433734127\n",
      "Iteration: 1477Loss: 3.1462353090046777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1478Loss: 2.951296096182734\n",
      "Iteration: 1479Loss: 2.912671049408084\n",
      "Iteration: 1480Loss: 2.860158989823876\n",
      "Iteration: 1481Loss: 3.0986866234387787\n",
      "Iteration: 1482Loss: 2.8852112309837024\n",
      "Iteration: 1483Loss: 3.0513561932904714\n",
      "Iteration: 1484Loss: 2.945037222476964\n",
      "Iteration: 1485Loss: 2.999710682477846\n",
      "Iteration: 1486Loss: 2.850130323865896\n",
      "Iteration: 1487Loss: 2.868374539852979\n",
      "Iteration: 1488Loss: 2.9044206542587734\n",
      "Iteration: 1489Loss: 3.0515460289137377\n",
      "Iteration: 1490Loss: 2.954964682827776\n",
      "Iteration: 1491Loss: 3.119002137419518\n",
      "Iteration: 1492Loss: 3.1608087648102927\n",
      "Iteration: 1493Loss: 3.0088431019737287\n",
      "Iteration: 1494Loss: 3.0609723516361487\n",
      "Iteration: 1495Loss: 2.9403502549933194\n",
      "Iteration: 1496Loss: 3.0276486732457815\n",
      "Iteration: 1497Loss: 3.083060662408133\n",
      "Iteration: 1498Loss: 2.8528967548902404\n",
      "Iteration: 1499Loss: 2.564235674363964\n",
      "Iteration: 1500Loss: 3.1689233566336346\n",
      "Iteration: 1501Loss: 2.93499955060177\n",
      "Iteration: 1502Loss: 2.7537842634661547\n",
      "Iteration: 1503Loss: 2.7685671017741007\n",
      "Iteration: 1504Loss: 2.9095221019949657\n",
      "Iteration: 1505Loss: 3.1512736744465397\n",
      "Iteration: 1506Loss: 2.923982049658209\n",
      "Iteration: 1507Loss: 3.003802189074799\n",
      "Iteration: 1508Loss: 2.8255794538415144\n",
      "Iteration: 1509Loss: 3.096199201338736\n",
      "Iteration: 1510Loss: 3.0481979996949216\n",
      "Iteration: 1511Loss: 3.115107950665456\n",
      "Iteration: 1512Loss: 2.755400755304795\n",
      "Iteration: 1513Loss: 3.0356871629045403\n",
      "Iteration: 1514Loss: 2.8496617639345154\n",
      "Iteration: 1515Loss: 3.030268031908759\n",
      "Iteration: 1516Loss: 3.0332349328462667\n",
      "Iteration: 1517Loss: 3.390741942739965\n",
      "Iteration: 1518Loss: 2.810292177707114\n",
      "Iteration: 1519Loss: 2.7523731879810702\n",
      "Iteration: 1520Loss: 3.0326993837066967\n",
      "Iteration: 1521Loss: 2.96150893161156\n",
      "Iteration: 1522Loss: 2.6629044501494397\n",
      "Iteration: 1523Loss: 3.2102982715894757\n",
      "Iteration: 1524Loss: 3.0294456812898956\n",
      "Iteration: 1525Loss: 2.862068350368645\n",
      "Iteration: 1526Loss: 3.138506424566396\n",
      "Iteration: 1527Loss: 2.787974809188245\n",
      "Iteration: 1528Loss: 2.9059571221695473\n",
      "Iteration: 1529Loss: 2.982355750082726\n",
      "Iteration: 1530Loss: 2.9873774093687535\n",
      "Iteration: 1531Loss: 3.0733489493421096\n",
      "Iteration: 1532Loss: 2.957213132356542\n",
      "Iteration: 1533Loss: 2.856149571089862\n",
      "Iteration: 1534Loss: 3.0066796491046075\n",
      "Iteration: 1535Loss: 3.1705694513649862\n",
      "Iteration: 1536Loss: 2.8830283072329026\n",
      "Iteration: 1537Loss: 3.038590688400028\n",
      "Iteration: 1538Loss: 3.282648735989049\n",
      "Iteration: 1539Loss: 3.207162375579465\n",
      "Iteration: 1540Loss: 3.0963964856007924\n",
      "Iteration: 1541Loss: 3.074716141636786\n",
      "Iteration: 1542Loss: 3.1407613409108834\n",
      "Iteration: 1543Loss: 2.9651761214207046\n",
      "Iteration: 1544Loss: 3.0057749114224412\n",
      "Iteration: 1545Loss: 2.98476008870981\n",
      "Iteration: 1546Loss: 2.715908139615997\n",
      "Iteration: 1547Loss: 2.8675718085578064\n",
      "Iteration: 1548Loss: 2.750255640516567\n",
      "Iteration: 1549Loss: 2.963164626326702\n",
      "Iteration: 1550Loss: 3.037321010366141\n",
      "Iteration: 1551Loss: 2.8942694590459097\n",
      "Iteration: 1552Loss: 2.671416700695744\n",
      "Iteration: 1553Loss: 2.9376459760726945\n",
      "Iteration: 1554Loss: 2.9606497436136343\n",
      "Iteration: 1555Loss: 2.855294969333288\n",
      "Iteration: 1556Loss: 3.1828908079742733\n",
      "Iteration: 1557Loss: 2.946511836598465\n",
      "Iteration: 1558Loss: 3.1017843691674485\n",
      "Iteration: 1559Loss: 2.9741385999221968\n",
      "Iteration: 1560Loss: 2.965066514106758\n",
      "Iteration: 1561Loss: 3.0781389488342223\n",
      "Iteration: 1562Loss: 3.0622062793650424\n",
      "Iteration: 1563Loss: 2.985259589674226\n",
      "Iteration: 1564Loss: 2.9176855012308804\n",
      "Iteration: 1565Loss: 2.9768549266023254\n",
      "Iteration: 1566Loss: 2.8633700578764696\n",
      "Iteration: 1567Loss: 3.223038580081033\n",
      "Iteration: 1568Loss: 3.0353610343573556\n",
      "Iteration: 1569Loss: 3.236840325421345\n",
      "Iteration: 1570Loss: 3.2146277211200682\n",
      "Iteration: 1571Loss: 2.859866706911707\n",
      "Iteration: 1572Loss: 3.069986445325668\n",
      "Iteration: 1573Loss: 3.2095666653948998\n",
      "Iteration: 1574Loss: 2.872618206189229\n",
      "Iteration: 1575Loss: 2.989560444786669\n",
      "Iteration: 1576Loss: 3.06443744417553\n",
      "Iteration: 1577Loss: 2.9536341853678207\n",
      "Iteration: 1578Loss: 3.159559984412044\n",
      "Iteration: 1579Loss: 2.948701385832287\n",
      "Iteration: 1580Loss: 3.178667035612491\n",
      "Iteration: 1581Loss: 2.967367281296372\n",
      "Iteration: 1582Loss: 3.2440622126196286\n",
      "Iteration: 1583Loss: 3.143732467157026\n",
      "Iteration: 1584Loss: 3.0916673584246377\n",
      "Iteration: 1585Loss: 2.9216312531819617\n",
      "Iteration: 1586Loss: 2.9599635104401685\n",
      "Iteration: 1587Loss: 2.9506698283400468\n",
      "Iteration: 1588Loss: 3.00135932126752\n",
      "Iteration: 1589Loss: 2.8940506655101976\n",
      "Iteration: 1590Loss: 3.0614661891820454\n",
      "Iteration: 1591Loss: 2.8935395582076495\n",
      "Iteration: 1592Loss: 2.9452652378003195\n",
      "Iteration: 1593Loss: 3.0210967158010535\n",
      "Iteration: 1594Loss: 2.8241855416542445\n",
      "Iteration: 1595Loss: 3.092090261847313\n",
      "Iteration: 1596Loss: 3.1092746806808\n",
      "Iteration: 1597Loss: 2.7468939874202976\n",
      "Iteration: 1598Loss: 2.8410922923313726\n",
      "Iteration: 1599Loss: 3.127272807408097\n",
      "Iteration: 1600Loss: 2.932599455472561\n",
      "Iteration: 1601Loss: 2.9450457870711326\n",
      "Iteration: 1602Loss: 3.101055418343641\n",
      "Iteration: 1603Loss: 2.9666120430411516\n",
      "Iteration: 1604Loss: 3.2569416158115563\n",
      "Iteration: 1605Loss: 2.9833224665517406\n",
      "Iteration: 1606Loss: 3.0560786496266394\n",
      "Iteration: 1607Loss: 2.914526142713562\n",
      "Iteration: 1608Loss: 3.040728420524683\n",
      "Iteration: 1609Loss: 2.9123926397621136\n",
      "Iteration: 1610Loss: 3.009207738044707\n",
      "Iteration: 1611Loss: 2.947374089740586\n",
      "Iteration: 1612Loss: 3.0350401353807497\n",
      "Iteration: 1613Loss: 2.774484470046641\n",
      "Iteration: 1614Loss: 2.9152278486479957\n",
      "Iteration: 1615Loss: 2.973006576800924\n",
      "Iteration: 1616Loss: 3.1502426486813975\n",
      "Iteration: 1617Loss: 2.9962088574522046\n",
      "Iteration: 1618Loss: 2.446514582241628\n",
      "Iteration: 1619Loss: 3.0849187221314747\n",
      "Iteration: 1620Loss: 2.8901656272743517\n",
      "Iteration: 1621Loss: 3.3929325278726656\n",
      "Iteration: 1622Loss: 3.1979644283805593\n",
      "Iteration: 1623Loss: 2.79403812189051\n",
      "Iteration: 1624Loss: 3.054057511554265\n",
      "Iteration: 1625Loss: 2.798064831098471\n",
      "Iteration: 1626Loss: 3.0953930454766927\n",
      "Iteration: 1627Loss: 3.092466098875816\n",
      "Iteration: 1628Loss: 2.7692882738317603\n",
      "Iteration: 1629Loss: 2.9444674394859236\n",
      "Iteration: 1630Loss: 2.826467162143747\n",
      "Iteration: 1631Loss: 2.973835190814351\n",
      "Iteration: 1632Loss: 3.307373691679289\n",
      "Iteration: 1633Loss: 3.0160047873388978\n",
      "Iteration: 1634Loss: 2.887819950728534\n",
      "Iteration: 1635Loss: 3.044936032929347\n",
      "Iteration: 1636Loss: 3.0697169383212883\n",
      "Iteration: 1637Loss: 2.970505340130293\n",
      "Iteration: 1638Loss: 2.8975800497733646\n",
      "Iteration: 1639Loss: 2.920212224720209\n",
      "Iteration: 1640Loss: 2.8973388224301964\n",
      "Iteration: 1641Loss: 3.0663337552881678\n",
      "Iteration: 1642Loss: 3.2841014656613208\n",
      "Iteration: 1643Loss: 2.9887650372742227\n",
      "Iteration: 1644Loss: 3.019667659682739\n",
      "Iteration: 1645Loss: 2.7558047184802126\n",
      "Iteration: 1646Loss: 2.8337093020744395\n",
      "Iteration: 1647Loss: 2.9331143227100074\n",
      "Iteration: 1648Loss: 2.8734130347147584\n",
      "Iteration: 1649Loss: 3.218518223448624\n",
      "Iteration: 1650Loss: 3.120317925496267\n",
      "Iteration: 1651Loss: 3.032562176008302\n",
      "Iteration: 1652Loss: 2.907984737217238\n",
      "Iteration: 1653Loss: 2.846495200672167\n",
      "Iteration: 1654Loss: 2.814313260013682\n",
      "Iteration: 1655Loss: 2.8123324754430397\n",
      "Iteration: 1656Loss: 2.903078991292772\n",
      "Iteration: 1657Loss: 2.8300513359588346\n",
      "Iteration: 1658Loss: 2.946417868997573\n",
      "Iteration: 1659Loss: 3.0736100841447143\n",
      "Iteration: 1660Loss: 2.9143554523772517\n",
      "Iteration: 1661Loss: 3.2877137625596764\n",
      "Iteration: 1662Loss: 2.8381971944533073\n",
      "Iteration: 1663Loss: 2.9179424291097686\n",
      "Iteration: 1664Loss: 3.022955938959293\n",
      "Iteration: 1665Loss: 2.9553444049791504\n",
      "Iteration: 1666Loss: 3.0957796834285904\n",
      "Iteration: 1667Loss: 2.690589303401514\n",
      "Iteration: 1668Loss: 3.0908247549639647\n",
      "Iteration: 1669Loss: 2.9893127624591194\n",
      "Iteration: 1670Loss: 3.069057960206938\n",
      "Iteration: 1671Loss: 2.8525490893997794\n",
      "Iteration: 1672Loss: 2.8306766769205884\n",
      "Iteration: 1673Loss: 3.040024560844517\n",
      "Iteration: 1674Loss: 2.7213371148764938\n",
      "Iteration: 1675Loss: 3.0451583298170775\n",
      "Iteration: 1676Loss: 2.9597941917766417\n",
      "Iteration: 1677Loss: 2.9484275879823874\n",
      "Iteration: 1678Loss: 3.1010366177050077\n",
      "Iteration: 1679Loss: 2.914058306005767\n",
      "Iteration: 1680Loss: 2.937646598372042\n",
      "Iteration: 1681Loss: 2.9251087059053673\n",
      "Iteration: 1682Loss: 3.126321750907291\n",
      "Iteration: 1683Loss: 3.0667941269194294\n",
      "Iteration: 1684Loss: 2.8651925389777415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1685Loss: 2.9022094498358277\n",
      "Iteration: 1686Loss: 2.8438401325976335\n",
      "Iteration: 1687Loss: 2.9639225956743234\n",
      "Iteration: 1688Loss: 2.897880104724019\n",
      "Iteration: 1689Loss: 3.0091718319302596\n",
      "Iteration: 1690Loss: 3.2017962452935667\n",
      "Iteration: 1691Loss: 2.804053153703885\n",
      "Iteration: 1692Loss: 2.811326810289127\n",
      "Iteration: 1693Loss: 3.1615262871952643\n",
      "Iteration: 1694Loss: 2.9474524959422665\n",
      "Iteration: 1695Loss: 2.812039705667606\n",
      "Iteration: 1696Loss: 2.715063726007138\n",
      "Iteration: 1697Loss: 2.9921721633874476\n",
      "Iteration: 1698Loss: 3.0303541566988854\n",
      "Iteration: 1699Loss: 3.0585962774576965\n",
      "Iteration: 1700Loss: 3.029344618061636\n",
      "Iteration: 1701Loss: 3.029527409040198\n",
      "Iteration: 1702Loss: 2.8971818905611286\n",
      "Iteration: 1703Loss: 2.9302732048972664\n",
      "Iteration: 1704Loss: 3.1941935134087416\n",
      "Iteration: 1705Loss: 3.061539720963304\n",
      "Iteration: 1706Loss: 2.799826525083286\n",
      "Iteration: 1707Loss: 2.948259258823192\n",
      "Iteration: 1708Loss: 2.7678463280811214\n",
      "Iteration: 1709Loss: 3.008194119349827\n",
      "Iteration: 1710Loss: 2.882147457525134\n",
      "Iteration: 1711Loss: 2.867506790351299\n",
      "Iteration: 1712Loss: 2.998926833131778\n",
      "Iteration: 1713Loss: 2.5968057202368517\n",
      "Iteration: 1714Loss: 2.870992581363035\n",
      "Iteration: 1715Loss: 2.893561789567031\n",
      "Iteration: 1716Loss: 2.971811867000016\n",
      "Iteration: 1717Loss: 2.902380157956301\n",
      "Iteration: 1718Loss: 3.0444265632052914\n",
      "Iteration: 1719Loss: 3.1555020080953486\n",
      "Iteration: 1720Loss: 2.95956134842351\n",
      "Iteration: 1721Loss: 2.8577417048920943\n",
      "Iteration: 1722Loss: 3.0862716315734406\n",
      "Iteration: 1723Loss: 3.0141884488080306\n",
      "Iteration: 1724Loss: 2.909249228645046\n",
      "Iteration: 1725Loss: 2.626455398274781\n",
      "Iteration: 1726Loss: 2.9268384844805664\n",
      "Iteration: 1727Loss: 2.861588202927864\n",
      "Iteration: 1728Loss: 2.6799384341499692\n",
      "Iteration: 1729Loss: 2.9313040384545297\n",
      "Iteration: 1730Loss: 2.918831761478673\n",
      "Iteration: 1731Loss: 3.0406676478442045\n",
      "Iteration: 1732Loss: 2.9428496915076594\n",
      "Iteration: 1733Loss: 2.9244984287299918\n",
      "Iteration: 1734Loss: 2.875601710821444\n",
      "Iteration: 1735Loss: 3.1656280548965885\n",
      "Iteration: 1736Loss: 2.9863736139341572\n",
      "Iteration: 1737Loss: 2.943483216021835\n",
      "Iteration: 1738Loss: 2.9956230170730045\n",
      "Iteration: 1739Loss: 2.829335455758874\n",
      "Iteration: 1740Loss: 2.969850172212378\n",
      "Iteration: 1741Loss: 2.961564781192245\n",
      "Iteration: 1742Loss: 3.037264979377092\n",
      "Iteration: 1743Loss: 2.866824300246251\n",
      "Iteration: 1744Loss: 3.0362462626361952\n",
      "Iteration: 1745Loss: 3.053814817087177\n",
      "Iteration: 1746Loss: 2.8584705996303645\n",
      "Iteration: 1747Loss: 3.1497421890109925\n",
      "Iteration: 1748Loss: 2.8718292646388326\n",
      "Iteration: 1749Loss: 2.9087091579905335\n",
      "Iteration: 1750Loss: 3.0593496517469294\n",
      "Iteration: 1751Loss: 2.7592986125871537\n",
      "Iteration: 1752Loss: 3.0787827323598753\n",
      "Iteration: 1753Loss: 2.9898648029061383\n",
      "Iteration: 1754Loss: 3.0415938387122137\n",
      "Iteration: 1755Loss: 3.104218573161847\n",
      "Iteration: 1756Loss: 2.710129481988566\n",
      "Iteration: 1757Loss: 2.7178218258238447\n",
      "Iteration: 1758Loss: 2.966462714145437\n",
      "Iteration: 1759Loss: 2.8038470892594582\n",
      "Iteration: 1760Loss: 2.8272151029797774\n",
      "Iteration: 1761Loss: 2.99196253803655\n",
      "Iteration: 1762Loss: 2.9550272252398555\n",
      "Iteration: 1763Loss: 2.9967930793774493\n",
      "Iteration: 1764Loss: 2.804525818274519\n",
      "Iteration: 1765Loss: 2.873262726250035\n",
      "Iteration: 1766Loss: 3.149214526653759\n",
      "Iteration: 1767Loss: 3.037376850342874\n",
      "Iteration: 1768Loss: 2.8508489782791417\n",
      "Iteration: 1769Loss: 3.0506004312413757\n",
      "Iteration: 1770Loss: 3.414444733767356\n",
      "Iteration: 1771Loss: 2.8533455235360377\n",
      "Iteration: 1772Loss: 2.7608735369037567\n",
      "Iteration: 1773Loss: 2.9298239795602954\n",
      "Iteration: 1774Loss: 2.8897034708527762\n",
      "Iteration: 1775Loss: 3.115413841764059\n",
      "Iteration: 1776Loss: 3.044837592528635\n",
      "Iteration: 1777Loss: 2.8918363944287684\n",
      "Iteration: 1778Loss: 2.7577408661616882\n",
      "Iteration: 1779Loss: 3.0505966984352835\n",
      "Iteration: 1780Loss: 2.9204430854891292\n",
      "Iteration: 1781Loss: 3.0413307115473907\n",
      "Iteration: 1782Loss: 2.9332536093686175\n",
      "Iteration: 1783Loss: 3.2027295827570987\n",
      "Iteration: 1784Loss: 3.1458843226339765\n",
      "Iteration: 1785Loss: 2.9452878276346284\n",
      "Iteration: 1786Loss: 2.8966686765333733\n",
      "Iteration: 1787Loss: 2.7237447273883295\n",
      "Iteration: 1788Loss: 2.8823828017137743\n",
      "Iteration: 1789Loss: 3.1481532854860026\n",
      "Iteration: 1790Loss: 2.955316357602126\n",
      "Iteration: 1791Loss: 2.8525843852648904\n",
      "Iteration: 1792Loss: 2.8549095613073714\n",
      "Iteration: 1793Loss: 2.8118423959499705\n",
      "Iteration: 1794Loss: 2.936821812892977\n",
      "Iteration: 1795Loss: 2.756179896964176\n",
      "Iteration: 1796Loss: 2.877838790486268\n",
      "Iteration: 1797Loss: 2.9610395768894064\n",
      "Iteration: 1798Loss: 3.0646299762639355\n",
      "Iteration: 1799Loss: 3.030687144002095\n",
      "Iteration: 1800Loss: 3.0150839988403724\n",
      "Iteration: 1801Loss: 2.6417060203341616\n",
      "Iteration: 1802Loss: 3.2099159500546484\n",
      "Iteration: 1803Loss: 2.8707403795889013\n",
      "Iteration: 1804Loss: 2.8482890361886497\n",
      "Iteration: 1805Loss: 2.7946945530208747\n",
      "Iteration: 1806Loss: 3.0095316041857094\n",
      "Iteration: 1807Loss: 3.0849954807446256\n",
      "Iteration: 1808Loss: 3.320619506294615\n",
      "Iteration: 1809Loss: 2.92836159305148\n",
      "Iteration: 1810Loss: 2.915304028023685\n",
      "Iteration: 1811Loss: 2.9881882739291656\n",
      "Iteration: 1812Loss: 3.052538983824132\n",
      "Iteration: 1813Loss: 3.0085611359472226\n",
      "Iteration: 1814Loss: 3.002698863108467\n",
      "Iteration: 1815Loss: 3.120075640904866\n",
      "Iteration: 1816Loss: 3.053467846560819\n",
      "Iteration: 1817Loss: 3.049534010900159\n",
      "Iteration: 1818Loss: 2.7608996114700726\n",
      "Iteration: 1819Loss: 2.86714868398585\n",
      "Iteration: 1820Loss: 3.0474843830282112\n",
      "Iteration: 1821Loss: 2.993979999840724\n",
      "Iteration: 1822Loss: 3.252250096866776\n",
      "Iteration: 1823Loss: 2.8580611620692125\n",
      "Iteration: 1824Loss: 2.9074395917114475\n",
      "Iteration: 1825Loss: 2.8528282836464327\n",
      "Iteration: 1826Loss: 3.0336959393643257\n",
      "Iteration: 1827Loss: 2.8486021564034893\n",
      "Iteration: 1828Loss: 2.8079154068679295\n",
      "Iteration: 1829Loss: 2.807158702847621\n",
      "Iteration: 1830Loss: 3.343330188892011\n",
      "Iteration: 1831Loss: 2.8344548116998833\n",
      "Iteration: 1832Loss: 2.85309871520371\n",
      "Iteration: 1833Loss: 2.7613352698200497\n",
      "Iteration: 1834Loss: 2.9062067077999623\n",
      "Iteration: 1835Loss: 2.9708133464417976\n",
      "Iteration: 1836Loss: 2.9171056102961304\n",
      "Iteration: 1837Loss: 2.8391238528204523\n",
      "Iteration: 1838Loss: 2.7472666851123284\n",
      "Iteration: 1839Loss: 3.0182451035253295\n",
      "Iteration: 1840Loss: 2.9666598782475506\n",
      "Iteration: 1841Loss: 2.679176630354825\n",
      "Iteration: 1842Loss: 2.8724055150333716\n",
      "Iteration: 1843Loss: 2.671700757196932\n",
      "Iteration: 1844Loss: 3.0653307088371795\n",
      "Iteration: 1845Loss: 2.919269061139823\n",
      "Iteration: 1846Loss: 2.7739703273968694\n",
      "Iteration: 1847Loss: 2.99867582689186\n",
      "Iteration: 1848Loss: 2.9015903830572607\n",
      "Iteration: 1849Loss: 3.211741620197586\n",
      "Iteration: 1850Loss: 2.956685815137716\n",
      "Iteration: 1851Loss: 3.0593113432620886\n",
      "Iteration: 1852Loss: 2.7016674718509117\n",
      "Iteration: 1853Loss: 2.9867173312139688\n",
      "Iteration: 1854Loss: 2.8512288872263705\n",
      "Iteration: 1855Loss: 2.800096936296207\n",
      "Iteration: 1856Loss: 2.735440800772674\n",
      "Iteration: 1857Loss: 2.929977007185268\n",
      "Iteration: 1858Loss: 2.6937333887363684\n",
      "Iteration: 1859Loss: 2.7767875845352785\n",
      "Iteration: 1860Loss: 2.942215526240783\n",
      "Iteration: 1861Loss: 3.1645800615891373\n",
      "Iteration: 1862Loss: 2.6552811045304976\n",
      "Iteration: 1863Loss: 2.92238364956518\n",
      "Iteration: 1864Loss: 3.156978887555681\n",
      "Iteration: 1865Loss: 2.836691531364639\n",
      "Iteration: 1866Loss: 2.995028488737781\n",
      "Iteration: 1867Loss: 2.857378376055604\n",
      "Iteration: 1868Loss: 2.845667606362586\n",
      "Iteration: 1869Loss: 2.738821873579312\n",
      "Iteration: 1870Loss: 2.932809821402496\n",
      "Iteration: 1871Loss: 3.297932989142945\n",
      "Iteration: 1872Loss: 2.8050552308593826\n",
      "Iteration: 1873Loss: 2.7192162230935955\n",
      "Iteration: 1874Loss: 3.115225791309506\n",
      "Iteration: 1875Loss: 3.076274365772782\n",
      "Iteration: 1876Loss: 3.1496571694709816\n",
      "Iteration: 1877Loss: 3.2782488864667516\n",
      "Iteration: 1878Loss: 2.9288982519993874\n",
      "Iteration: 1879Loss: 2.8912494055606346\n",
      "Iteration: 1880Loss: 2.867943281282269\n",
      "Iteration: 1881Loss: 2.972140602204115\n",
      "Iteration: 1882Loss: 2.650055254491244\n",
      "Iteration: 1883Loss: 2.9983634828744474\n",
      "Iteration: 1884Loss: 2.90534812406896\n",
      "Iteration: 1885Loss: 2.9967052921761286\n",
      "Iteration: 1886Loss: 2.9489123385702665\n",
      "Iteration: 1887Loss: 2.822807671753069\n",
      "Iteration: 1888Loss: 3.3189932309396335\n",
      "Iteration: 1889Loss: 2.9295260729155532\n",
      "Iteration: 1890Loss: 2.767954507145982\n",
      "Iteration: 1891Loss: 2.9382950090188444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1892Loss: 2.9745885490267363\n",
      "Iteration: 1893Loss: 2.9853406872163473\n",
      "Iteration: 1894Loss: 3.04223013563751\n",
      "Iteration: 1895Loss: 2.854396031487106\n",
      "Iteration: 1896Loss: 2.873512811272137\n",
      "Iteration: 1897Loss: 2.7762418615079865\n",
      "Iteration: 1898Loss: 3.0128006931888054\n",
      "Iteration: 1899Loss: 3.174066570777847\n",
      "Iteration: 1900Loss: 2.8821464983152563\n",
      "Iteration: 1901Loss: 3.048225511821393\n",
      "Iteration: 1902Loss: 2.897974877082187\n",
      "Iteration: 1903Loss: 2.7807894401410405\n",
      "Iteration: 1904Loss: 3.030671363981413\n",
      "Iteration: 1905Loss: 3.0126520580604614\n",
      "Iteration: 1906Loss: 2.880175108999366\n",
      "Iteration: 1907Loss: 3.008696809885208\n",
      "Iteration: 1908Loss: 2.8615692497077716\n",
      "Iteration: 1909Loss: 2.900717661227729\n",
      "Iteration: 1910Loss: 3.0474846698726816\n",
      "Iteration: 1911Loss: 2.8497675881846494\n",
      "Iteration: 1912Loss: 2.9908525875693384\n",
      "Iteration: 1913Loss: 2.9228119556552308\n",
      "Iteration: 1914Loss: 2.9261608311757787\n",
      "Iteration: 1915Loss: 2.90689698497362\n",
      "Iteration: 1916Loss: 3.0374600772401834\n",
      "Iteration: 1917Loss: 3.013359087241694\n",
      "Iteration: 1918Loss: 2.6124400284441878\n",
      "Iteration: 1919Loss: 2.932377110177619\n",
      "Iteration: 1920Loss: 3.1048217336618538\n",
      "Iteration: 1921Loss: 2.7308452733372914\n",
      "Iteration: 1922Loss: 2.97503736139061\n",
      "Iteration: 1923Loss: 2.914642657395691\n",
      "Iteration: 1924Loss: 2.9622884122494426\n",
      "Iteration: 1925Loss: 2.928601214450865\n",
      "Iteration: 1926Loss: 2.916726910901554\n",
      "Iteration: 1927Loss: 2.8027159245002307\n",
      "Iteration: 1928Loss: 2.821807595904054\n",
      "Iteration: 1929Loss: 2.6760861188867033\n",
      "Iteration: 1930Loss: 2.79573373799024\n",
      "Iteration: 1931Loss: 2.8810347626322366\n",
      "Iteration: 1932Loss: 2.7856863532435994\n",
      "Iteration: 1933Loss: 3.1827706695747096\n",
      "Iteration: 1934Loss: 2.6789151082217786\n",
      "Iteration: 1935Loss: 2.991193982946536\n",
      "Iteration: 1936Loss: 2.8119188143966207\n",
      "Iteration: 1937Loss: 2.7867961250686726\n",
      "Iteration: 1938Loss: 2.9122885052188945\n",
      "Iteration: 1939Loss: 2.88817412147128\n",
      "Iteration: 1940Loss: 2.83955306196837\n",
      "Iteration: 1941Loss: 2.8874046102391016\n",
      "Iteration: 1942Loss: 2.7546393818085253\n",
      "Iteration: 1943Loss: 2.990683985553987\n",
      "Iteration: 1944Loss: 2.8923417976968944\n",
      "Iteration: 1945Loss: 2.6384036078040176\n",
      "Iteration: 1946Loss: 2.873845864275577\n",
      "Iteration: 1947Loss: 2.690597261898232\n",
      "Iteration: 1948Loss: 2.8239510245433244\n",
      "Iteration: 1949Loss: 3.0456937993401416\n",
      "Iteration: 1950Loss: 3.0637758520238187\n",
      "Iteration: 1951Loss: 2.8138027224793203\n",
      "Iteration: 1952Loss: 2.915522256275465\n",
      "Iteration: 1953Loss: 2.7109480620988666\n",
      "Iteration: 1954Loss: 2.973264905748424\n",
      "Iteration: 1955Loss: 2.8383595682210574\n",
      "Iteration: 1956Loss: 3.200441230826328\n",
      "Iteration: 1957Loss: 3.1302033378464627\n",
      "Iteration: 1958Loss: 2.9311028687671037\n",
      "Iteration: 1959Loss: 2.8443086034404015\n",
      "Iteration: 1960Loss: 2.918828516129363\n",
      "Iteration: 1961Loss: 2.6844350313240626\n",
      "Iteration: 1962Loss: 2.80112958478524\n",
      "Iteration: 1963Loss: 2.7786680646524364\n",
      "Iteration: 1964Loss: 2.8427725123088767\n",
      "Iteration: 1965Loss: 2.652948568927346\n",
      "Iteration: 1966Loss: 2.8762943373252163\n",
      "Iteration: 1967Loss: 3.0258813821757613\n",
      "Iteration: 1968Loss: 2.9101196223883856\n",
      "Iteration: 1969Loss: 3.094545073500008\n",
      "Iteration: 1970Loss: 2.911079266529388\n",
      "Iteration: 1971Loss: 2.872368693592087\n",
      "Iteration: 1972Loss: 2.7479781485880013\n",
      "Iteration: 1973Loss: 2.7520872628717887\n",
      "Iteration: 1974Loss: 2.5659720732885454\n",
      "Iteration: 1975Loss: 2.6342223429628246\n",
      "Iteration: 1976Loss: 2.8949180596962045\n",
      "Iteration: 1977Loss: 2.9480221581015846\n",
      "Iteration: 1978Loss: 2.9812649859318343\n",
      "Iteration: 1979Loss: 2.9638601981074317\n",
      "Iteration: 1980Loss: 2.850695789641624\n",
      "Iteration: 1981Loss: 2.972528904723954\n",
      "Iteration: 1982Loss: 2.7059018131330768\n",
      "Iteration: 1983Loss: 2.6499563941998656\n",
      "Iteration: 1984Loss: 2.900225244171318\n",
      "Iteration: 1985Loss: 3.07656242345204\n",
      "Iteration: 1986Loss: 2.9777110670959144\n",
      "Iteration: 1987Loss: 2.7048193600523303\n",
      "Iteration: 1988Loss: 3.0874911955740285\n",
      "Iteration: 1989Loss: 2.6856755376062877\n",
      "Iteration: 1990Loss: 2.608447676684738\n",
      "Iteration: 1991Loss: 2.776101695315761\n",
      "Iteration: 1992Loss: 3.016432708627896\n",
      "Iteration: 1993Loss: 2.94475000105786\n",
      "Iteration: 1994Loss: 3.0698851231944317\n",
      "Iteration: 1995Loss: 2.6907710651120613\n",
      "Iteration: 1996Loss: 2.8674811823016935\n",
      "Iteration: 1997Loss: 2.784102935659499\n",
      "Iteration: 1998Loss: 2.788479840818731\n",
      "Iteration: 1999Loss: 2.6334990283313835\n",
      "Iteration: 2000Loss: 2.568620278722268\n",
      "Iteration: 2001Loss: 2.661306172785827\n",
      "Iteration: 2002Loss: 2.6748342238430407\n",
      "Iteration: 2003Loss: 2.8666631377587604\n",
      "Iteration: 2004Loss: 2.827774544765552\n",
      "Iteration: 2005Loss: 2.8326827029702253\n",
      "Iteration: 2006Loss: 2.8715482195378477\n",
      "Iteration: 2007Loss: 2.892693307463962\n",
      "Iteration: 2008Loss: 3.0045049647115727\n",
      "Iteration: 2009Loss: 2.9167510050395173\n",
      "Iteration: 2010Loss: 2.6795968473808647\n",
      "Iteration: 2011Loss: 2.6699886549273892\n",
      "Iteration: 2012Loss: 2.950972555724\n",
      "Iteration: 2013Loss: 2.8871011239283133\n",
      "Iteration: 2014Loss: 2.9859768661970585\n",
      "Iteration: 2015Loss: 2.9322594294079556\n",
      "Iteration: 2016Loss: 2.626050014007904\n",
      "Iteration: 2017Loss: 2.9767600438870345\n",
      "Iteration: 2018Loss: 2.9025707869980333\n",
      "Iteration: 2019Loss: 2.8030116524793867\n",
      "Iteration: 2020Loss: 2.999830704651323\n",
      "Iteration: 2021Loss: 2.885922844332161\n",
      "Iteration: 2022Loss: 2.7537222732974804\n",
      "Iteration: 2023Loss: 2.8615683622771715\n",
      "Iteration: 2024Loss: 2.7606196802368834\n",
      "Iteration: 2025Loss: 2.7664740934531213\n",
      "Iteration: 2026Loss: 2.909296316721358\n",
      "Iteration: 2027Loss: 2.8164518865026476\n",
      "Iteration: 2028Loss: 2.606755858592494\n",
      "Iteration: 2029Loss: 3.0892619173057745\n",
      "Iteration: 2030Loss: 2.8122579585880234\n",
      "Iteration: 2031Loss: 2.9321701794455883\n",
      "Iteration: 2032Loss: 2.8379099394746348\n",
      "Iteration: 2033Loss: 2.981718373186279\n",
      "Iteration: 2034Loss: 2.9940356272394197\n",
      "Iteration: 2035Loss: 2.9320955124980803\n",
      "Iteration: 2036Loss: 3.0582609930189455\n",
      "Iteration: 2037Loss: 2.8949942822467554\n",
      "Iteration: 2038Loss: 2.7436851332513243\n",
      "Iteration: 2039Loss: 2.7858999412587186\n",
      "Iteration: 2040Loss: 3.145509082842434\n",
      "Iteration: 2041Loss: 2.711224870451987\n",
      "Iteration: 2042Loss: 2.9127887722348116\n",
      "Iteration: 2043Loss: 3.030580595931142\n",
      "Iteration: 2044Loss: 2.7815900737463926\n",
      "Iteration: 2045Loss: 3.066671149479584\n",
      "Iteration: 2046Loss: 2.6954455312650283\n",
      "Iteration: 2047Loss: 2.8924132951311297\n",
      "Iteration: 2048Loss: 2.9526743779365483\n",
      "Iteration: 2049Loss: 2.8998591988252467\n",
      "Iteration: 2050Loss: 2.737047565218839\n",
      "Iteration: 2051Loss: 2.7106999649050456\n",
      "Iteration: 2052Loss: 2.9884271093616173\n",
      "Iteration: 2053Loss: 3.0168711229476703\n",
      "Iteration: 2054Loss: 2.748282974914728\n",
      "Iteration: 2055Loss: 2.805746949385091\n",
      "Iteration: 2056Loss: 3.0493887823495944\n",
      "Iteration: 2057Loss: 2.9847117334583335\n",
      "Iteration: 2058Loss: 2.827953556059439\n",
      "Iteration: 2059Loss: 2.8036156616667784\n",
      "Iteration: 2060Loss: 2.7634067376935154\n",
      "Iteration: 2061Loss: 2.8487892908607284\n",
      "Iteration: 2062Loss: 2.867940455333877\n",
      "Iteration: 2063Loss: 2.857869661057939\n",
      "Iteration: 2064Loss: 2.5955535107545695\n",
      "Iteration: 2065Loss: 2.561888309874873\n",
      "Iteration: 2066Loss: 3.1914177259870766\n",
      "Iteration: 2067Loss: 2.8849992626064056\n",
      "Iteration: 2068Loss: 2.9855399419470587\n",
      "Iteration: 2069Loss: 2.82236885504562\n",
      "Iteration: 2070Loss: 2.773943044330749\n",
      "Iteration: 2071Loss: 2.831815390384302\n",
      "Iteration: 2072Loss: 2.8100003677519285\n",
      "Iteration: 2073Loss: 2.8345693285864684\n",
      "Iteration: 2074Loss: 2.7997230651790597\n",
      "Iteration: 2075Loss: 2.722866082830259\n",
      "Iteration: 2076Loss: 2.862881025121743\n",
      "Iteration: 2077Loss: 2.9453923847745185\n",
      "Iteration: 2078Loss: 3.030494516996867\n",
      "Iteration: 2079Loss: 2.6956931749305135\n",
      "Iteration: 2080Loss: 2.7749817027848174\n",
      "Iteration: 2081Loss: 2.843868950972032\n",
      "Iteration: 2082Loss: 2.8469184044884264\n",
      "Iteration: 2083Loss: 2.9632128817507715\n",
      "Iteration: 2084Loss: 2.8018393290069836\n",
      "Iteration: 2085Loss: 3.0636371351702483\n",
      "Iteration: 2086Loss: 2.9489114938376693\n",
      "Iteration: 2087Loss: 2.863879507515089\n",
      "Iteration: 2088Loss: 3.1695621331625694\n",
      "Iteration: 2089Loss: 3.0473756189804773\n",
      "Iteration: 2090Loss: 2.6092063116625908\n",
      "Iteration: 2091Loss: 3.0695415728379376\n",
      "Iteration: 2092Loss: 3.338337244025847\n",
      "Iteration: 2093Loss: 2.72068419398009\n",
      "Iteration: 2094Loss: 2.8294054047548256\n",
      "Iteration: 2095Loss: 2.8907807301368527\n",
      "Iteration: 2096Loss: 2.8144098070935915\n",
      "Iteration: 2097Loss: 2.7569363262612736\n",
      "Iteration: 2098Loss: 2.6143043882241495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2099Loss: 2.822135364933859\n",
      "Iteration: 2100Loss: 2.6205002930322996\n",
      "Iteration: 2101Loss: 2.7972580359436763\n",
      "Iteration: 2102Loss: 2.5479810883544856\n",
      "Iteration: 2103Loss: 3.0753938217127765\n",
      "Iteration: 2104Loss: 2.52124913637016\n",
      "Iteration: 2105Loss: 2.807722917377529\n",
      "Iteration: 2106Loss: 2.707264500315516\n",
      "Iteration: 2107Loss: 2.9424675256747195\n",
      "Iteration: 2108Loss: 2.6867832626442913\n",
      "Iteration: 2109Loss: 3.15806665757271\n",
      "Iteration: 2110Loss: 2.8204252683743056\n",
      "Iteration: 2111Loss: 2.8572692665944084\n",
      "Iteration: 2112Loss: 2.9742947020152184\n",
      "Iteration: 2113Loss: 2.960928167690666\n",
      "Iteration: 2114Loss: 2.882918522785806\n",
      "Iteration: 2115Loss: 2.686723585429507\n",
      "Iteration: 2116Loss: 2.8418700318119288\n",
      "Iteration: 2117Loss: 3.028706924058497\n",
      "Iteration: 2118Loss: 2.7100209359298733\n",
      "Iteration: 2119Loss: 2.7613626507466655\n",
      "Iteration: 2120Loss: 2.709146188557148\n",
      "Iteration: 2121Loss: 2.8352060026408976\n",
      "Iteration: 2122Loss: 3.1049567493920525\n",
      "Iteration: 2123Loss: 2.9748051400023456\n",
      "Iteration: 2124Loss: 2.761344078792414\n",
      "Iteration: 2125Loss: 2.895093323632573\n",
      "Iteration: 2126Loss: 2.79000193979238\n",
      "Iteration: 2127Loss: 2.745837220300776\n",
      "Iteration: 2128Loss: 2.9583378138445853\n",
      "Iteration: 2129Loss: 2.8753992984677232\n",
      "Iteration: 2130Loss: 2.683922354232128\n",
      "Iteration: 2131Loss: 2.667261855155184\n",
      "Iteration: 2132Loss: 2.5359559437532613\n",
      "Iteration: 2133Loss: 2.8126831319273986\n",
      "Iteration: 2134Loss: 2.7798245064971714\n",
      "Iteration: 2135Loss: 2.7725314080798498\n",
      "Iteration: 2136Loss: 2.951034352117277\n",
      "Iteration: 2137Loss: 2.969609183468136\n",
      "Iteration: 2138Loss: 2.690698119115439\n",
      "Iteration: 2139Loss: 2.720502968839907\n",
      "Iteration: 2140Loss: 2.745603651895555\n",
      "Iteration: 2141Loss: 2.839600952197436\n",
      "Iteration: 2142Loss: 2.7145786896022828\n",
      "Iteration: 2143Loss: 2.6532765974664625\n",
      "Iteration: 2144Loss: 2.895952043807523\n",
      "Iteration: 2145Loss: 3.021614438222719\n",
      "Iteration: 2146Loss: 3.1661342928362166\n",
      "Iteration: 2147Loss: 2.818314738073455\n",
      "Iteration: 2148Loss: 3.0260999853214283\n",
      "Iteration: 2149Loss: 2.699301877650843\n",
      "Iteration: 2150Loss: 2.8685613678433275\n",
      "Iteration: 2151Loss: 2.920093545002503\n",
      "Iteration: 2152Loss: 2.8910737140289577\n",
      "Iteration: 2153Loss: 2.9737098593772813\n",
      "Iteration: 2154Loss: 2.733495906573988\n",
      "Iteration: 2155Loss: 2.8510732126555274\n",
      "Iteration: 2156Loss: 2.9419071042358325\n",
      "Iteration: 2157Loss: 2.879990669962325\n",
      "Iteration: 2158Loss: 2.7926563850858828\n",
      "Iteration: 2159Loss: 2.8397307297145233\n",
      "Iteration: 2160Loss: 2.9834399941264897\n",
      "Iteration: 2161Loss: 2.935809942018458\n",
      "Iteration: 2162Loss: 2.7312819281866982\n",
      "Iteration: 2163Loss: 2.8167430808509355\n",
      "Iteration: 2164Loss: 2.564238443771865\n",
      "Iteration: 2165Loss: 2.8549473099920766\n",
      "Iteration: 2166Loss: 2.677146121331408\n",
      "Iteration: 2167Loss: 2.7503572770661817\n",
      "Iteration: 2168Loss: 2.7546863370183403\n",
      "Iteration: 2169Loss: 2.850091841283816\n",
      "Iteration: 2170Loss: 2.8453482904930096\n",
      "Iteration: 2171Loss: 3.014193237038716\n",
      "Iteration: 2172Loss: 2.9729528524835875\n",
      "Iteration: 2173Loss: 2.7376042387720405\n",
      "Iteration: 2174Loss: 2.7905854785925532\n",
      "Iteration: 2175Loss: 2.8900581365024465\n",
      "Iteration: 2176Loss: 2.7469665968775208\n",
      "Iteration: 2177Loss: 2.900276501803061\n",
      "Iteration: 2178Loss: 2.900445661163952\n",
      "Iteration: 2179Loss: 3.047007424555137\n",
      "Iteration: 2180Loss: 2.7670980529777083\n",
      "Iteration: 2181Loss: 2.7720469902419014\n",
      "Iteration: 2182Loss: 2.675251359913831\n",
      "Iteration: 2183Loss: 2.7807931476074956\n",
      "Iteration: 2184Loss: 2.729875896696806\n",
      "Iteration: 2185Loss: 2.9203119683100494\n",
      "Iteration: 2186Loss: 2.8932156968655898\n",
      "Iteration: 2187Loss: 3.0144580705639195\n",
      "Iteration: 2188Loss: 2.832373452352548\n",
      "Iteration: 2189Loss: 2.7492791225626574\n",
      "Iteration: 2190Loss: 2.788378991025218\n",
      "Iteration: 2191Loss: 2.690908908849318\n",
      "Iteration: 2192Loss: 2.908307454172868\n",
      "Iteration: 2193Loss: 2.6010396016294224\n",
      "Iteration: 2194Loss: 2.689129036186654\n",
      "Iteration: 2195Loss: 2.938970400900206\n",
      "Iteration: 2196Loss: 2.7545352861640455\n",
      "Iteration: 2197Loss: 2.8797012065092282\n",
      "Iteration: 2198Loss: 3.18901888570007\n",
      "Iteration: 2199Loss: 2.745535684050117\n",
      "Iteration: 2200Loss: 2.658153363030384\n",
      "Iteration: 2201Loss: 2.904400663950916\n",
      "Iteration: 2202Loss: 2.885807218598101\n",
      "Iteration: 2203Loss: 2.8116759841151886\n",
      "Iteration: 2204Loss: 2.880100258760207\n",
      "Iteration: 2205Loss: 2.938129903715533\n",
      "Iteration: 2206Loss: 2.6649093905239427\n",
      "Iteration: 2207Loss: 2.7228698550919184\n",
      "Iteration: 2208Loss: 2.969612397764351\n",
      "Iteration: 2209Loss: 2.755485559393713\n",
      "Iteration: 2210Loss: 2.851745496375684\n",
      "Iteration: 2211Loss: 2.9636957787535163\n",
      "Iteration: 2212Loss: 2.6830813617785148\n",
      "Iteration: 2213Loss: 2.655605639947924\n",
      "Iteration: 2214Loss: 2.803937758427023\n",
      "Iteration: 2215Loss: 2.8036895074941848\n",
      "Iteration: 2216Loss: 2.9961985336616634\n",
      "Iteration: 2217Loss: 2.9168951857697745\n",
      "Iteration: 2218Loss: 2.8837262207686196\n",
      "Iteration: 2219Loss: 2.7808160001854296\n",
      "Iteration: 2220Loss: 2.7998436603727996\n",
      "Iteration: 2221Loss: 2.6820716690070023\n",
      "Iteration: 2222Loss: 2.7960771095841395\n",
      "Iteration: 2223Loss: 3.0439664871496754\n",
      "Iteration: 2224Loss: 2.8904849953666516\n",
      "Iteration: 2225Loss: 2.9303671724045106\n",
      "Iteration: 2226Loss: 2.484168968950557\n",
      "Iteration: 2227Loss: 2.9782935772400068\n",
      "Iteration: 2228Loss: 2.6318624410487246\n",
      "Iteration: 2229Loss: 2.7820330965356326\n",
      "Iteration: 2230Loss: 2.7846520273663256\n",
      "Iteration: 2231Loss: 3.1993117601824634\n",
      "Iteration: 2232Loss: 2.850227514700838\n",
      "Iteration: 2233Loss: 2.8179224841319277\n",
      "Iteration: 2234Loss: 2.809201148704472\n",
      "Iteration: 2235Loss: 2.7229434067294713\n",
      "Iteration: 2236Loss: 2.811975589953363\n",
      "Iteration: 2237Loss: 2.783749626870085\n",
      "Iteration: 2238Loss: 2.8419602417241125\n",
      "Iteration: 2239Loss: 2.657348737526623\n",
      "Iteration: 2240Loss: 2.700028822565203\n",
      "Iteration: 2241Loss: 2.7441301533194244\n",
      "Iteration: 2242Loss: 2.8468739041435165\n",
      "Iteration: 2243Loss: 2.774105771923489\n",
      "Iteration: 2244Loss: 2.885771786039695\n",
      "Iteration: 2245Loss: 2.7497608424309488\n",
      "Iteration: 2246Loss: 2.601481844676011\n",
      "Iteration: 2247Loss: 2.8099536941231738\n",
      "Iteration: 2248Loss: 2.8148919266357697\n",
      "Iteration: 2249Loss: 2.7999334590572174\n",
      "Iteration: 2250Loss: 2.742963677311937\n",
      "Iteration: 2251Loss: 2.954245614221405\n",
      "Iteration: 2252Loss: 2.9298643594749967\n",
      "Iteration: 2253Loss: 2.777601907024545\n",
      "Iteration: 2254Loss: 2.850695711054702\n",
      "Iteration: 2255Loss: 2.8489629531555134\n",
      "Iteration: 2256Loss: 2.699687855753121\n",
      "Iteration: 2257Loss: 2.71815107641649\n",
      "Iteration: 2258Loss: 2.70605028541391\n",
      "Iteration: 2259Loss: 2.726115210634175\n",
      "Iteration: 2260Loss: 3.01631715667373\n",
      "Iteration: 2261Loss: 2.7772764496730726\n",
      "Iteration: 2262Loss: 2.579961351839704\n",
      "Iteration: 2263Loss: 3.0524807087910335\n",
      "Iteration: 2264Loss: 2.754055623887153\n",
      "Iteration: 2265Loss: 2.8904318767020554\n",
      "Iteration: 2266Loss: 3.010518580999223\n",
      "Iteration: 2267Loss: 2.547385303311083\n",
      "Iteration: 2268Loss: 2.867303321242828\n",
      "Iteration: 2269Loss: 2.864219615444541\n",
      "Iteration: 2270Loss: 2.9287457167807385\n",
      "Iteration: 2271Loss: 2.541184328686898\n",
      "Iteration: 2272Loss: 2.965615994269457\n",
      "Iteration: 2273Loss: 2.6264264409019953\n",
      "Iteration: 2274Loss: 2.8120652343736263\n",
      "Iteration: 2275Loss: 2.6161945617236464\n",
      "Iteration: 2276Loss: 2.949986460582137\n",
      "Iteration: 2277Loss: 2.849372416097886\n",
      "Iteration: 2278Loss: 2.6014094710271194\n",
      "Iteration: 2279Loss: 2.9297435762863273\n",
      "Iteration: 2280Loss: 2.691721039728047\n",
      "Iteration: 2281Loss: 2.8758409518117647\n",
      "Iteration: 2282Loss: 2.4630008565879833\n",
      "Iteration: 2283Loss: 2.9035102445495626\n",
      "Iteration: 2284Loss: 2.621456332127253\n",
      "Iteration: 2285Loss: 2.9368136802804656\n",
      "Iteration: 2286Loss: 2.6955893600808056\n",
      "Iteration: 2287Loss: 2.6274026236333694\n",
      "Iteration: 2288Loss: 3.0163207591847447\n",
      "Iteration: 2289Loss: 2.649119310877859\n",
      "Iteration: 2290Loss: 2.799942522681917\n",
      "Iteration: 2291Loss: 3.205252792899652\n",
      "Iteration: 2292Loss: 2.885140866550924\n",
      "Iteration: 2293Loss: 2.698240616710756\n",
      "Iteration: 2294Loss: 2.90406141584568\n",
      "Iteration: 2295Loss: 2.758105871928401\n",
      "Iteration: 2296Loss: 3.099152762894657\n",
      "Iteration: 2297Loss: 2.881348547452659\n",
      "Iteration: 2298Loss: 2.9421918380539864\n",
      "Iteration: 2299Loss: 2.860058057200265\n",
      "Iteration: 2300Loss: 2.5466058872629334\n",
      "Iteration: 2301Loss: 2.8146977693167994\n",
      "Iteration: 2302Loss: 2.986857487896174\n",
      "Iteration: 2303Loss: 2.6049598933328033\n",
      "Iteration: 2304Loss: 2.8508475434823106\n",
      "Iteration: 2305Loss: 2.9809792310538064\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2306Loss: 2.6477741764968523\n",
      "Iteration: 2307Loss: 2.6568541314725467\n",
      "Iteration: 2308Loss: 2.666247310716253\n",
      "Iteration: 2309Loss: 2.8405121556680033\n",
      "Iteration: 2310Loss: 2.9424421430524923\n",
      "Iteration: 2311Loss: 2.710372920017271\n",
      "Iteration: 2312Loss: 2.683887214367699\n",
      "Iteration: 2313Loss: 2.9230346909355163\n",
      "Iteration: 2314Loss: 2.906130468993673\n",
      "Iteration: 2315Loss: 2.9024219163288945\n",
      "Iteration: 2316Loss: 2.95168341189408\n",
      "Iteration: 2317Loss: 2.8272280051795025\n",
      "Iteration: 2318Loss: 2.7830889623135033\n",
      "Iteration: 2319Loss: 2.826814128575425\n",
      "Iteration: 2320Loss: 2.6407869293814064\n",
      "Iteration: 2321Loss: 2.6378526451477153\n",
      "Iteration: 2322Loss: 2.8688975184069325\n",
      "Iteration: 2323Loss: 2.7591361043640696\n",
      "Iteration: 2324Loss: 2.7058584326576844\n",
      "Iteration: 2325Loss: 2.940018215690429\n",
      "Iteration: 2326Loss: 2.666124157331069\n",
      "Iteration: 2327Loss: 2.579754271042\n",
      "Iteration: 2328Loss: 2.725933718465815\n",
      "Iteration: 2329Loss: 2.873768517410408\n",
      "Iteration: 2330Loss: 2.691187437277292\n",
      "Iteration: 2331Loss: 2.8116932331345743\n",
      "Iteration: 2332Loss: 2.5938911794452597\n",
      "Iteration: 2333Loss: 2.3991781407993593\n",
      "Iteration: 2334Loss: 2.7474191391643354\n",
      "Iteration: 2335Loss: 2.649818427262705\n",
      "Iteration: 2336Loss: 2.5588157986042566\n",
      "Iteration: 2337Loss: 2.747186267113671\n",
      "Iteration: 2338Loss: 2.8430642694832264\n",
      "Iteration: 2339Loss: 2.8823116288991764\n",
      "Iteration: 2340Loss: 2.8383093874457836\n",
      "Iteration: 2341Loss: 2.6506922413623557\n",
      "Iteration: 2342Loss: 2.7916137071870253\n",
      "Iteration: 2343Loss: 2.7580483221225256\n",
      "Iteration: 2344Loss: 2.90703598076506\n",
      "Iteration: 2345Loss: 2.8866480151791434\n",
      "Iteration: 2346Loss: 2.555175629097687\n",
      "Iteration: 2347Loss: 2.639396083122928\n",
      "Iteration: 2348Loss: 2.860846744165241\n",
      "Iteration: 2349Loss: 2.7395647114571084\n",
      "Iteration: 2350Loss: 2.7708220003856314\n",
      "Iteration: 2351Loss: 2.5575264594758997\n",
      "Iteration: 2352Loss: 2.797559887587734\n",
      "Iteration: 2353Loss: 2.816477695411443\n",
      "Iteration: 2354Loss: 2.628978159309739\n",
      "Iteration: 2355Loss: 2.7718568495091542\n",
      "Iteration: 2356Loss: 2.7576199242189148\n",
      "Iteration: 2357Loss: 2.946627502777901\n",
      "Iteration: 2358Loss: 2.7807216885896016\n",
      "Iteration: 2359Loss: 2.6652032541002066\n",
      "Iteration: 2360Loss: 2.8970656092898244\n",
      "Iteration: 2361Loss: 2.791447527785733\n",
      "Iteration: 2362Loss: 2.927220711906799\n",
      "Iteration: 2363Loss: 2.8013095094200517\n",
      "Iteration: 2364Loss: 2.9375233786559742\n",
      "Iteration: 2365Loss: 2.7515212885443834\n",
      "Iteration: 2366Loss: 2.764205701563376\n",
      "Iteration: 2367Loss: 2.865264146031377\n",
      "Iteration: 2368Loss: 2.8108981327543288\n",
      "Iteration: 2369Loss: 2.9692467895075225\n",
      "Iteration: 2370Loss: 2.8773973934550305\n",
      "Iteration: 2371Loss: 2.904527281263713\n",
      "Iteration: 2372Loss: 2.862137167716906\n",
      "Iteration: 2373Loss: 2.8241127188578177\n",
      "Iteration: 2374Loss: 2.828250376502876\n",
      "Iteration: 2375Loss: 2.8730581404673625\n",
      "Iteration: 2376Loss: 3.137894391644285\n",
      "Iteration: 2377Loss: 2.6703075465153563\n",
      "Iteration: 2378Loss: 2.7315960266344557\n",
      "Iteration: 2379Loss: 3.193621356422385\n",
      "Iteration: 2380Loss: 2.7922937079630277\n",
      "Iteration: 2381Loss: 2.7338801030319884\n",
      "Iteration: 2382Loss: 2.7396193364378703\n",
      "Iteration: 2383Loss: 2.6868787229122177\n",
      "Iteration: 2384Loss: 2.886397501676005\n",
      "Iteration: 2385Loss: 2.74195129352038\n",
      "Iteration: 2386Loss: 2.7643333265639347\n",
      "Iteration: 2387Loss: 3.030478937978803\n",
      "Iteration: 2388Loss: 2.726777891700659\n",
      "Iteration: 2389Loss: 2.633337702669131\n",
      "Iteration: 2390Loss: 2.7672637302614747\n",
      "Iteration: 2391Loss: 2.7188668947663035\n",
      "Iteration: 2392Loss: 2.656210648527605\n",
      "Iteration: 2393Loss: 2.4571190430420424\n",
      "Iteration: 2394Loss: 2.725685698656261\n",
      "Iteration: 2395Loss: 2.779057097498638\n",
      "Iteration: 2396Loss: 2.92529892927254\n",
      "Iteration: 2397Loss: 2.9815501195218372\n",
      "Iteration: 2398Loss: 2.567197482321657\n",
      "Iteration: 2399Loss: 2.8666374623371986\n",
      "Iteration: 2400Loss: 2.469356459620323\n",
      "Iteration: 2401Loss: 2.6808934261284576\n",
      "Iteration: 2402Loss: 3.1312232583854347\n",
      "Iteration: 2403Loss: 2.71613185480237\n",
      "Iteration: 2404Loss: 2.796971980762474\n",
      "Iteration: 2405Loss: 3.0907155245904976\n",
      "Iteration: 2406Loss: 2.6475555073263957\n",
      "Iteration: 2407Loss: 2.970222506856096\n",
      "Iteration: 2408Loss: 2.794653248327917\n",
      "Iteration: 2409Loss: 2.7541401710674625\n",
      "Iteration: 2410Loss: 2.979864417897511\n",
      "Iteration: 2411Loss: 2.9293444384722465\n",
      "Iteration: 2412Loss: 2.9711414311191233\n",
      "Iteration: 2413Loss: 2.8179754523874876\n",
      "Iteration: 2414Loss: 2.724871057067205\n",
      "Iteration: 2415Loss: 2.849036028456627\n",
      "Iteration: 2416Loss: 2.769503437084532\n",
      "Iteration: 2417Loss: 2.626494903604095\n",
      "Iteration: 2418Loss: 2.6935901750892968\n",
      "Iteration: 2419Loss: 2.545506870001912\n",
      "Iteration: 2420Loss: 2.67851879931563\n",
      "Iteration: 2421Loss: 2.720199983609889\n",
      "Iteration: 2422Loss: 2.7313783720856963\n",
      "Iteration: 2423Loss: 2.784974798053494\n",
      "Iteration: 2424Loss: 2.8776742094826724\n",
      "Iteration: 2425Loss: 2.6769777358524154\n",
      "Iteration: 2426Loss: 3.074189527199932\n",
      "Iteration: 2427Loss: 2.6433387809894913\n",
      "Iteration: 2428Loss: 2.8982669012002757\n",
      "Iteration: 2429Loss: 2.81222088769181\n",
      "Iteration: 2430Loss: 2.74981914104608\n",
      "Iteration: 2431Loss: 2.6428054874582636\n",
      "Iteration: 2432Loss: 2.806534587124215\n",
      "Iteration: 2433Loss: 3.017955607767861\n",
      "Iteration: 2434Loss: 2.592643831396665\n",
      "Iteration: 2435Loss: 2.8036356786656893\n",
      "Iteration: 2436Loss: 2.7411988411699495\n",
      "Iteration: 2437Loss: 2.5546984353750113\n",
      "Iteration: 2438Loss: 2.7820127209684746\n",
      "Iteration: 2439Loss: 2.619702470023185\n",
      "Iteration: 2440Loss: 2.7340519725860846\n",
      "Iteration: 2441Loss: 2.709327022015488\n",
      "Iteration: 2442Loss: 2.6229133031699363\n",
      "Iteration: 2443Loss: 2.596658707618061\n",
      "Iteration: 2444Loss: 2.7855353262525555\n",
      "Iteration: 2445Loss: 2.8797001085539415\n",
      "Iteration: 2446Loss: 2.6715502834255553\n",
      "Iteration: 2447Loss: 2.964062102734842\n",
      "Iteration: 2448Loss: 2.624503749064528\n",
      "Iteration: 2449Loss: 2.7162033771477123\n",
      "Iteration: 2450Loss: 2.6058915119085992\n",
      "Iteration: 2451Loss: 2.9679481254076276\n",
      "Iteration: 2452Loss: 2.904403510586417\n",
      "Iteration: 2453Loss: 2.921569101144741\n",
      "Iteration: 2454Loss: 2.7316594146753563\n",
      "Iteration: 2455Loss: 3.0604162897830185\n",
      "Iteration: 2456Loss: 2.5335418697140994\n",
      "Iteration: 2457Loss: 2.910788966200552\n",
      "Iteration: 2458Loss: 2.74455099539191\n",
      "Iteration: 2459Loss: 2.5054581233809015\n",
      "Iteration: 2460Loss: 2.5913295105252128\n",
      "Iteration: 2461Loss: 2.5590931429416037\n",
      "Iteration: 2462Loss: 2.681011467216314\n",
      "Iteration: 2463Loss: 2.8135348586144424\n",
      "Iteration: 2464Loss: 2.7452087038769744\n",
      "Iteration: 2465Loss: 2.6127578976216186\n",
      "Iteration: 2466Loss: 2.655785475215411\n",
      "Iteration: 2467Loss: 2.984522719041568\n",
      "Iteration: 2468Loss: 2.856671291677442\n",
      "Iteration: 2469Loss: 2.708116148923632\n",
      "Iteration: 2470Loss: 2.61066964302686\n",
      "Iteration: 2471Loss: 2.9849920953329847\n",
      "Iteration: 2472Loss: 2.953432490722008\n",
      "Iteration: 2473Loss: 2.6533257323197166\n",
      "Iteration: 2474Loss: 2.900004590913793\n",
      "Iteration: 2475Loss: 2.6099325439369636\n",
      "Iteration: 2476Loss: 2.7602831180353435\n",
      "Iteration: 2477Loss: 2.9406783648576242\n",
      "Iteration: 2478Loss: 2.931043018288583\n",
      "Iteration: 2479Loss: 2.5804794340862522\n",
      "Iteration: 2480Loss: 2.6212468331717926\n",
      "Iteration: 2481Loss: 2.7947157841118333\n",
      "Iteration: 2482Loss: 2.691044897060894\n",
      "Iteration: 2483Loss: 2.8838218384561456\n",
      "Iteration: 2484Loss: 2.872980503104524\n",
      "Iteration: 2485Loss: 2.8064857478660414\n",
      "Iteration: 2486Loss: 2.8914631842770726\n",
      "Iteration: 2487Loss: 2.6347115932279292\n",
      "Iteration: 2488Loss: 2.524419819970833\n",
      "Iteration: 2489Loss: 2.763333935399025\n",
      "Iteration: 2490Loss: 2.7639355862455846\n",
      "Iteration: 2491Loss: 2.891910218761628\n",
      "Iteration: 2492Loss: 2.747594137255411\n",
      "Iteration: 2493Loss: 2.880584248755299\n",
      "Iteration: 2494Loss: 2.5442349468742034\n",
      "Iteration: 2495Loss: 2.775547150997808\n",
      "Iteration: 2496Loss: 2.974615651827592\n",
      "Iteration: 2497Loss: 2.770874937729677\n",
      "Iteration: 2498Loss: 2.639379697797977\n",
      "Iteration: 2499Loss: 2.773672501403819\n",
      "Iteration: 2500Loss: 2.798557963633691\n",
      "Iteration: 2501Loss: 2.575768687562017\n",
      "Iteration: 2502Loss: 2.9604298534804268\n",
      "Iteration: 2503Loss: 2.7823603415821103\n",
      "Iteration: 2504Loss: 2.6739874189815795\n",
      "Iteration: 2505Loss: 2.65486573564411\n",
      "Iteration: 2506Loss: 2.810282305760453\n",
      "Iteration: 2507Loss: 2.7486707802633816\n",
      "Iteration: 2508Loss: 2.9940323501825334\n",
      "Iteration: 2509Loss: 2.77142964851384\n",
      "Iteration: 2510Loss: 2.6708692253256836\n",
      "Iteration: 2511Loss: 2.7865225616280753\n",
      "Iteration: 2512Loss: 2.635719256698672\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2513Loss: 2.6557611555201586\n",
      "Iteration: 2514Loss: 2.7386155813784883\n",
      "Iteration: 2515Loss: 2.541092326854191\n",
      "Iteration: 2516Loss: 2.7489921467890737\n",
      "Iteration: 2517Loss: 2.974369233256116\n",
      "Iteration: 2518Loss: 2.742689217742902\n",
      "Iteration: 2519Loss: 2.9327159335032347\n",
      "Iteration: 2520Loss: 2.914888446839954\n",
      "Iteration: 2521Loss: 2.842767535056665\n",
      "Iteration: 2522Loss: 2.7115365745305406\n",
      "Iteration: 2523Loss: 2.7106500558143285\n",
      "Iteration: 2524Loss: 2.7920141974474046\n",
      "Iteration: 2525Loss: 2.7690142191846636\n",
      "Iteration: 2526Loss: 2.9734231980109307\n",
      "Iteration: 2527Loss: 2.594677335312895\n",
      "Iteration: 2528Loss: 2.44421718944082\n",
      "Iteration: 2529Loss: 2.83640225396284\n",
      "Iteration: 2530Loss: 2.5011956996009803\n",
      "Iteration: 2531Loss: 2.6593845873901336\n",
      "Iteration: 2532Loss: 2.631517014844906\n",
      "Iteration: 2533Loss: 2.479338074126342\n",
      "Iteration: 2534Loss: 2.682917644258035\n",
      "Iteration: 2535Loss: 2.726678204997893\n",
      "Iteration: 2536Loss: 2.61499313070298\n",
      "Iteration: 2537Loss: 2.58520843941674\n",
      "Iteration: 2538Loss: 2.5686286472800104\n",
      "Iteration: 2539Loss: 2.843231676565538\n",
      "Iteration: 2540Loss: 2.7479735848532783\n",
      "Iteration: 2541Loss: 2.8288237595697865\n",
      "Iteration: 2542Loss: 2.696457311803084\n",
      "Iteration: 2543Loss: 2.7196897119394254\n",
      "Iteration: 2544Loss: 2.910221378250701\n",
      "Iteration: 2545Loss: 2.767398192673116\n",
      "Iteration: 2546Loss: 2.7385827731893313\n",
      "Iteration: 2547Loss: 2.671134706889279\n",
      "Iteration: 2548Loss: 2.8010625705458225\n",
      "Iteration: 2549Loss: 2.2105340845045194\n",
      "Iteration: 2550Loss: 2.5676638775309057\n",
      "Iteration: 2551Loss: 2.646058882687026\n",
      "Iteration: 2552Loss: 2.841873976739167\n",
      "Iteration: 2553Loss: 2.908941885815943\n",
      "Iteration: 2554Loss: 2.659995275815369\n",
      "Iteration: 2555Loss: 2.968578263808922\n",
      "Iteration: 2556Loss: 2.932900147633566\n",
      "Iteration: 2557Loss: 2.796121515421958\n",
      "Iteration: 2558Loss: 2.5598664008173277\n",
      "Iteration: 2559Loss: 2.6701069198474587\n",
      "Iteration: 2560Loss: 2.7183245056626757\n",
      "Iteration: 2561Loss: 2.4710217238515044\n",
      "Iteration: 2562Loss: 2.8759709886976528\n",
      "Iteration: 2563Loss: 2.7059252810833656\n",
      "Iteration: 2564Loss: 2.699085404934775\n",
      "Iteration: 2565Loss: 2.5115869346165205\n",
      "Iteration: 2566Loss: 2.813872950921065\n",
      "Iteration: 2567Loss: 2.604879201164153\n",
      "Iteration: 2568Loss: 2.7199542935027767\n",
      "Iteration: 2569Loss: 2.8076817059988977\n",
      "Iteration: 2570Loss: 2.809477692422559\n",
      "Iteration: 2571Loss: 2.7793938403112386\n",
      "Iteration: 2572Loss: 2.605704847663353\n",
      "Iteration: 2573Loss: 2.814069224366603\n",
      "Iteration: 2574Loss: 2.8399643380981403\n",
      "Iteration: 2575Loss: 2.7965038119047745\n",
      "Iteration: 2576Loss: 2.771616179589758\n",
      "Iteration: 2577Loss: 2.5323474879806795\n",
      "Iteration: 2578Loss: 2.8569854218129422\n",
      "Iteration: 2579Loss: 2.6894966157710716\n",
      "Iteration: 2580Loss: 2.7304442094347907\n",
      "Iteration: 2581Loss: 2.7044822350624016\n",
      "Iteration: 2582Loss: 2.6263593018885847\n",
      "Iteration: 2583Loss: 2.772651632509988\n",
      "Iteration: 2584Loss: 2.666191273529352\n",
      "Iteration: 2585Loss: 2.813730222430033\n",
      "Iteration: 2586Loss: 2.7440846998718156\n",
      "Iteration: 2587Loss: 2.6605486130022973\n",
      "Iteration: 2588Loss: 2.6481638426926954\n",
      "Iteration: 2589Loss: 2.663874805735567\n",
      "Iteration: 2590Loss: 2.6292065454159723\n",
      "Iteration: 2591Loss: 2.618290849153281\n",
      "Iteration: 2592Loss: 2.8941640129387403\n",
      "Iteration: 2593Loss: 2.8957434892650937\n",
      "Iteration: 2594Loss: 2.885118995716987\n",
      "Iteration: 2595Loss: 2.7930610960913236\n",
      "Iteration: 2596Loss: 2.7818862316122215\n",
      "Iteration: 2597Loss: 2.525480314233714\n",
      "Iteration: 2598Loss: 2.519921272202293\n",
      "Iteration: 2599Loss: 2.848263268349807\n",
      "Iteration: 2600Loss: 2.8510836175450276\n",
      "Iteration: 2601Loss: 2.4814377017992726\n",
      "Iteration: 2602Loss: 2.6874201683527734\n",
      "Iteration: 2603Loss: 2.579635201834004\n",
      "Iteration: 2604Loss: 2.7707146798097644\n",
      "Iteration: 2605Loss: 2.873775681424526\n",
      "Iteration: 2606Loss: 2.7408066497634826\n",
      "Iteration: 2607Loss: 2.773303945809554\n",
      "Iteration: 2608Loss: 2.8412942866736097\n",
      "Iteration: 2609Loss: 2.5888530104778127\n",
      "Iteration: 2610Loss: 2.5234273450598788\n",
      "Iteration: 2611Loss: 2.6907652342348425\n",
      "Iteration: 2612Loss: 2.802592547262542\n",
      "Iteration: 2613Loss: 2.7970904127536977\n",
      "Iteration: 2614Loss: 2.8948576138931372\n",
      "Iteration: 2615Loss: 2.7587660825676057\n",
      "Iteration: 2616Loss: 2.6447820995995044\n",
      "Iteration: 2617Loss: 2.708157411688865\n",
      "Iteration: 2618Loss: 2.655165331096554\n",
      "Iteration: 2619Loss: 2.5056688344770195\n",
      "Iteration: 2620Loss: 2.854903005841253\n",
      "Iteration: 2621Loss: 2.786938088846715\n",
      "Iteration: 2622Loss: 2.7329291074100754\n",
      "Iteration: 2623Loss: 2.723032647787782\n",
      "Iteration: 2624Loss: 2.793353256490923\n",
      "Iteration: 2625Loss: 2.6439063370631346\n",
      "Iteration: 2626Loss: 2.5037308244034646\n",
      "Iteration: 2627Loss: 2.6312072305092133\n",
      "Iteration: 2628Loss: 2.592437150332248\n",
      "Iteration: 2629Loss: 2.8434180849298634\n",
      "Iteration: 2630Loss: 2.7394761265876393\n",
      "Iteration: 2631Loss: 2.5552957093353488\n",
      "Iteration: 2632Loss: 2.57915139475128\n",
      "Iteration: 2633Loss: 2.4341758659676365\n",
      "Iteration: 2634Loss: 2.741846436742446\n",
      "Iteration: 2635Loss: 2.5333595667264346\n",
      "Iteration: 2636Loss: 2.8003476475668507\n",
      "Iteration: 2637Loss: 2.8405543881871993\n",
      "Iteration: 2638Loss: 2.480770384820835\n",
      "Iteration: 2639Loss: 2.712395598071268\n",
      "Iteration: 2640Loss: 2.6176506308779266\n",
      "Iteration: 2641Loss: 2.625625347329888\n",
      "Iteration: 2642Loss: 2.886321312299229\n",
      "Iteration: 2643Loss: 2.7781532241538347\n",
      "Iteration: 2644Loss: 2.5831926318543106\n",
      "Iteration: 2645Loss: 2.8216372294275947\n",
      "Iteration: 2646Loss: 2.4822934842280424\n",
      "Iteration: 2647Loss: 2.6821840903827314\n",
      "Iteration: 2648Loss: 2.526567317088089\n",
      "Iteration: 2649Loss: 2.8061645762711382\n",
      "Iteration: 2650Loss: 2.8137586709224034\n",
      "Iteration: 2651Loss: 2.772052639008266\n",
      "Iteration: 2652Loss: 3.2265107836915043\n",
      "Iteration: 2653Loss: 2.57053581802712\n",
      "Iteration: 2654Loss: 2.715608737235515\n",
      "Iteration: 2655Loss: 2.819002077974322\n",
      "Iteration: 2656Loss: 2.7275650242870477\n",
      "Iteration: 2657Loss: 2.5842819559657384\n",
      "Iteration: 2658Loss: 2.437153412726404\n",
      "Iteration: 2659Loss: 2.762132622397591\n",
      "Iteration: 2660Loss: 2.723084124882433\n",
      "Iteration: 2661Loss: 2.6872972222548164\n",
      "Iteration: 2662Loss: 2.629955165433111\n",
      "Iteration: 2663Loss: 2.832401056519603\n",
      "Iteration: 2664Loss: 2.646783785052111\n",
      "Iteration: 2665Loss: 2.536494773379766\n",
      "Iteration: 2666Loss: 2.683771295655302\n",
      "Iteration: 2667Loss: 2.8154984906918687\n",
      "Iteration: 2668Loss: 2.784687097434113\n",
      "Iteration: 2669Loss: 2.3507959721217087\n",
      "Iteration: 2670Loss: 2.6860789920398545\n",
      "Iteration: 2671Loss: 2.787265050420393\n",
      "Iteration: 2672Loss: 2.6559910256887393\n",
      "Iteration: 2673Loss: 2.735669105719559\n",
      "Iteration: 2674Loss: 2.589509947103628\n",
      "Iteration: 2675Loss: 2.8399721675161875\n",
      "Iteration: 2676Loss: 2.856365197429984\n",
      "Iteration: 2677Loss: 2.5816209768173786\n",
      "Iteration: 2678Loss: 2.773897837552972\n",
      "Iteration: 2679Loss: 2.601278636784116\n",
      "Iteration: 2680Loss: 2.83078119297903\n",
      "Iteration: 2681Loss: 2.4573371235220693\n",
      "Iteration: 2682Loss: 2.5040514569684373\n",
      "Iteration: 2683Loss: 2.7465377324003373\n",
      "Iteration: 2684Loss: 2.803399954774113\n",
      "Iteration: 2685Loss: 2.8066166157655954\n",
      "Iteration: 2686Loss: 2.6923560844164522\n",
      "Iteration: 2687Loss: 2.616278663210351\n",
      "Iteration: 2688Loss: 2.780947551510016\n",
      "Iteration: 2689Loss: 2.457494384513067\n",
      "Iteration: 2690Loss: 2.543919682520993\n",
      "Iteration: 2691Loss: 2.694148086773267\n",
      "Iteration: 2692Loss: 2.6479100575192844\n",
      "Iteration: 2693Loss: 2.409662582299661\n",
      "Iteration: 2694Loss: 2.6411328560145915\n",
      "Iteration: 2695Loss: 2.613088693206407\n",
      "Iteration: 2696Loss: 2.7496866938655615\n",
      "Iteration: 2697Loss: 2.5997445842465665\n",
      "Iteration: 2698Loss: 2.9959614487115305\n",
      "Iteration: 2699Loss: 2.7490873299630274\n",
      "Iteration: 2700Loss: 2.6851708749341934\n",
      "Iteration: 2701Loss: 2.6774415822286532\n",
      "Iteration: 2702Loss: 2.5655658652881916\n",
      "Iteration: 2703Loss: 2.565623369327398\n",
      "Iteration: 2704Loss: 2.612487347858172\n",
      "Iteration: 2705Loss: 2.6065462768230647\n",
      "Iteration: 2706Loss: 2.746449210154562\n",
      "Iteration: 2707Loss: 2.5861026173730894\n",
      "Iteration: 2708Loss: 2.5497475243071106\n",
      "Iteration: 2709Loss: 2.777332102732053\n",
      "Iteration: 2710Loss: 2.809330131133961\n",
      "Iteration: 2711Loss: 2.7715524775952507\n",
      "Iteration: 2712Loss: 2.638426436575537\n",
      "Iteration: 2713Loss: 2.488116725866493\n",
      "Iteration: 2714Loss: 2.8305748837851783\n",
      "Iteration: 2715Loss: 2.485130870332234\n",
      "Iteration: 2716Loss: 2.6843228653907962\n",
      "Iteration: 2717Loss: 2.706424792693429\n",
      "Iteration: 2718Loss: 2.432477088624113\n",
      "Iteration: 2719Loss: 2.7705261699564434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2720Loss: 2.5754341756716355\n",
      "Iteration: 2721Loss: 2.7949402022962246\n",
      "Iteration: 2722Loss: 2.7867114788411222\n",
      "Iteration: 2723Loss: 2.6155251117493252\n",
      "Iteration: 2724Loss: 2.779347843277369\n",
      "Iteration: 2725Loss: 2.6176458970133196\n",
      "Iteration: 2726Loss: 2.6763789982066357\n",
      "Iteration: 2727Loss: 2.59677032958489\n",
      "Iteration: 2728Loss: 2.606375315716362\n",
      "Iteration: 2729Loss: 2.700166128261544\n",
      "Iteration: 2730Loss: 2.5912491364153256\n",
      "Iteration: 2731Loss: 2.616707970157508\n",
      "Iteration: 2732Loss: 2.719103497019885\n",
      "Iteration: 2733Loss: 2.754674293762742\n",
      "Iteration: 2734Loss: 2.767978605782208\n",
      "Iteration: 2735Loss: 2.470469811885228\n",
      "Iteration: 2736Loss: 2.4276879601651196\n",
      "Iteration: 2737Loss: 2.859805247369336\n",
      "Iteration: 2738Loss: 2.728961481511839\n",
      "Iteration: 2739Loss: 2.816807920113206\n",
      "Iteration: 2740Loss: 2.5970448319160555\n",
      "Iteration: 2741Loss: 2.6424523484464246\n",
      "Iteration: 2742Loss: 2.613290106208608\n",
      "Iteration: 2743Loss: 2.5652555641950485\n",
      "Iteration: 2744Loss: 2.746698706450456\n",
      "Iteration: 2745Loss: 2.7560894572834242\n",
      "Iteration: 2746Loss: 2.920302241388223\n",
      "Iteration: 2747Loss: 2.7511349447890856\n",
      "Iteration: 2748Loss: 2.471371189812682\n",
      "Iteration: 2749Loss: 2.5245411535797757\n",
      "Iteration: 2750Loss: 2.6178826614044404\n",
      "Iteration: 2751Loss: 2.518383369454774\n",
      "Iteration: 2752Loss: 2.733349012864281\n",
      "Iteration: 2753Loss: 2.734915642587044\n",
      "Iteration: 2754Loss: 2.465291161566491\n",
      "Iteration: 2755Loss: 2.9415742356144583\n",
      "Iteration: 2756Loss: 2.647200647867102\n",
      "Iteration: 2757Loss: 2.8044652205535043\n",
      "Iteration: 2758Loss: 2.48973339190883\n",
      "Iteration: 2759Loss: 2.7042147361892717\n",
      "Iteration: 2760Loss: 2.8016262948455823\n",
      "Iteration: 2761Loss: 2.5600944680680486\n",
      "Iteration: 2762Loss: 2.7022296411943016\n",
      "Iteration: 2763Loss: 2.6328497840091587\n",
      "Iteration: 2764Loss: 2.518697755117698\n",
      "Iteration: 2765Loss: 2.673576169951233\n",
      "Iteration: 2766Loss: 2.9520194245324665\n",
      "Iteration: 2767Loss: 2.6375385586085636\n",
      "Iteration: 2768Loss: 2.6925443374294336\n",
      "Iteration: 2769Loss: 2.7316584724237947\n",
      "Iteration: 2770Loss: 2.617977315209255\n",
      "Iteration: 2771Loss: 2.741290469355374\n",
      "Iteration: 2772Loss: 2.5645357777180178\n",
      "Iteration: 2773Loss: 2.721172479732181\n",
      "Iteration: 2774Loss: 2.770318145188474\n",
      "Iteration: 2775Loss: 2.706629753296866\n",
      "Iteration: 2776Loss: 2.769743608530492\n",
      "Iteration: 2777Loss: 2.7351848925815188\n",
      "Iteration: 2778Loss: 2.6389619011906458\n",
      "Iteration: 2779Loss: 2.626745362525148\n",
      "Iteration: 2780Loss: 2.63088430687285\n",
      "Iteration: 2781Loss: 2.7340472275281287\n",
      "Iteration: 2782Loss: 2.9984222294373732\n",
      "Iteration: 2783Loss: 2.5936129337535654\n",
      "Iteration: 2784Loss: 2.4480220524354888\n",
      "Iteration: 2785Loss: 2.779382509494057\n",
      "Iteration: 2786Loss: 2.5768057457529583\n",
      "Iteration: 2787Loss: 2.8486937103318235\n",
      "Iteration: 2788Loss: 2.8615531957144067\n",
      "Iteration: 2789Loss: 2.7901175270060627\n",
      "Iteration: 2790Loss: 2.686979834665559\n",
      "Iteration: 2791Loss: 3.0696171751736485\n",
      "Iteration: 2792Loss: 2.8073955800546035\n",
      "Iteration: 2793Loss: 2.473102516535184\n",
      "Iteration: 2794Loss: 2.5627112946828436\n",
      "Iteration: 2795Loss: 2.837641544558275\n",
      "Iteration: 2796Loss: 2.6376384918346876\n",
      "Iteration: 2797Loss: 2.593685772470521\n",
      "Iteration: 2798Loss: 2.728711741619588\n",
      "Iteration: 2799Loss: 2.8570661896735694\n",
      "Iteration: 2800Loss: 2.560542888899656\n",
      "Iteration: 2801Loss: 2.426387272506021\n",
      "Iteration: 2802Loss: 2.6514769909647047\n",
      "Iteration: 2803Loss: 2.5417484433545\n",
      "Iteration: 2804Loss: 2.87554442466892\n",
      "Iteration: 2805Loss: 2.754336626489381\n",
      "Iteration: 2806Loss: 2.3666598061076027\n",
      "Iteration: 2807Loss: 2.7754193884398832\n",
      "Iteration: 2808Loss: 2.622191761712189\n",
      "Iteration: 2809Loss: 2.4475953621864694\n",
      "Iteration: 2810Loss: 2.6195854260158074\n",
      "Iteration: 2811Loss: 2.5502914303036284\n",
      "Iteration: 2812Loss: 2.623532643106027\n",
      "Iteration: 2813Loss: 2.7054687679327567\n",
      "Iteration: 2814Loss: 2.75002933598685\n",
      "Iteration: 2815Loss: 2.473517863642382\n",
      "Iteration: 2816Loss: 2.861949600471145\n",
      "Iteration: 2817Loss: 2.566765866096692\n",
      "Iteration: 2818Loss: 2.5333213254114035\n",
      "Iteration: 2819Loss: 2.5903413982389147\n",
      "Iteration: 2820Loss: 2.698916317148593\n",
      "Iteration: 2821Loss: 2.6636286393884587\n",
      "Iteration: 2822Loss: 2.7695369777439254\n",
      "Iteration: 2823Loss: 2.655938881388291\n",
      "Iteration: 2824Loss: 2.6424579893674283\n",
      "Iteration: 2825Loss: 2.542861174925756\n",
      "Iteration: 2826Loss: 2.447681867380092\n",
      "Iteration: 2827Loss: 2.632222192674412\n",
      "Iteration: 2828Loss: 2.787084246022124\n",
      "Iteration: 2829Loss: 2.613428015674486\n",
      "Iteration: 2830Loss: 2.627449925909292\n",
      "Iteration: 2831Loss: 2.6338854129520572\n",
      "Iteration: 2832Loss: 2.561707244232207\n",
      "Iteration: 2833Loss: 2.585219971305175\n",
      "Iteration: 2834Loss: 2.6788215174876435\n",
      "Iteration: 2835Loss: 2.7812115042044976\n",
      "Iteration: 2836Loss: 2.5330565594364227\n",
      "Iteration: 2837Loss: 2.578123088030247\n",
      "Iteration: 2838Loss: 2.7102391030366135\n",
      "Iteration: 2839Loss: 2.5731749336939522\n",
      "Iteration: 2840Loss: 2.8252945828954785\n",
      "Iteration: 2841Loss: 2.438813544797749\n",
      "Iteration: 2842Loss: 2.443947826915731\n",
      "Iteration: 2843Loss: 2.7387997859934297\n",
      "Iteration: 2844Loss: 2.7404344064988457\n",
      "Iteration: 2845Loss: 2.4211767251110827\n",
      "Iteration: 2846Loss: 2.481230804818509\n",
      "Iteration: 2847Loss: 2.706974018034008\n",
      "Iteration: 2848Loss: 2.6454299498817533\n",
      "Iteration: 2849Loss: 2.759283751944766\n",
      "Iteration: 2850Loss: 2.413824454430611\n",
      "Iteration: 2851Loss: 2.8050975389266273\n",
      "Iteration: 2852Loss: 2.8048992865101443\n",
      "Iteration: 2853Loss: 2.8080927952017523\n",
      "Iteration: 2854Loss: 2.5903547259431594\n",
      "Iteration: 2855Loss: 2.682607677322672\n",
      "Iteration: 2856Loss: 2.7865878191066558\n",
      "Iteration: 2857Loss: 2.5993559071337713\n",
      "Iteration: 2858Loss: 2.8146511132340457\n",
      "Iteration: 2859Loss: 2.662028588738008\n",
      "Iteration: 2860Loss: 2.3354375031403856\n",
      "Iteration: 2861Loss: 2.572280298927822\n",
      "Iteration: 2862Loss: 2.7171924662552303\n",
      "Iteration: 2863Loss: 2.80267133115395\n",
      "Iteration: 2864Loss: 2.599687698986956\n",
      "Iteration: 2865Loss: 2.8526378518932662\n",
      "Iteration: 2866Loss: 2.556658616187126\n",
      "Iteration: 2867Loss: 2.9111117627653114\n",
      "Iteration: 2868Loss: 2.63085325115748\n",
      "Iteration: 2869Loss: 2.5848503485893093\n",
      "Iteration: 2870Loss: 2.586111465923334\n",
      "Iteration: 2871Loss: 2.6613062093725417\n",
      "Iteration: 2872Loss: 2.8554710598179587\n",
      "Iteration: 2873Loss: 2.8717216011012923\n",
      "Iteration: 2874Loss: 2.643917352334358\n",
      "Iteration: 2875Loss: 2.472389168072673\n",
      "Iteration: 2876Loss: 2.742339647941725\n",
      "Iteration: 2877Loss: 2.829518607169114\n",
      "Iteration: 2878Loss: 2.6526950436466805\n",
      "Iteration: 2879Loss: 2.59294467591091\n",
      "Iteration: 2880Loss: 2.618657104057605\n",
      "Iteration: 2881Loss: 2.526936272173955\n",
      "Iteration: 2882Loss: 2.717468942588725\n",
      "Iteration: 2883Loss: 2.4334312454546114\n",
      "Iteration: 2884Loss: 2.674858442332783\n",
      "Iteration: 2885Loss: 2.8492935463880436\n",
      "Iteration: 2886Loss: 2.6626537114413122\n",
      "Iteration: 2887Loss: 2.4333071395503794\n",
      "Iteration: 2888Loss: 2.478405279654191\n",
      "Iteration: 2889Loss: 2.6749694297582125\n",
      "Iteration: 2890Loss: 2.685056406328764\n",
      "Iteration: 2891Loss: 2.366749582253897\n",
      "Iteration: 2892Loss: 2.7252767785570677\n",
      "Iteration: 2893Loss: 2.5608526363922284\n",
      "Iteration: 2894Loss: 2.562997277597992\n",
      "Iteration: 2895Loss: 2.652358072896879\n",
      "Iteration: 2896Loss: 2.640847533028549\n",
      "Iteration: 2897Loss: 2.489686199802886\n",
      "Iteration: 2898Loss: 2.5116839224975163\n",
      "Iteration: 2899Loss: 2.6233027113657728\n",
      "Iteration: 2900Loss: 2.724601373736831\n",
      "Iteration: 2901Loss: 2.611788511409291\n",
      "Iteration: 2902Loss: 2.7196407128783178\n",
      "Iteration: 2903Loss: 2.788968416298236\n",
      "Iteration: 2904Loss: 2.7112014294857634\n",
      "Iteration: 2905Loss: 2.5679706437792054\n",
      "Iteration: 2906Loss: 2.6647328213590904\n",
      "Iteration: 2907Loss: 2.6754397328536337\n",
      "Iteration: 2908Loss: 2.75290719027549\n",
      "Iteration: 2909Loss: 2.609173331077424\n",
      "Iteration: 2910Loss: 2.5519182512252336\n",
      "Iteration: 2911Loss: 2.755285430118762\n",
      "Iteration: 2912Loss: 2.8304524321825637\n",
      "Iteration: 2913Loss: 2.661634594322388\n",
      "Iteration: 2914Loss: 2.4827265173235795\n",
      "Iteration: 2915Loss: 2.788173652226382\n",
      "Iteration: 2916Loss: 2.512829739583582\n",
      "Iteration: 2917Loss: 2.7850062538937825\n",
      "Iteration: 2918Loss: 2.52880954723207\n",
      "Iteration: 2919Loss: 2.432194669740144\n",
      "Iteration: 2920Loss: 2.796595445940405\n",
      "Iteration: 2921Loss: 2.6283213523717763\n",
      "Iteration: 2922Loss: 2.787004469583808\n",
      "Iteration: 2923Loss: 2.5544440546467166\n",
      "Iteration: 2924Loss: 2.5391752870949476\n",
      "Iteration: 2925Loss: 2.7258855389938863\n",
      "Iteration: 2926Loss: 2.4213026077511475\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2927Loss: 2.568241762622408\n",
      "Iteration: 2928Loss: 2.8732156885697013\n",
      "Iteration: 2929Loss: 2.3638615650888632\n",
      "Iteration: 2930Loss: 2.521823605220878\n",
      "Iteration: 2931Loss: 2.671226650049574\n",
      "Iteration: 2932Loss: 2.372893565944687\n",
      "Iteration: 2933Loss: 2.6961834141856116\n",
      "Iteration: 2934Loss: 2.4010119943162853\n",
      "Iteration: 2935Loss: 2.6237381650463347\n",
      "Iteration: 2936Loss: 2.590825695735186\n",
      "Iteration: 2937Loss: 2.7252689349267247\n",
      "Iteration: 2938Loss: 2.7311274036226503\n",
      "Iteration: 2939Loss: 2.63534022926683\n",
      "Iteration: 2940Loss: 2.775535631037759\n",
      "Iteration: 2941Loss: 2.456308379045968\n",
      "Iteration: 2942Loss: 2.60896042199264\n",
      "Iteration: 2943Loss: 2.5162022284798673\n",
      "Iteration: 2944Loss: 2.598292830586964\n",
      "Iteration: 2945Loss: 2.7759377310061804\n",
      "Iteration: 2946Loss: 2.540946937114019\n",
      "Iteration: 2947Loss: 2.627126383786526\n",
      "Iteration: 2948Loss: 2.445643836823207\n",
      "Iteration: 2949Loss: 2.7678693392456455\n",
      "Iteration: 2950Loss: 2.5214571433484245\n",
      "Iteration: 2951Loss: 2.593756360283899\n",
      "Iteration: 2952Loss: 2.7713570296330863\n",
      "Iteration: 2953Loss: 2.8504123483063775\n",
      "Iteration: 2954Loss: 2.631216166378355\n",
      "Iteration: 2955Loss: 2.7300833648988263\n",
      "Iteration: 2956Loss: 2.64943792382844\n",
      "Iteration: 2957Loss: 2.683357975224236\n",
      "Iteration: 2958Loss: 2.9760201277198046\n",
      "Iteration: 2959Loss: 2.460475898240922\n",
      "Iteration: 2960Loss: 2.602901770421327\n",
      "Iteration: 2961Loss: 2.5309751872542545\n",
      "Iteration: 2962Loss: 2.445021782308729\n",
      "Iteration: 2963Loss: 2.623961786394711\n",
      "Iteration: 2964Loss: 2.5859037038944934\n",
      "Iteration: 2965Loss: 2.642411296330682\n",
      "Iteration: 2966Loss: 2.8651727667891893\n",
      "Iteration: 2967Loss: 2.3849933847999565\n",
      "Iteration: 2968Loss: 3.005133061267927\n",
      "Iteration: 2969Loss: 2.787169144674173\n",
      "Iteration: 2970Loss: 2.555247995531802\n",
      "Iteration: 2971Loss: 2.8444968067761747\n",
      "Iteration: 2972Loss: 2.8061161079055186\n",
      "Iteration: 2973Loss: 2.660981816478134\n",
      "Iteration: 2974Loss: 2.4655542530359305\n",
      "Iteration: 2975Loss: 2.6143356862923373\n",
      "Iteration: 2976Loss: 2.521956085051283\n",
      "Iteration: 2977Loss: 2.5727352575591835\n",
      "Iteration: 2978Loss: 2.4682922136161407\n",
      "Iteration: 2979Loss: 2.520216689959725\n",
      "Iteration: 2980Loss: 2.823409818176298\n",
      "Iteration: 2981Loss: 2.6866122377184753\n",
      "Iteration: 2982Loss: 2.557934812566321\n",
      "Iteration: 2983Loss: 2.551701012669284\n",
      "Iteration: 2984Loss: 2.620082192739072\n",
      "Iteration: 2985Loss: 2.596612224556691\n",
      "Iteration: 2986Loss: 2.7267659712508032\n",
      "Iteration: 2987Loss: 2.6143732886013327\n",
      "Iteration: 2988Loss: 2.3475699943950077\n",
      "Iteration: 2989Loss: 2.7407814348277193\n",
      "Iteration: 2990Loss: 2.6788631782401353\n",
      "Iteration: 2991Loss: 2.769780689535913\n",
      "Iteration: 2992Loss: 2.458867857790446\n",
      "Iteration: 2993Loss: 2.578520826021207\n",
      "Iteration: 2994Loss: 2.5099524583381756\n",
      "Iteration: 2995Loss: 2.6054788060917904\n",
      "Iteration: 2996Loss: 2.408534541763837\n",
      "Iteration: 2997Loss: 2.5992930921616093\n",
      "Iteration: 2998Loss: 2.6535382013894\n",
      "Iteration: 2999Loss: 2.6905229375212607\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "clip=50.0\n",
    "teacher_forcing_ratio=1.0\n",
    "learning_rate=0.0001\n",
    "decoder_learning_ratio=5.0\n",
    "n_iteration=3000\n",
    "print_every=1\n",
    "save_every=200\n",
    "\n",
    "save_dir=\"not defined\"\n",
    "\n",
    "encoder.train()\n",
    "decoder.train()\n",
    "\n",
    "# encoder.cuda()\n",
    "# decoder.cuda()\n",
    "\n",
    "encoder_optimizer=torch.optim.Adam(encoder.parameters(),lr=learning_rate)\n",
    "decoder_optimizer=torch.optim.Adam(decoder.parameters(),lr=learning_rate*decoder_learning_ratio)\n",
    "\n",
    "plotter=VisdomLinePlotter('main')\n",
    "\n",
    "\n",
    "trainIters(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer,\n",
    "           embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size,\n",
    "           print_every, save_every, clip, corpus_name, loadFilename)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LuongAttnDecoderRNN(\n",
       "  (embedding): Embedding(7816, 500)\n",
       "  (embedding_dropout): Dropout(p=0.1)\n",
       "  (gru): GRU(500, 500, num_layers=2, dropout=0.1)\n",
       "  (concat): Linear(in_features=1000, out_features=500, bias=True)\n",
       "  (out): Linear(in_features=500, out_features=7816, bias=True)\n",
       "  (attn): Attn()\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.eval()\n",
    "decoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human> hello\n",
      "The Normalized Input Sentence: hello\n",
      "Indexed sentence: [[787, 2]]\n",
      "The Lengths tensor: tensor([2])\n",
      "Decoder hidden state: tensor([[[ 0.6900, -0.0672,  0.2335, -0.2118,  0.0102,  0.0275,  0.3303,\n",
      "           0.2687,  0.3290, -0.1629,  0.0191, -0.2451, -0.4923, -0.3990,\n",
      "          -0.0548,  0.5105, -0.0763,  0.4938,  0.3577, -0.2208,  0.2920,\n",
      "           0.1379, -0.1821,  0.2260, -0.0721, -0.2554, -0.2657,  0.0326,\n",
      "          -0.4042,  0.0902, -0.2439, -0.3453, -0.3522,  0.3463,  0.4994,\n",
      "          -0.0085,  0.1932,  0.2056,  0.5034, -0.1016,  0.1220, -0.0847,\n",
      "          -0.5088, -0.1127, -0.1530,  0.2237, -0.3991,  0.2472,  0.4362,\n",
      "          -0.4280, -0.3719,  0.3951,  0.1783, -0.4633, -0.2265,  0.1252,\n",
      "          -0.3185, -0.1368, -0.1236, -0.1177, -0.0124,  0.0987, -0.4224,\n",
      "          -0.2099,  0.0366, -0.2663, -0.3907,  0.1264, -0.0055,  0.2324,\n",
      "           0.1507,  0.4044,  0.0443,  0.1641,  0.0776, -0.3851,  0.0276,\n",
      "           0.0700,  0.1899, -0.0119,  0.3572, -0.2555, -0.2948,  0.0972,\n",
      "           0.2653, -0.3509,  0.2366,  0.1604,  0.1283, -0.5081, -0.0780,\n",
      "           0.4997,  0.1690,  0.1421,  0.3618, -0.2258, -0.1341,  0.1155,\n",
      "           0.2885,  0.0936, -0.1059,  0.3680,  0.1844,  0.1228,  0.2261,\n",
      "          -0.0242,  0.5435,  0.1898, -0.0394, -0.2348,  0.0668, -0.1590,\n",
      "          -0.4056, -0.2855,  0.3460,  0.5492, -0.2054,  0.4544,  0.0468,\n",
      "           0.0876,  0.3188, -0.1835,  0.4149, -0.4180, -0.3557,  0.0322,\n",
      "          -0.3537,  0.2161, -0.1971,  0.2181,  0.1501,  0.5218,  0.0250,\n",
      "           0.3209,  0.2223, -0.3146,  0.1830,  0.1665,  0.0354,  0.2254,\n",
      "          -0.4255, -0.2716, -0.0474,  0.1392, -0.3155, -0.1978,  0.5067,\n",
      "          -0.4699, -0.5281,  0.0173, -0.7960,  0.4016,  0.1468, -0.3677,\n",
      "          -0.5304, -0.4618,  0.3990, -0.2511,  0.3113,  0.1998,  0.0694,\n",
      "          -0.2816, -0.0625, -0.5652,  0.3727,  0.0200,  0.1193,  0.4169,\n",
      "          -0.1272,  0.0893, -0.3144, -0.3573,  0.6300,  0.0264,  0.4544,\n",
      "          -0.0840, -0.1535, -0.7031,  0.3039,  0.0785,  0.5170, -0.5081,\n",
      "          -0.6673, -0.0939, -0.2486,  0.2752, -0.2221, -0.0432,  0.5997,\n",
      "          -0.1140,  0.6324,  0.0402, -0.1586, -0.0850,  0.1085, -0.0638,\n",
      "          -0.5961,  0.1937, -0.0785,  0.1158, -0.3445,  0.1753,  0.2219,\n",
      "           0.0042,  0.2688,  0.0732,  0.0108, -0.2045, -0.2965,  0.4336,\n",
      "          -0.2075, -0.5674,  0.3181,  0.5420, -0.0486,  0.1514,  0.0867,\n",
      "          -0.2454, -0.0652,  0.4800, -0.0312, -0.3240,  0.2823, -0.4035,\n",
      "           0.0994,  0.2920,  0.2961, -0.2374,  0.3129,  0.5197,  0.3020,\n",
      "          -0.1733, -0.6314,  0.0027,  0.3889,  0.2218,  0.4013, -0.4115,\n",
      "          -0.1878,  0.2216, -0.0437,  0.1669,  0.4232,  0.0920,  0.1446,\n",
      "          -0.1519, -0.3504,  0.4267,  0.3371,  0.2060,  0.1012, -0.3731,\n",
      "          -0.1010,  0.4905, -0.0168,  0.3632, -0.0822, -0.0046,  0.4570,\n",
      "          -0.2733,  0.6418, -0.0328, -0.0650, -0.2588, -0.5751, -0.0287,\n",
      "           0.5336, -0.0056, -0.1254, -0.2027, -0.2326, -0.0709,  0.0617,\n",
      "           0.1872, -0.0957, -0.2701,  0.6273, -0.0983,  0.0103,  0.2892,\n",
      "           0.3395, -0.2164, -0.2723,  0.1753,  0.1406,  0.0069,  0.1374,\n",
      "          -0.5153,  0.2509,  0.3839,  0.3097,  0.1279, -0.1606, -0.2809,\n",
      "          -0.2377,  0.0497,  0.1916,  0.3055,  0.5092,  0.1164, -0.0918,\n",
      "           0.0726, -0.2657, -0.1900, -0.1224,  0.4462,  0.2718,  0.7577,\n",
      "          -0.6214, -0.2152, -0.3459,  0.5260,  0.1404, -0.0698, -0.0352,\n",
      "          -0.2876,  0.2289, -0.0438, -0.2339,  0.4226,  0.2788, -0.0715,\n",
      "           0.3821, -0.1793, -0.1658,  0.4351, -0.1751,  0.1496, -0.3539,\n",
      "          -0.3402, -0.4936,  0.2781, -0.3660, -0.1087, -0.3148,  0.0813,\n",
      "          -0.0665,  0.1943, -0.1470,  0.5530, -0.1093,  0.1386, -0.1991,\n",
      "           0.2804, -0.1710,  0.5644,  0.5047,  0.0635,  0.0070, -0.0203,\n",
      "          -0.2494,  0.2741, -0.0579, -0.0598,  0.1909, -0.1715,  0.2594,\n",
      "           0.0454,  0.2605,  0.0600,  0.2146, -0.3074, -0.0330, -0.5675,\n",
      "           0.2301,  0.1341,  0.0740,  0.1692, -0.4292,  0.2364,  0.1481,\n",
      "           0.4319, -0.1113, -0.0111,  0.2905, -0.4096,  0.6917,  0.0180,\n",
      "           0.2008, -0.4043,  0.0343,  0.0461, -0.4208, -0.0129, -0.1689,\n",
      "           0.5131, -0.4305,  0.2873, -0.0514, -0.2417, -0.1860,  0.1135,\n",
      "          -0.0908, -0.0259, -0.0255, -0.4949, -0.0261,  0.0263,  0.1725,\n",
      "          -0.1497,  0.1046,  0.0809, -0.2561, -0.2354, -0.0234, -0.4057,\n",
      "           0.1268, -0.3235, -0.2538,  0.2599, -0.2735, -0.3173, -0.0961,\n",
      "           0.0131,  0.6277, -0.0847, -0.1400, -0.5715, -0.1177,  0.1150,\n",
      "           0.2469,  0.5386, -0.2865, -0.1826, -0.1888, -0.5817, -0.1769,\n",
      "           0.1714,  0.3570, -0.0666,  0.0038,  0.0707, -0.2031, -0.2018,\n",
      "          -0.4315,  0.2971,  0.2290, -0.3097,  0.0641, -0.1236,  0.2023,\n",
      "          -0.2846,  0.1762, -0.3021, -0.1861, -0.1402, -0.1282,  0.3794,\n",
      "          -0.1588, -0.4081,  0.0801, -0.1569, -0.3639,  0.2973,  0.0531,\n",
      "          -0.1513,  0.0407, -0.2714,  0.6017,  0.5438, -0.2670, -0.2704,\n",
      "           0.0259, -0.2740,  0.2057,  0.1355,  0.6961,  0.1572,  0.0657,\n",
      "           0.3023,  0.1965,  0.0978,  0.0014,  0.1311, -0.5081,  0.1776,\n",
      "          -0.2982,  0.0712, -0.0971,  0.1907, -0.0584, -0.5620,  0.3100,\n",
      "          -0.3408, -0.3190, -0.5434, -0.2074,  0.2439,  0.4892, -0.4076,\n",
      "          -0.2459, -0.0262,  0.5006, -0.2098,  0.5172,  0.2957, -0.2927,\n",
      "          -0.3363,  0.0212,  0.4705]],\n",
      "\n",
      "        [[-0.0165, -0.2173, -0.5812,  0.4778,  0.1237,  0.1390,  0.2805,\n",
      "           0.0924, -0.0397,  0.1111,  0.1545, -0.1152, -0.0888,  0.2895,\n",
      "          -0.5803,  0.0164, -0.0437,  0.3718,  0.3002, -0.2038,  0.3064,\n",
      "           0.1150,  0.1385, -0.0247, -0.5326, -0.3495, -0.0790, -0.7421,\n",
      "           0.2859,  0.2252,  0.4382, -0.4894,  0.4936,  0.3396,  0.4671,\n",
      "           0.0581, -0.0857, -0.1451, -0.3840,  0.5132,  0.2439, -0.2576,\n",
      "          -0.4688,  0.3520,  0.2653, -0.0498,  0.1140,  0.1458,  0.0355,\n",
      "           0.1981,  0.1056,  0.0201,  0.1080,  0.3814, -0.1243, -0.2403,\n",
      "          -0.3984, -0.1705, -0.1560,  0.0968, -0.4727, -0.6558, -0.1935,\n",
      "           0.3985,  0.1855,  0.4223, -0.3978, -0.0922,  0.4637, -0.3849,\n",
      "          -0.0034,  0.0409, -0.3632, -0.1357, -0.2531, -0.6019, -0.2114,\n",
      "          -0.6860, -0.0776,  0.3638,  0.2164, -0.1610,  0.5460, -0.1467,\n",
      "          -0.4529, -0.4029,  0.0636, -0.4716,  0.1038,  0.0595, -0.4716,\n",
      "           0.4383,  0.2279, -0.4791, -0.2086,  0.2265, -0.5056, -0.3204,\n",
      "           0.2462, -0.3245, -0.2406, -0.4918, -0.1695, -0.3521, -0.0629,\n",
      "           0.0436, -0.1808, -0.3723,  0.0834, -0.0615, -0.4599, -0.1901,\n",
      "          -0.0583,  0.3096,  0.0174,  0.3402, -0.1350,  0.1089, -0.4780,\n",
      "          -0.3096,  0.2331,  0.6031,  0.0085,  0.3473,  0.2233,  0.5060,\n",
      "           0.4478, -0.0913, -0.1210,  0.3396,  0.3932,  0.4586,  0.2313,\n",
      "          -0.0577, -0.1754, -0.4566, -0.2167, -0.5019,  0.0506, -0.1407,\n",
      "           0.3562, -0.0467,  0.1228, -0.6382,  0.3212, -0.0687,  0.0851,\n",
      "           0.1331, -0.0620, -0.3122, -0.1356,  0.4532, -0.1682,  0.0494,\n",
      "           0.7316,  0.3019, -0.4117,  0.1048, -0.1630, -0.4031,  0.1470,\n",
      "          -0.4629, -0.3446,  0.4388,  0.4100, -0.2014, -0.2500,  0.6270,\n",
      "          -0.1455, -0.0924,  0.0292,  0.1720, -0.3459,  0.2016, -0.1552,\n",
      "          -0.1166, -0.1647,  0.2322, -0.2809, -0.1660, -0.1199,  0.1892,\n",
      "           0.1090, -0.5731,  0.0975, -0.3713, -0.5335, -0.1632,  0.5918,\n",
      "           0.1465,  0.2758,  0.4561,  0.3220,  0.6535,  0.2649,  0.3232,\n",
      "          -0.0938,  0.0637,  0.1076, -0.3527, -0.1081,  0.1891, -0.1503,\n",
      "          -0.7456, -0.2401,  0.6114,  0.4534, -0.4366,  0.2170,  0.0153,\n",
      "          -0.7549, -0.3086,  0.0138,  0.1798, -0.1673,  0.1201, -0.5036,\n",
      "          -0.0784,  0.2370,  0.5922, -0.2099, -0.3282, -0.4780, -0.1254,\n",
      "          -0.4947,  0.6262,  0.1748, -0.2243,  0.0431, -0.1503, -0.1515,\n",
      "          -0.0790,  0.3562,  0.1848, -0.3972,  0.2839, -0.3214,  0.5262,\n",
      "           0.6289,  0.0764,  0.1702,  0.2881, -0.3027,  0.3821,  0.3968,\n",
      "           0.4386,  0.0298, -0.0602, -0.1609, -0.4255, -0.1368,  0.3773,\n",
      "           0.1503,  0.1382,  0.1174, -0.1768, -0.1808,  0.4883,  0.5016,\n",
      "          -0.0691, -0.2132, -0.2682,  0.0345,  0.6611, -0.3183,  0.2274,\n",
      "           0.2291, -0.2836,  0.5159, -0.2509,  0.2525,  0.0054, -0.1482,\n",
      "           0.3896,  0.5956,  0.2596, -0.5052,  0.6590, -0.1677, -0.7721,\n",
      "           0.4689,  0.0075,  0.3095, -0.0532,  0.0417, -0.0723, -0.2477,\n",
      "          -0.1390,  0.0152, -0.4781, -0.1037,  0.3562, -0.0530, -0.0988,\n",
      "          -0.3916,  0.1078, -0.2824,  0.6214,  0.1488, -0.5092, -0.2145,\n",
      "           0.5785, -0.6557,  0.4154,  0.0704, -0.3991, -0.4587,  0.0530,\n",
      "          -0.1180, -0.4564, -0.0269, -0.3093, -0.0984,  0.1358, -0.1085,\n",
      "           0.2595,  0.2709,  0.1478, -0.0487, -0.3321,  0.1488, -0.3179,\n",
      "          -0.5641, -0.2651, -0.5894, -0.1573, -0.1383,  0.0791, -0.4975,\n",
      "          -0.4222, -0.0015,  0.7805, -0.4025, -0.0766, -0.2121, -0.2159,\n",
      "           0.0385, -0.2865,  0.2753,  0.3463,  0.2149, -0.0059, -0.0379,\n",
      "           0.3289,  0.3238, -0.2671,  0.3184,  0.0020, -0.1973,  0.1056,\n",
      "          -0.1769,  0.4458,  0.3271,  0.3007, -0.2671, -0.1123,  0.0441,\n",
      "          -0.2049,  0.3221, -0.5376, -0.1167, -0.1980, -0.0720, -0.2866,\n",
      "           0.1146, -0.1070, -0.0333,  0.5055, -0.1492, -0.3308, -0.0126,\n",
      "           0.5024,  0.1049, -0.3721,  0.2281, -0.3552, -0.4607, -0.1351,\n",
      "          -0.3684,  0.1049, -0.2724,  0.5684,  0.5094, -0.2420,  0.3456,\n",
      "          -0.4041,  0.0204, -0.4161,  0.2864,  0.0069, -0.1844,  0.1912,\n",
      "           0.6331, -0.0733,  0.5331,  0.1711,  0.3642,  0.0078,  0.5193,\n",
      "          -0.0357, -0.2310, -0.5435, -0.1940, -0.0928, -0.1898, -0.2939,\n",
      "           0.0784, -0.1699, -0.3662, -0.0181, -0.4860,  0.0163,  0.0608,\n",
      "          -0.1430,  0.0425, -0.0225, -0.0720, -0.1916, -0.1912, -0.4347,\n",
      "           0.0630, -0.0095,  0.1631,  0.2182,  0.0389, -0.5576, -0.1141,\n",
      "          -0.6786, -0.1989, -0.2639,  0.1042,  0.3204,  0.3382, -0.2104,\n",
      "           0.2387,  0.3918, -0.2098, -0.5353,  0.1577, -0.3492,  0.3478,\n",
      "          -0.3936, -0.0197,  0.0127,  0.2522,  0.2785, -0.5407,  0.0679,\n",
      "           0.0998,  0.0359,  0.0217, -0.1775,  0.6623,  0.3626,  0.1302,\n",
      "          -0.5724, -0.3541, -0.1725,  0.0728, -0.4823,  0.3084, -0.0635,\n",
      "           0.2292, -0.0189, -0.1053,  0.2565, -0.0899, -0.2959, -0.0271,\n",
      "           0.3411, -0.3424, -0.1558, -0.0642,  0.5419, -0.1200,  0.2063,\n",
      "          -0.1578, -0.4256, -0.2446, -0.5326, -0.3886, -0.3557, -0.0228,\n",
      "           0.4814,  0.3196,  0.5063,  0.1191, -0.1004,  0.2666,  0.0748,\n",
      "          -0.2974, -0.4383, -0.4763, -0.1775,  0.1806,  0.2558, -0.3401,\n",
      "           0.3635,  0.1231, -0.1301]]], device='cuda:0',\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       grad_fn=<SliceBackward>)\n",
      "Decoder's Input: tensor([[1]], device='cuda:0')\n",
      "Decoder Output: tensor([[9.1629e-08, 9.4176e-08, 5.8350e-04,  ..., 1.5006e-05, 3.2877e-07,\n",
      "         7.0421e-07]], device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
      "Decoder Output: tensor([[4.5308e-08, 4.7355e-08, 1.9065e-02,  ..., 1.5049e-06, 1.7371e-06,\n",
      "         7.2217e-07]], device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
      "Decoder Output: tensor([[3.1102e-10, 3.1412e-10, 8.9124e-01,  ..., 9.9455e-09, 9.8980e-09,\n",
      "         8.9740e-09]], device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
      "Decoder Output: tensor([[1.8648e-09, 1.6038e-09, 2.8965e-02,  ..., 1.6922e-07, 8.4043e-08,\n",
      "         2.0183e-07]], device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
      "Decoder Output: tensor([[2.6685e-10, 3.0087e-10, 8.9815e-01,  ..., 3.4600e-09, 8.0961e-09,\n",
      "         1.0341e-08]], device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
      "Decoder Output: tensor([[3.7604e-09, 3.9795e-09, 3.4141e-02,  ..., 1.3909e-07, 2.1025e-07,\n",
      "         3.1572e-07]], device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
      "Decoder Output: tensor([[6.6290e-10, 8.2457e-10, 9.3820e-01,  ..., 3.9560e-09, 1.5148e-08,\n",
      "         1.6375e-08]], device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
      "Decoder Output: tensor([[6.4274e-09, 6.9971e-09, 2.1746e-02,  ..., 1.8043e-07, 2.9462e-07,\n",
      "         3.8473e-07]], device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
      "Decoder Output: tensor([[1.7361e-10, 2.9124e-10, 9.9404e-01,  ..., 7.0772e-10, 2.8925e-09,\n",
      "         3.2844e-09]], device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
      "Decoder Output: tensor([[1.8753e-09, 1.9532e-09, 1.3241e-01,  ..., 7.5849e-08, 7.1095e-08,\n",
      "         1.4069e-07]], device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
      "The tokens: tensor([787,   4,   2,   4,   2,   4,   2,   4,   2,   4], device='cuda:0')\n",
      "Bot: hello . . . . .\n",
      "Human> how are you\n",
      "The Normalized Input Sentence: how are you\n",
      "Indexed sentence: [[147, 92, 7, 2]]\n",
      "The Lengths tensor: tensor([4])\n",
      "Decoder hidden state: tensor([[[ 5.2396e-01,  9.7115e-02,  1.6255e-01, -5.1952e-01,  4.2256e-01,\n",
      "          -3.1616e-01, -2.2293e-02, -3.6821e-01, -3.0329e-01,  1.9006e-01,\n",
      "           3.4121e-01,  3.2604e-01, -2.7426e-01,  4.1328e-02,  1.8451e-01,\n",
      "          -3.7967e-01,  3.1625e-01,  2.9980e-01,  4.9747e-01, -2.0399e-01,\n",
      "           3.9946e-01,  6.1363e-01,  4.4802e-01,  5.1969e-01, -3.5027e-01,\n",
      "          -6.2952e-01, -4.1212e-01,  4.9718e-01, -2.3820e-01,  3.0488e-01,\n",
      "           2.3985e-01,  2.7807e-01, -3.4526e-01, -2.5916e-02,  4.0920e-01,\n",
      "           2.3438e-01,  6.3763e-02, -2.5975e-01,  3.9302e-01,  3.6095e-01,\n",
      "           5.7676e-01,  2.1197e-01, -3.1738e-01,  1.1802e-01,  2.1324e-01,\n",
      "          -2.1188e-01,  1.9831e-01, -4.0063e-01,  1.6282e-01, -5.4811e-01,\n",
      "           3.8186e-02,  2.6315e-01,  2.5963e-01, -2.3746e-01, -2.9077e-01,\n",
      "           4.0904e-01, -6.3016e-01, -2.3838e-01, -3.3729e-01,  3.5337e-01,\n",
      "           1.8106e-01,  7.3875e-01, -5.9494e-01, -7.1558e-02, -3.6301e-02,\n",
      "          -2.8719e-01, -1.7428e-01,  2.1672e-01,  6.5608e-02,  4.7148e-01,\n",
      "           2.1003e-01, -4.2592e-01, -2.2722e-01, -2.7585e-01, -5.2633e-01,\n",
      "          -2.9861e-01,  2.2261e-01,  2.8752e-01,  1.1184e-01,  9.5338e-02,\n",
      "           1.8788e-01,  3.6701e-01,  4.5266e-01,  6.4931e-01,  1.8097e-01,\n",
      "          -3.5183e-01, -7.9620e-02,  6.1530e-01,  4.6425e-01, -2.9026e-01,\n",
      "          -3.6795e-01, -3.5889e-01,  2.4987e-01, -3.0186e-01, -3.0560e-01,\n",
      "           1.6438e-01, -2.1907e-01,  3.9885e-01, -2.6895e-02, -2.3138e-01,\n",
      "          -8.8297e-02,  1.2931e-01, -3.2573e-01,  2.2826e-01,  5.5231e-01,\n",
      "          -2.5268e-02,  5.8848e-01,  8.7520e-02,  2.4251e-01, -3.6283e-01,\n",
      "           1.0682e-01, -8.1443e-02,  6.3727e-02, -6.2291e-01, -2.2011e-01,\n",
      "           2.7498e-01, -3.3040e-01,  7.4392e-01, -4.2811e-01,  3.1808e-01,\n",
      "           5.7229e-01,  1.7414e-01, -1.3750e-01,  1.4909e-01, -1.3845e-01,\n",
      "          -2.8441e-01, -1.7860e-01,  4.2722e-01, -7.8998e-02,  1.1564e-01,\n",
      "           1.0199e-01, -6.9278e-02,  1.6068e-01,  3.5077e-01,  4.1165e-01,\n",
      "          -8.3679e-02, -7.8380e-02,  2.6479e-01, -1.4730e-01, -6.3062e-02,\n",
      "          -1.6721e-01, -9.3051e-02,  7.2001e-01,  2.1677e-01, -1.8500e-01,\n",
      "           3.5118e-01,  4.0873e-01, -3.6695e-01,  1.0705e-01, -2.9679e-01,\n",
      "          -3.8519e-01,  8.2357e-02,  2.4657e-01, -5.3330e-02, -1.1275e-01,\n",
      "           1.5233e-01,  5.7537e-01, -3.6612e-01,  6.1073e-02,  6.2177e-01,\n",
      "          -2.4302e-01,  3.7308e-01,  1.8551e-01, -7.2502e-01,  1.1045e-01,\n",
      "           2.6737e-01,  9.1323e-02,  4.4070e-01,  1.1975e-01,  2.4702e-01,\n",
      "           5.4373e-01, -3.8182e-01, -3.5725e-01, -1.0357e-01, -3.4989e-01,\n",
      "          -3.1762e-01, -4.7040e-01, -7.4130e-01,  7.9760e-02,  8.9366e-02,\n",
      "          -4.8889e-01, -5.3930e-01,  2.5095e-02, -1.8798e-01,  3.5577e-02,\n",
      "          -1.8788e-02, -2.4277e-01,  2.1552e-01, -5.3338e-01, -3.0129e-01,\n",
      "           7.2161e-01,  8.9735e-02,  6.4314e-02, -3.5250e-01,  9.2019e-02,\n",
      "           2.5319e-01, -6.0521e-01, -7.6064e-01, -3.5128e-01, -5.0670e-01,\n",
      "          -1.3119e-01, -4.7024e-01,  2.2632e-01, -4.3674e-01, -1.7487e-01,\n",
      "           2.9306e-01, -3.6880e-01,  1.0732e-01,  7.9399e-02,  4.1472e-01,\n",
      "           8.7892e-02,  1.5293e-01, -1.6525e-01, -1.7921e-02, -2.6865e-01,\n",
      "           6.4995e-01,  4.1845e-01,  4.4629e-01,  4.1197e-01, -2.9208e-01,\n",
      "          -1.6458e-01, -7.0643e-01,  1.4484e-01, -2.1424e-01,  2.5314e-01,\n",
      "          -1.9771e-01,  1.2926e-01, -5.3346e-01,  4.0904e-01,  2.9997e-01,\n",
      "           5.3485e-01,  6.4648e-01,  5.4415e-02, -2.4460e-01,  1.4054e-01,\n",
      "          -4.9690e-02, -3.3069e-01,  2.4264e-01,  6.5265e-02,  2.6250e-01,\n",
      "           7.4638e-02, -5.3001e-02,  4.4536e-01, -1.5533e-02, -5.0614e-01,\n",
      "           6.5254e-01,  2.8744e-01, -7.1179e-02,  7.2541e-01, -1.6177e-01,\n",
      "          -2.2788e-01,  2.8020e-01, -3.1123e-01,  5.9923e-01, -2.8424e-01,\n",
      "           4.7415e-01, -4.9668e-01,  7.2234e-01, -6.7379e-02, -5.0429e-01,\n",
      "           3.3123e-01, -1.5200e-01,  1.0013e-01, -1.9517e-01, -3.1193e-01,\n",
      "           1.4240e-01,  2.9931e-01,  6.1280e-01, -6.8951e-01, -4.9909e-01,\n",
      "           1.1551e-01,  1.3488e-01,  3.1036e-01,  3.0369e-01, -2.5375e-01,\n",
      "           2.2866e-01,  4.1444e-01,  3.3443e-02, -5.0122e-01,  6.3124e-01,\n",
      "           4.6161e-01, -8.0684e-02, -1.3821e-01,  1.3872e-01,  5.7944e-01,\n",
      "           4.7088e-03,  2.8191e-01, -5.4325e-01,  4.0403e-02,  7.7173e-02,\n",
      "          -1.6601e-02,  5.5712e-01, -1.0211e-02, -6.7396e-01, -1.5262e-01,\n",
      "          -1.7412e-01, -1.4738e-01,  4.4859e-01,  1.7882e-01,  4.2308e-01,\n",
      "          -5.4420e-01,  4.4607e-01, -4.2316e-02, -4.3775e-02,  3.8542e-01,\n",
      "          -1.1142e-01, -9.5502e-02,  2.3661e-01,  4.0071e-01,  1.5303e-02,\n",
      "          -2.4897e-01,  3.4076e-01, -2.2143e-01, -5.7796e-01,  3.5695e-01,\n",
      "          -2.0879e-01, -7.5517e-01, -7.1402e-02,  6.3511e-01,  4.8890e-01,\n",
      "          -3.3973e-01, -9.3144e-02,  7.0195e-02,  4.5983e-01, -4.6073e-01,\n",
      "          -3.0563e-01, -4.3700e-02, -2.1927e-02, -4.7496e-01,  3.0269e-01,\n",
      "          -5.9881e-01,  3.9560e-02, -6.7703e-02, -3.4437e-01, -4.5193e-01,\n",
      "          -6.1317e-01,  6.7541e-01,  5.7514e-01, -1.6896e-02,  1.6661e-01,\n",
      "          -2.2492e-01,  2.9549e-01, -1.2855e-01,  3.5556e-01, -2.7603e-01,\n",
      "          -4.1183e-01,  3.8760e-01, -3.8019e-01,  5.4906e-01,  1.5977e-01,\n",
      "          -3.9027e-03,  5.0950e-01,  3.2584e-01, -3.3917e-01,  2.9777e-01,\n",
      "           2.2974e-01,  3.0285e-01, -1.8876e-01,  1.9869e-01,  9.0185e-02,\n",
      "           3.5719e-01, -3.4361e-01,  2.8745e-01,  1.0862e-01,  4.3226e-02,\n",
      "          -9.4653e-02,  4.5147e-01,  5.6883e-02, -5.2109e-01,  1.9741e-01,\n",
      "           3.9378e-01,  3.0808e-01, -5.1973e-01, -4.7818e-01, -6.1337e-01,\n",
      "          -2.2048e-02, -6.5026e-02,  3.6627e-01, -3.0182e-01,  1.3691e-01,\n",
      "          -2.7057e-01, -4.2183e-01,  3.1828e-01,  2.0526e-01, -6.9695e-01,\n",
      "          -4.5072e-01,  8.4282e-04,  2.2697e-01, -3.6248e-01,  3.2017e-02,\n",
      "          -1.1362e-01, -1.1915e-01, -4.1577e-01, -9.2073e-02, -3.2521e-01,\n",
      "          -6.5597e-01,  2.9511e-01,  5.3780e-01,  5.8627e-01,  3.6154e-01,\n",
      "           3.3209e-01,  2.0534e-01, -1.9739e-01,  6.3612e-03,  2.8470e-01,\n",
      "           5.4186e-01,  2.3007e-01, -3.4557e-02, -3.6025e-01, -1.4909e-01,\n",
      "           7.4072e-02, -3.5942e-01, -4.6531e-01,  3.2139e-01,  8.7406e-03,\n",
      "          -3.5806e-01, -3.3713e-01, -4.9846e-01,  2.5374e-01,  3.0365e-01,\n",
      "           5.7472e-01,  5.7865e-01,  9.7300e-02, -5.0626e-01,  9.8854e-02,\n",
      "          -2.0877e-01, -5.3322e-01,  5.1282e-01, -2.0223e-01, -2.0574e-01,\n",
      "          -2.0909e-01, -4.8035e-01,  1.4538e-02, -3.1196e-01,  1.1349e-02,\n",
      "           3.1625e-01,  1.9905e-01,  6.8908e-02, -5.1110e-01, -2.0467e-01,\n",
      "          -5.6794e-01, -6.7360e-03,  7.1253e-02,  9.9352e-02, -7.0060e-01,\n",
      "           1.0697e-01, -5.2416e-01, -2.1700e-01, -4.2962e-01, -2.9707e-01,\n",
      "          -2.1407e-01,  1.7215e-01,  2.2385e-01, -9.8340e-02,  4.4505e-01,\n",
      "          -3.7903e-01, -1.3983e-01,  3.0708e-01, -4.2181e-01,  5.2100e-01,\n",
      "          -1.6160e-01, -5.6883e-01,  5.2685e-01, -1.7305e-01,  7.1160e-02,\n",
      "           4.9089e-01,  5.0015e-01,  4.3403e-02, -1.0537e-01,  8.6972e-02,\n",
      "          -2.9355e-01,  3.6341e-02, -1.4043e-01,  3.6700e-01,  4.1455e-01,\n",
      "          -2.3847e-04, -1.5675e-01, -1.3957e-01,  1.5836e-01, -2.8392e-01,\n",
      "           2.8869e-01,  4.9615e-01, -1.8846e-01,  2.8836e-01, -1.8419e-01,\n",
      "           2.5237e-01, -2.2842e-01,  3.4522e-01,  3.3761e-01, -2.0711e-01,\n",
      "           3.1667e-01, -5.2459e-01, -3.8628e-01,  1.4163e-02,  4.8810e-01,\n",
      "           3.5983e-02, -1.0563e-02, -6.0160e-01, -1.7042e-01,  1.2371e-01]],\n",
      "\n",
      "        [[ 7.5881e-01,  3.7921e-01, -8.5896e-01, -4.9599e-01,  9.1549e-01,\n",
      "          -2.2252e-01, -2.2318e-01, -5.7333e-03,  4.1054e-01, -3.8654e-01,\n",
      "          -2.3072e-01,  6.7897e-01, -1.3782e-01,  7.9273e-01,  5.0272e-01,\n",
      "          -7.9084e-01, -1.8223e-01,  4.4628e-01, -3.5014e-01, -2.2217e-01,\n",
      "          -1.4344e-01, -5.6438e-01, -4.2565e-01, -6.7843e-01, -3.3639e-01,\n",
      "          -7.0417e-01,  5.7492e-02, -5.1293e-01, -6.6943e-01,  7.3844e-01,\n",
      "           5.6842e-01, -3.7443e-01, -5.6850e-01,  2.1627e-01, -7.3692e-01,\n",
      "          -3.4559e-01,  4.6274e-01, -4.9257e-01, -5.2458e-01,  2.3207e-02,\n",
      "           1.6769e-01,  3.2252e-01,  2.4453e-01, -6.2639e-02,  4.3999e-02,\n",
      "          -1.2393e-01, -3.5640e-01,  9.8056e-02,  1.0667e-02, -3.2858e-01,\n",
      "           3.7715e-01,  8.1674e-01,  4.6993e-02,  1.0154e-01,  2.8030e-01,\n",
      "          -7.0641e-01, -1.1494e-01,  7.6484e-01, -1.9252e-01,  1.8443e-01,\n",
      "          -2.3776e-01, -3.3347e-01,  6.0093e-01,  5.7300e-01, -6.1718e-01,\n",
      "           6.1993e-01,  7.5689e-01, -5.8608e-02, -4.3517e-01, -1.8353e-01,\n",
      "           5.3965e-01,  1.3771e-01,  2.8244e-01,  5.0159e-01, -3.3828e-01,\n",
      "          -6.0984e-01,  2.4250e-01,  1.6543e-01, -3.5785e-01, -9.4139e-02,\n",
      "          -5.5996e-01, -5.5869e-01,  2.2343e-02,  6.9660e-01,  1.7627e-01,\n",
      "          -3.2302e-01,  4.5928e-01,  9.1074e-01, -1.7509e-01,  3.6164e-02,\n",
      "           1.1556e-01,  5.2574e-01, -7.9184e-01, -5.0783e-01,  3.4224e-01,\n",
      "          -5.1606e-01, -1.3488e-01, -3.5368e-01,  3.6868e-01,  6.8771e-01,\n",
      "          -3.6285e-01, -7.9116e-03, -1.2106e-01,  2.6504e-03,  5.0778e-01,\n",
      "           3.1281e-01, -6.8831e-01,  5.8207e-01,  5.9835e-01,  6.9076e-03,\n",
      "          -4.8422e-01,  3.2181e-01, -3.9962e-01, -3.4228e-02, -1.9463e-01,\n",
      "          -4.3535e-01, -5.0689e-01,  1.6717e-01,  3.0875e-01,  4.0295e-01,\n",
      "          -9.8921e-02, -1.9149e-01,  6.0595e-01,  7.0419e-01,  2.2100e-01,\n",
      "           3.3596e-01,  5.5480e-01, -4.2733e-01,  6.8461e-01, -4.5700e-01,\n",
      "          -2.7550e-01, -2.2615e-01,  2.4470e-01, -6.2923e-01, -9.1191e-01,\n",
      "          -7.2923e-02, -7.0429e-01, -6.1523e-01,  3.7910e-01, -6.0678e-01,\n",
      "          -8.1072e-02,  2.9601e-01,  6.1416e-01,  7.7692e-01, -8.3363e-01,\n",
      "           5.5176e-01,  1.7731e-01, -4.4158e-01, -4.2560e-01,  3.7239e-01,\n",
      "           4.6223e-02,  5.8707e-01, -6.6434e-01, -8.9004e-02,  8.4303e-01,\n",
      "          -6.4359e-01,  5.4642e-03, -3.2569e-01,  6.8858e-01, -5.1205e-01,\n",
      "          -3.4015e-01,  1.4470e-02, -3.7927e-01, -9.5078e-02,  2.2451e-01,\n",
      "           5.8368e-01,  1.8068e-01,  8.0364e-01,  1.3210e-01, -4.1922e-01,\n",
      "           8.7659e-02, -5.6229e-01,  4.7013e-01, -6.8695e-02,  3.3387e-01,\n",
      "           4.6198e-01,  3.2038e-01, -4.7742e-01, -3.4084e-01, -1.8952e-02,\n",
      "           5.4475e-01,  9.3536e-02,  4.9822e-01,  5.7693e-01,  2.1030e-02,\n",
      "          -3.4087e-01, -5.0812e-01,  4.7700e-01, -1.8764e-01, -3.2376e-01,\n",
      "           1.6495e-01,  2.5281e-01,  2.7892e-01,  6.3942e-01, -6.3077e-01,\n",
      "           9.4312e-02, -1.2542e-01,  2.4447e-02, -2.1392e-01,  4.4509e-01,\n",
      "          -4.0456e-03,  8.3673e-01,  1.3694e-02,  4.2133e-01,  5.1969e-02,\n",
      "          -7.4410e-01,  1.3403e-01,  3.9741e-01, -1.9173e-01, -7.1041e-01,\n",
      "           7.7010e-01, -3.0761e-01, -7.8294e-02, -3.0373e-02,  1.3082e-01,\n",
      "          -7.8042e-01, -1.4075e-01,  2.0586e-01,  5.0251e-01,  5.3104e-01,\n",
      "           6.3019e-01,  1.3067e-01,  5.6943e-01, -2.2380e-02, -2.5571e-02,\n",
      "           1.6380e-01,  8.8545e-02,  3.2504e-01, -2.5441e-01,  1.0314e-01,\n",
      "          -3.8048e-01, -7.1003e-02, -4.9625e-01, -4.9154e-01, -4.0073e-01,\n",
      "           9.7094e-02, -1.3300e-02,  6.4797e-01,  2.6802e-01, -3.3810e-01,\n",
      "          -2.2914e-01,  1.4106e-01,  3.2567e-01,  8.4220e-02,  4.7257e-01,\n",
      "           4.5485e-01, -8.8954e-02,  2.4319e-01,  8.9185e-02,  4.9480e-01,\n",
      "          -1.9583e-01,  3.4324e-02, -6.4277e-01, -1.7684e-01, -1.6498e-01,\n",
      "          -2.6057e-01,  2.2192e-02, -3.4045e-01, -3.2017e-01,  3.7889e-01,\n",
      "           7.3670e-01,  3.4412e-02,  1.5986e-01,  2.2443e-01, -1.0405e-01,\n",
      "           5.5997e-01, -6.1604e-01, -4.9910e-04,  2.5723e-02, -8.9321e-01,\n",
      "           6.9320e-01,  3.0928e-02, -8.2075e-01,  1.3400e-01, -4.2312e-03,\n",
      "           2.6614e-01, -4.3499e-01, -1.5309e-01,  8.0197e-02,  5.7921e-01,\n",
      "          -2.8679e-02, -1.5579e-01,  8.1305e-01, -1.4645e-02,  6.7789e-02,\n",
      "           5.3514e-02,  3.5450e-01, -2.0132e-01,  3.5488e-01, -2.9862e-01,\n",
      "           2.1277e-01, -6.8180e-01, -7.3608e-01, -2.5296e-01,  5.3039e-01,\n",
      "           7.1819e-01, -1.4140e-01, -6.4406e-01, -1.7313e-01, -6.0591e-01,\n",
      "           7.4403e-01,  4.1915e-01,  4.1040e-01, -1.2871e-01,  7.9288e-01,\n",
      "          -5.5672e-01,  7.7981e-01,  3.8089e-01,  6.0051e-01, -2.9557e-01,\n",
      "          -8.7073e-01,  4.7770e-01,  5.3850e-01,  1.1831e-02,  1.6872e-01,\n",
      "          -2.6807e-02,  4.0040e-01, -2.1393e-01,  6.3215e-01,  2.5177e-02,\n",
      "          -5.5499e-01,  2.0624e-01,  3.5218e-01,  7.0812e-01,  3.2402e-01,\n",
      "           1.8271e-01,  1.4388e-01,  5.8005e-01,  6.4309e-01, -1.8546e-01,\n",
      "           2.8466e-01, -1.4751e-01,  1.1987e-01, -1.3329e-01, -3.4857e-02,\n",
      "           6.3349e-01,  6.0546e-01, -2.1967e-02, -7.5035e-01, -1.2199e-01,\n",
      "           2.5212e-01,  4.5879e-01, -4.0823e-01, -6.2398e-02,  6.9925e-04,\n",
      "           6.3697e-01,  6.1383e-01,  6.0859e-01,  4.3439e-01, -4.6139e-02,\n",
      "           7.3658e-02, -7.6558e-01,  3.4136e-01,  2.6730e-01,  1.3278e-01,\n",
      "          -7.4222e-01,  7.2449e-01, -4.2763e-01, -2.6556e-01, -4.0790e-01,\n",
      "          -7.0538e-02,  7.3261e-01, -5.2046e-01,  6.6832e-01,  5.4414e-01,\n",
      "          -1.9374e-01, -5.8467e-02, -6.4702e-02, -3.3378e-01, -6.5545e-01,\n",
      "          -3.9620e-01,  4.8012e-01,  9.8339e-02,  1.3511e-01, -3.5131e-01,\n",
      "           2.2721e-01,  3.8441e-01, -3.0620e-02, -7.6462e-01,  2.3875e-01,\n",
      "           5.6186e-01,  5.5934e-01,  1.8858e-01, -4.7313e-01,  3.9582e-01,\n",
      "           1.1539e-01, -6.5612e-01, -5.4747e-01,  9.6068e-02,  1.5051e-01,\n",
      "           6.1174e-01, -4.5030e-01,  6.8165e-01, -4.5443e-01,  2.7526e-01,\n",
      "           3.4402e-02,  1.6800e-01,  2.1115e-01,  9.8613e-02,  7.1354e-01,\n",
      "           7.7565e-01,  2.9832e-01,  6.3066e-01, -7.5680e-01,  5.0890e-01,\n",
      "           3.8033e-01, -2.5646e-01, -4.4344e-01,  4.1537e-01,  7.7189e-02,\n",
      "           4.8742e-01,  3.4639e-01,  2.6040e-01, -4.6976e-01,  5.7871e-01,\n",
      "          -7.4648e-01,  7.0841e-01,  2.2384e-01,  7.5436e-02, -2.0524e-01,\n",
      "          -8.7218e-02, -5.5026e-01,  2.7902e-01, -1.1895e-01, -3.6395e-01,\n",
      "          -1.3430e-01,  6.2735e-01, -7.3210e-01,  5.4348e-01,  1.7603e-01,\n",
      "           3.0842e-02,  3.5059e-01, -3.3602e-01,  1.9155e-01, -1.2800e-01,\n",
      "           4.7647e-01, -1.8502e-01,  6.7374e-01,  2.1316e-01, -4.5512e-02,\n",
      "           6.1814e-01, -6.7537e-01,  7.3461e-04, -7.2435e-01, -1.5576e-01,\n",
      "          -7.4596e-02,  3.0511e-01,  5.1742e-01,  2.2089e-01,  1.5631e-01,\n",
      "          -1.3391e-01,  5.1990e-01,  1.2153e-01,  4.6437e-01,  8.9466e-01,\n",
      "           6.3034e-01,  4.4305e-02,  4.8341e-01,  4.6835e-01,  2.5761e-01,\n",
      "           2.3509e-01,  6.4869e-01, -3.7342e-01,  1.4554e-01, -6.0337e-01,\n",
      "           2.4161e-01, -4.5714e-01,  1.5544e-01,  7.1862e-01,  8.3330e-01,\n",
      "           2.0834e-02, -1.2163e-01, -6.3094e-02, -6.4347e-01,  7.6264e-01,\n",
      "          -9.0660e-01,  1.8977e-01,  7.0280e-01,  2.4207e-01,  3.9785e-01,\n",
      "           7.4176e-01,  8.6771e-01, -5.2159e-01,  4.3959e-01,  4.9282e-01,\n",
      "          -7.1277e-01,  1.7816e-01,  4.5217e-01, -8.3654e-02,  6.4192e-01,\n",
      "           3.6219e-01,  7.4046e-01, -4.4268e-01, -4.4575e-01, -3.5781e-02,\n",
      "           5.1404e-01, -5.1044e-01,  3.8770e-01, -3.9915e-01,  3.0029e-01]]],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "Decoder's Input: tensor([[1]], device='cuda:0')\n",
      "Decoder Output: tensor([[7.8787e-07, 7.3143e-07, 3.2763e-04,  ..., 5.6283e-06, 1.5250e-07,\n",
      "         2.9884e-06]], device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
      "Decoder Output: tensor([[7.2249e-09, 6.9449e-09, 1.1231e-01,  ..., 1.7195e-08, 2.8996e-08,\n",
      "         2.1519e-07]], device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
      "Decoder Output: tensor([[7.3648e-10, 8.8344e-10, 4.9490e-01,  ..., 6.9793e-09, 4.8956e-09,\n",
      "         2.3148e-08]], device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
      "Decoder Output: tensor([[4.2466e-11, 4.6860e-11, 3.2519e-02,  ..., 4.1080e-09, 8.6747e-10,\n",
      "         4.9245e-08]], device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
      "Decoder Output: tensor([[8.7441e-10, 1.1589e-09, 6.6313e-01,  ..., 7.3737e-09, 1.9380e-08,\n",
      "         3.9711e-08]], device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
      "Decoder Output: tensor([[7.4572e-11, 9.0499e-11, 7.8074e-02,  ..., 4.0753e-09, 1.8305e-09,\n",
      "         5.8165e-08]], device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
      "Decoder Output: tensor([[1.0590e-09, 1.5411e-09, 7.5509e-01,  ..., 6.4370e-09, 2.8361e-08,\n",
      "         4.8131e-08]], device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
      "Decoder Output: tensor([[2.3230e-10, 2.2360e-10, 5.9265e-02,  ..., 7.3814e-09, 6.1980e-09,\n",
      "         1.0696e-07]], device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
      "Decoder Output: tensor([[1.3736e-09, 2.0247e-09, 9.0444e-01,  ..., 3.0462e-09, 1.9529e-08,\n",
      "         3.7410e-08]], device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
      "Decoder Output: tensor([[6.1976e-10, 4.8971e-10, 7.0803e-02,  ..., 1.1297e-08, 1.4307e-08,\n",
      "         1.4145e-07]], device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
      "The tokens: tensor([274,   4,   2,   4,   2,   4,   2,   4,   2,   4], device='cuda:0')\n",
      "Bot: fine . . . . .\n",
      "Human> what do you do\n",
      "The Normalized Input Sentence: what do you do\n",
      "Indexed sentence: [[50, 47, 7, 47, 2]]\n",
      "The Lengths tensor: tensor([5])\n",
      "Decoder hidden state: tensor([[[ 6.0963e-01, -2.3184e-02,  6.0645e-01,  6.0796e-01, -4.6622e-01,\n",
      "           1.8115e-01, -3.6710e-01,  8.5538e-02, -1.7335e-01, -1.7879e-01,\n",
      "           4.7294e-01,  1.0303e-01,  2.9332e-01, -3.5231e-01, -9.8494e-02,\n",
      "          -6.0529e-01,  9.2188e-02, -3.4899e-01, -1.2791e-01, -1.2725e-01,\n",
      "           3.7216e-01,  5.7886e-01,  1.3529e-02,  3.4110e-01,  3.2937e-01,\n",
      "          -4.0600e-01, -5.8812e-01,  5.4995e-01, -8.5547e-02,  1.0378e-01,\n",
      "          -3.8631e-01, -3.8472e-03, -3.6379e-01,  4.3742e-01,  5.9983e-01,\n",
      "           3.9480e-01,  5.9168e-01, -6.1364e-01, -3.3643e-01, -1.5123e-01,\n",
      "          -9.9817e-02, -2.0027e-02, -3.0527e-01,  3.1466e-01, -1.0681e-01,\n",
      "          -1.8325e-01, -2.8416e-01, -2.1087e-01, -2.7458e-03,  2.8751e-01,\n",
      "           7.6222e-02,  6.5541e-01, -1.9477e-02,  1.8871e-01, -3.1539e-01,\n",
      "           3.2569e-01, -5.8712e-02, -6.5867e-01, -3.7699e-01,  5.0557e-02,\n",
      "           5.6934e-01,  1.6449e-01, -1.3363e-01,  3.6704e-01,  6.8484e-01,\n",
      "          -5.3612e-01, -3.9773e-01,  5.0154e-01, -1.6692e-01,  1.6674e-01,\n",
      "           3.2648e-01, -2.2263e-01, -1.1459e-01,  5.2697e-03, -5.6061e-01,\n",
      "          -6.0258e-01, -9.7163e-03,  1.4969e-01,  4.3636e-01,  2.2140e-01,\n",
      "          -3.0011e-01, -1.0446e-03, -3.1076e-01,  2.3995e-01,  3.1796e-01,\n",
      "          -2.6687e-01, -5.9207e-02, -5.6039e-01, -2.2614e-01,  1.2136e-01,\n",
      "           5.3554e-01, -1.5738e-01, -4.6483e-03, -3.9061e-02, -4.8856e-01,\n",
      "          -3.7507e-01,  5.9277e-01, -1.5143e-01,  6.3809e-02,  6.7356e-02,\n",
      "           1.6776e-01, -9.5015e-02, -2.5721e-02,  2.5571e-01,  3.9019e-01,\n",
      "           3.4035e-01,  7.6435e-01, -4.6225e-01, -4.6741e-01,  4.9131e-01,\n",
      "           5.1469e-01, -4.6255e-01, -2.0236e-01, -3.0803e-01, -1.6825e-01,\n",
      "           5.3749e-01, -7.6261e-02,  4.8072e-01,  2.8149e-01, -2.7081e-01,\n",
      "           5.2855e-01, -2.7832e-01,  4.0137e-01,  1.8551e-01, -4.0204e-01,\n",
      "          -1.7218e-01, -6.4694e-01,  6.3323e-03,  1.1418e-01, -2.0404e-01,\n",
      "          -9.4630e-02,  2.8037e-01, -2.0071e-01,  3.4185e-02,  7.9230e-01,\n",
      "          -1.1229e-02, -5.7939e-01,  2.8439e-01, -2.0487e-01, -3.5843e-01,\n",
      "          -5.7910e-02,  1.7329e-01,  2.9090e-01,  4.0648e-01,  3.9202e-02,\n",
      "           4.2777e-02,  7.3425e-01,  4.7484e-01, -5.8799e-01,  2.1351e-01,\n",
      "           2.8707e-02,  3.6575e-01,  4.3945e-01, -6.3861e-01, -6.4883e-01,\n",
      "          -4.4199e-01, -3.2881e-01, -7.7501e-01,  1.9667e-01,  8.3791e-02,\n",
      "          -7.7776e-01,  2.5943e-01, -3.6912e-01, -7.4857e-01, -1.6698e-01,\n",
      "           4.1885e-01,  2.4650e-01, -1.7254e-01, -9.1411e-02,  3.1380e-01,\n",
      "           3.8580e-01, -6.0144e-01, -7.7020e-02,  8.7596e-02,  6.8835e-01,\n",
      "           3.2630e-01, -4.3683e-01, -7.7286e-01, -6.4205e-01, -2.8093e-01,\n",
      "          -2.6870e-01,  2.0886e-02, -5.8916e-02,  5.3719e-01,  1.4435e-01,\n",
      "          -2.9920e-01, -2.3790e-01, -7.9134e-02, -7.3609e-01,  1.6746e-01,\n",
      "           5.9663e-01,  6.1322e-02, -2.0058e-01, -4.8872e-02, -2.9596e-01,\n",
      "          -3.2896e-01, -6.8009e-01, -3.8571e-01, -3.0912e-01, -3.9772e-01,\n",
      "           6.6084e-01,  3.1599e-01,  1.8092e-01, -9.7925e-02, -2.4537e-01,\n",
      "          -1.5048e-01, -5.4802e-02,  1.6724e-01,  2.0417e-01,  4.0251e-01,\n",
      "           5.6852e-01,  9.0801e-02,  2.7955e-01, -2.2038e-01,  4.5647e-01,\n",
      "           3.4714e-01,  3.1254e-01, -1.5250e-03,  3.9668e-01, -1.3066e-02,\n",
      "          -3.2668e-01, -5.0578e-01,  2.6837e-02,  2.7880e-01,  2.0502e-01,\n",
      "           8.1985e-02,  3.1153e-01, -4.7539e-01,  4.0305e-01,  7.4534e-01,\n",
      "           2.7486e-01,  2.4980e-01,  4.7477e-01,  7.1688e-01,  3.8632e-01,\n",
      "          -3.7466e-01,  2.7832e-01,  5.5131e-02, -2.9814e-01,  3.2005e-02,\n",
      "           4.7615e-01,  4.5313e-01,  4.5416e-01,  4.7573e-01, -5.9582e-01,\n",
      "          -6.6244e-01, -8.2795e-02, -2.1263e-01,  7.2678e-01, -1.7472e-01,\n",
      "           2.5887e-02, -1.3311e-01,  4.8992e-02,  5.9642e-01, -3.6024e-01,\n",
      "           3.9414e-01, -2.7639e-01,  6.4189e-01,  2.3641e-01, -3.0688e-01,\n",
      "           6.4991e-01,  9.0926e-02,  1.9559e-01, -7.4103e-01,  3.5100e-01,\n",
      "           4.3143e-02,  3.5459e-01,  3.3262e-01, -7.0016e-01, -6.6658e-01,\n",
      "           6.9711e-02,  5.8020e-01, -9.3860e-02,  4.5908e-02, -3.8982e-01,\n",
      "          -5.5122e-01, -1.6334e-01, -5.3088e-01, -4.4022e-01, -1.9170e-01,\n",
      "           4.7912e-01,  1.3406e-01, -8.2544e-01,  1.3651e-01,  1.7934e-01,\n",
      "          -1.9169e-02,  6.1986e-02, -5.6237e-01, -2.5523e-02,  5.0180e-01,\n",
      "           2.0157e-01,  4.7886e-01,  1.9254e-01, -3.8529e-01, -5.0512e-01,\n",
      "          -5.1876e-01, -1.2288e-01,  3.1307e-01,  5.3251e-01, -9.0208e-02,\n",
      "           2.9825e-01, -2.3957e-01, -2.8623e-01, -2.3719e-01,  2.5843e-01,\n",
      "           3.8671e-01,  2.4494e-01,  5.9890e-01,  5.4757e-01,  2.1360e-01,\n",
      "           6.1877e-01,  3.0544e-01, -1.8062e-01, -4.2444e-01, -4.2599e-01,\n",
      "          -2.4840e-01, -6.8207e-01,  3.1780e-01,  5.1141e-01,  4.8294e-01,\n",
      "           7.9563e-02,  4.4341e-01, -4.5412e-01,  1.5115e-01, -4.4804e-01,\n",
      "           7.0127e-01, -6.5457e-01,  3.2890e-01, -7.5179e-01, -7.7603e-01,\n",
      "          -5.7383e-01,  3.3689e-01, -1.0804e-01, -3.6501e-02, -1.0508e-01,\n",
      "          -7.5390e-01,  5.3184e-01, -3.1790e-01, -5.4079e-01,  2.9292e-01,\n",
      "           2.0385e-01,  5.9934e-01,  7.1714e-01,  9.5898e-02, -1.0081e-01,\n",
      "           1.7790e-01,  8.7819e-02, -2.6817e-01,  4.4672e-01, -3.7445e-01,\n",
      "          -6.2436e-01,  8.3979e-02,  3.7735e-01, -1.2384e-01, -9.3960e-02,\n",
      "           2.1356e-01,  4.1952e-01,  5.0415e-01, -2.2910e-01, -6.8071e-02,\n",
      "          -5.4617e-01, -2.2494e-01,  3.1430e-01,  2.1940e-01, -1.2458e-01,\n",
      "          -1.5342e-01,  3.4470e-01,  4.7878e-01,  2.0254e-02,  1.3928e-01,\n",
      "           3.0342e-01,  5.0289e-01, -7.8681e-01, -6.2923e-01,  2.5564e-01,\n",
      "          -2.3269e-01,  3.6997e-02, -4.3256e-01, -6.7291e-01,  2.5364e-01,\n",
      "          -1.7964e-01, -3.4509e-01, -5.4926e-01, -2.1138e-01, -1.7546e-01,\n",
      "           5.4228e-02, -4.9547e-03,  4.6850e-01, -9.8510e-02, -1.9116e-01,\n",
      "          -4.6859e-01, -1.7851e-02, -1.1595e-01,  5.4889e-02,  3.9874e-01,\n",
      "          -3.1988e-01, -6.9781e-02, -2.1239e-01, -2.8721e-01,  5.5110e-01,\n",
      "           1.8072e-01, -1.4665e-01, -1.5779e-01, -1.7304e-01,  8.7731e-02,\n",
      "           3.7352e-01,  5.1928e-01, -3.6245e-01,  3.5481e-01, -1.1689e-01,\n",
      "           2.0583e-01, -3.6806e-01,  5.5222e-01,  3.9011e-01,  1.4389e-01,\n",
      "           4.0805e-01, -1.7445e-01, -6.7676e-01, -1.8307e-01,  7.5476e-01,\n",
      "           3.5194e-01,  4.8473e-01,  5.1941e-01, -3.9377e-01,  2.6250e-01,\n",
      "          -5.4670e-01,  1.6919e-01,  3.0179e-01,  1.1656e-01,  4.3683e-01,\n",
      "          -2.0357e-01, -8.3475e-02,  1.1476e-01, -2.7852e-01,  4.8667e-03,\n",
      "           1.3741e-01, -5.9829e-01,  3.0613e-01, -7.0965e-01,  3.0940e-01,\n",
      "           9.5861e-02, -2.6623e-01, -1.3350e-01, -1.6147e-02, -5.8875e-01,\n",
      "          -1.3201e-02, -6.8014e-01, -2.7133e-01, -6.0520e-01,  1.5654e-01,\n",
      "          -4.4074e-02,  4.5888e-01,  2.9592e-01,  4.8310e-01,  1.1964e-01,\n",
      "          -3.8855e-01,  3.3563e-01, -3.4862e-01, -4.1600e-02,  5.9548e-01,\n",
      "          -2.6639e-01, -7.2520e-01, -7.0829e-01,  2.8930e-03, -5.3140e-01,\n",
      "           1.7306e-01,  7.1605e-01, -1.6871e-01, -6.4536e-01,  2.0055e-01,\n",
      "          -6.7092e-01,  1.7166e-01,  3.0749e-03, -1.5348e-01, -8.3103e-03,\n",
      "          -4.4420e-02, -1.2969e-01, -8.4935e-01,  1.5635e-01, -4.1522e-01,\n",
      "          -6.2713e-02, -1.6774e-01,  4.8142e-01,  3.1747e-01, -2.8029e-02,\n",
      "           4.5015e-01, -4.2175e-02,  6.3290e-01,  3.3798e-01, -3.1122e-01,\n",
      "          -1.7550e-01, -4.3091e-01,  3.4921e-01, -2.3990e-01,  4.5730e-01,\n",
      "          -6.7744e-02, -3.5117e-01,  2.7271e-01,  3.9692e-01, -1.0309e-01]],\n",
      "\n",
      "        [[-3.3054e-01,  3.2007e-01,  2.1281e-01, -5.9154e-01,  5.2390e-01,\n",
      "          -1.8504e-01, -2.0049e-01,  4.6213e-01,  5.8200e-01, -5.4672e-02,\n",
      "           7.8626e-01,  8.1652e-01,  2.7325e-01, -3.1062e-01, -9.3193e-01,\n",
      "          -1.6585e-01, -2.3268e-01,  3.8092e-01, -1.3152e-01, -6.5181e-01,\n",
      "           3.7516e-01,  3.1944e-01, -4.8426e-01,  3.1743e-01,  6.6493e-01,\n",
      "           3.7974e-01,  2.1895e-01, -3.3104e-01, -7.5890e-01,  6.5709e-01,\n",
      "           2.9688e-01, -4.6766e-01, -3.0111e-01, -9.9702e-02,  3.9962e-01,\n",
      "          -6.8127e-01, -5.8417e-01,  1.5694e-01,  6.5694e-01,  5.1312e-01,\n",
      "          -6.6112e-01, -7.0514e-02,  4.0764e-01,  5.3795e-01, -2.0887e-01,\n",
      "          -4.9537e-01,  3.6384e-01,  1.2725e-01, -3.2809e-01, -7.9124e-01,\n",
      "           3.0024e-02,  2.3072e-01,  1.6816e-02,  9.2995e-02, -9.5085e-02,\n",
      "           1.4778e-01, -2.7452e-02, -4.3947e-01,  7.6557e-01,  7.0682e-01,\n",
      "          -2.8350e-01,  4.8774e-01, -2.8022e-01, -7.8537e-02, -6.7030e-01,\n",
      "          -6.6408e-01,  8.1458e-01, -4.1856e-01, -4.5027e-01, -1.5228e-02,\n",
      "           5.8654e-01, -7.0480e-01,  3.9731e-01,  1.0112e-01, -4.0553e-01,\n",
      "          -5.2785e-01, -1.4304e-01,  3.0649e-01,  1.3623e-01,  5.5884e-02,\n",
      "          -2.5171e-01, -3.3762e-01, -5.3947e-01, -2.8942e-01,  2.1210e-01,\n",
      "          -7.0148e-01,  2.9630e-01, -3.0230e-01, -6.7783e-01,  5.2655e-01,\n",
      "          -5.3540e-03,  5.7898e-01, -3.5983e-01, -3.4614e-01,  6.7031e-01,\n",
      "          -4.7557e-01, -2.9203e-01, -9.0494e-02, -2.4487e-01,  8.1667e-03,\n",
      "          -5.0397e-01,  1.6848e-01,  3.5467e-01,  2.0112e-01,  1.7958e-01,\n",
      "           4.5355e-02, -9.2402e-01,  2.5044e-01,  3.9196e-01,  1.3921e-01,\n",
      "          -2.6970e-01,  5.1225e-01, -8.3317e-01,  1.5359e-01, -5.6208e-01,\n",
      "           6.7925e-01, -8.0850e-01, -2.1310e-01,  2.3956e-01, -2.4858e-01,\n",
      "           8.4965e-01,  1.2485e-01, -3.4575e-01, -2.0782e-02,  6.6463e-01,\n",
      "          -4.8888e-01, -4.8866e-01,  4.4701e-01,  8.3279e-01, -5.0981e-01,\n",
      "          -5.4292e-01, -5.5122e-03,  7.4168e-01,  8.5230e-01, -7.1287e-02,\n",
      "           5.1574e-01, -2.8932e-01, -8.4856e-01,  3.8889e-01, -2.1115e-02,\n",
      "           1.8818e-01,  5.5744e-01,  6.5751e-01, -7.5215e-01,  3.3164e-01,\n",
      "          -7.8249e-01,  4.2138e-01,  2.3931e-01,  8.0677e-01, -2.5082e-01,\n",
      "          -6.5545e-01, -1.1807e-01, -9.3456e-02, -3.0710e-01,  6.7771e-01,\n",
      "          -2.7675e-01, -2.7577e-01,  5.8502e-01, -1.3751e-01, -2.0480e-01,\n",
      "          -5.3313e-01,  6.4801e-01, -9.2135e-02,  5.3248e-01, -2.9054e-01,\n",
      "          -5.2378e-01,  2.9666e-01, -1.3287e-01, -1.2863e-01, -1.3912e-01,\n",
      "           1.9134e-01, -8.4878e-01, -4.5655e-01, -6.3971e-01,  2.6987e-01,\n",
      "           2.6071e-01,  8.9188e-01,  2.8719e-02,  4.8343e-01, -7.1767e-01,\n",
      "          -1.7815e-01,  1.8048e-01,  5.7470e-01,  2.6608e-01, -2.3452e-01,\n",
      "          -5.3664e-01,  3.9704e-01,  8.1406e-01, -3.4641e-01,  1.9538e-01,\n",
      "          -2.2841e-01, -2.3558e-02,  6.2834e-01,  3.0413e-02, -5.2610e-01,\n",
      "          -7.2968e-01,  6.3632e-01, -3.7552e-01,  2.5822e-02,  5.8029e-01,\n",
      "          -5.5197e-01, -3.2274e-01,  5.9596e-01, -1.3963e-01,  3.3423e-01,\n",
      "           7.6163e-01,  9.7935e-02, -6.4398e-01,  6.4781e-01,  2.7280e-01,\n",
      "          -1.3433e-01, -3.2983e-01, -2.9858e-01,  8.4191e-02, -5.1451e-01,\n",
      "          -5.4632e-01, -5.2982e-01, -6.7252e-01,  1.7992e-01,  4.0280e-01,\n",
      "           7.8888e-01, -2.0360e-01, -3.0627e-01, -6.9534e-01,  7.2581e-01,\n",
      "          -7.2116e-01, -2.7199e-01,  1.1921e-01,  2.9828e-01,  6.2486e-01,\n",
      "           5.0000e-01, -3.2594e-01, -1.4366e-01,  6.3397e-01,  6.3488e-01,\n",
      "           7.8151e-01,  8.2862e-01,  4.1681e-01, -9.0562e-01, -1.0846e-02,\n",
      "          -3.7813e-01,  3.3320e-01, -6.5428e-02,  3.3860e-01,  5.2113e-01,\n",
      "          -8.6524e-01, -4.8727e-01, -3.1484e-01,  4.6837e-01,  1.6391e-01,\n",
      "           6.8390e-01, -8.2465e-01, -2.7480e-01,  1.0412e-01,  5.6807e-01,\n",
      "          -5.1489e-01,  3.3826e-01,  1.2034e-01,  5.9778e-02,  5.3020e-01,\n",
      "           3.7887e-01, -7.0394e-01,  4.5362e-01, -2.8249e-01, -5.8984e-01,\n",
      "           3.8625e-01,  4.8682e-01, -1.5975e-02, -3.9104e-01,  4.3234e-01,\n",
      "           5.5346e-01, -5.1786e-01, -3.7853e-01, -2.4309e-01,  3.3909e-01,\n",
      "          -7.3028e-02, -3.8193e-01,  2.9734e-01,  4.6705e-01,  5.1224e-02,\n",
      "          -4.8832e-01,  5.7081e-01, -8.1102e-01, -2.8816e-02,  6.1794e-01,\n",
      "           1.5562e-01, -1.6020e-01, -7.3748e-01,  6.4348e-01, -1.4565e-01,\n",
      "          -2.4875e-01, -3.8035e-01, -3.3500e-01,  2.4804e-01, -1.8069e-02,\n",
      "           1.7428e-01, -2.5072e-01,  1.1982e-02,  7.9603e-02, -8.2936e-02,\n",
      "          -2.1892e-01,  1.1019e-01, -6.0042e-01, -5.6924e-05, -4.6762e-02,\n",
      "          -1.2764e-01, -7.3903e-01, -4.4809e-01,  1.8697e-01, -6.4184e-01,\n",
      "          -2.5991e-01,  6.4166e-01, -3.8961e-01, -3.8586e-01, -5.0566e-01,\n",
      "          -2.6520e-01,  3.3570e-01,  9.4467e-03,  8.0753e-01, -1.9332e-01,\n",
      "          -1.9981e-01, -3.9518e-01,  9.4318e-01,  1.4622e-01, -4.2665e-01,\n",
      "           6.1213e-01, -6.6663e-01,  6.2147e-01, -3.5285e-01,  4.3770e-01,\n",
      "           4.1930e-01, -8.3222e-01, -4.4568e-01, -2.1755e-02,  1.7861e-01,\n",
      "           9.0069e-01,  6.4510e-01,  4.6366e-01,  1.0482e-01, -2.7358e-01,\n",
      "           2.9875e-01, -1.9791e-01,  5.8170e-01,  2.1576e-01, -2.9185e-02,\n",
      "          -6.8544e-01, -2.4210e-02,  7.2372e-01,  6.5718e-01, -8.1523e-01,\n",
      "          -6.2600e-01, -3.7993e-01,  9.6146e-02,  5.7606e-02, -2.4007e-01,\n",
      "          -1.5430e-01,  4.3099e-01,  1.2016e-01, -5.6103e-01, -2.6937e-01,\n",
      "           6.0582e-01,  6.8508e-01, -3.6990e-01,  8.6259e-01,  2.3388e-01,\n",
      "          -3.5214e-02, -7.9530e-01,  1.7427e-01,  2.0906e-01,  3.9320e-01,\n",
      "          -1.6516e-01,  7.6614e-01,  8.0764e-01, -3.0303e-01,  1.3723e-01,\n",
      "           7.2307e-01,  1.7380e-01, -8.8462e-01,  5.0831e-01, -7.9895e-02,\n",
      "           6.6667e-01,  2.4036e-01, -1.0697e-02, -9.0679e-02,  6.1756e-01,\n",
      "           1.2656e-01, -7.3532e-01,  8.9793e-02,  8.0363e-01,  5.6680e-01,\n",
      "          -1.3480e-01,  1.2615e-02, -6.1981e-01,  4.3999e-01, -8.8702e-01,\n",
      "          -1.4177e-01, -8.2696e-01, -9.0104e-01, -3.4283e-01, -4.2920e-01,\n",
      "          -6.9398e-01, -2.4046e-01,  3.0001e-01, -1.9192e-01,  1.2563e-01,\n",
      "           4.1964e-01, -3.6311e-01, -8.2544e-01, -6.4546e-02, -6.4668e-01,\n",
      "          -7.3478e-01,  3.2655e-01,  4.8966e-01,  1.0640e-01, -3.5440e-01,\n",
      "          -8.7981e-01, -7.2318e-01,  6.1073e-01, -2.1605e-01,  3.1135e-01,\n",
      "           7.8143e-01,  3.4840e-02, -5.2211e-01, -3.5079e-01, -2.8778e-01,\n",
      "           3.6195e-02,  5.3763e-01, -6.4170e-01,  6.4320e-01,  1.9546e-01,\n",
      "           2.4541e-01,  2.6158e-01, -8.6086e-01,  1.4409e-01, -1.3429e-02,\n",
      "          -8.0915e-01,  4.1703e-01,  5.8680e-01, -1.2780e-01, -6.2226e-01,\n",
      "           2.2563e-01,  5.0252e-01,  2.8437e-01,  8.4261e-01, -2.2298e-01,\n",
      "          -6.2181e-01,  8.0248e-01, -4.0223e-01, -1.7638e-01,  5.1443e-01,\n",
      "          -6.1801e-01,  4.2306e-01,  7.9124e-01, -3.7903e-01,  1.0120e-01,\n",
      "           7.1815e-01, -4.2810e-01,  1.0978e-02,  8.9330e-01,  3.7199e-02,\n",
      "           2.4905e-01,  4.0082e-01, -5.7281e-01, -1.1247e-01, -2.2395e-01,\n",
      "          -2.9024e-01, -7.1016e-02, -1.3341e-01,  8.2219e-01,  6.5268e-01,\n",
      "          -3.3558e-01,  4.2239e-01, -5.6397e-01, -3.7345e-01,  5.6294e-01,\n",
      "           3.8701e-01, -8.0288e-01, -7.5366e-02, -3.4766e-01,  1.8354e-01,\n",
      "           5.8708e-02,  1.6848e-01,  6.4674e-01, -3.7912e-01, -1.2106e-01,\n",
      "          -7.8653e-01, -5.4636e-01,  7.5646e-02,  7.8939e-01,  9.0230e-01,\n",
      "          -7.5251e-01,  4.0783e-01,  6.0718e-01,  1.7703e-02,  3.9069e-01,\n",
      "           5.4557e-01, -6.4649e-01, -2.9273e-01, -5.6651e-01, -4.6319e-01]]],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "Decoder's Input: tensor([[1]], device='cuda:0')\n",
      "Decoder Output: tensor([[3.7733e-08, 4.2165e-08, 5.3784e-04,  ..., 2.8930e-06, 1.8344e-07,\n",
      "         3.0798e-07]], device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
      "Decoder Output: tensor([[7.5724e-09, 7.0758e-09, 4.9618e-04,  ..., 1.4034e-08, 3.9268e-09,\n",
      "         1.0273e-07]], device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
      "Decoder Output: tensor([[1.0643e-07, 8.2680e-08, 8.7707e-04,  ..., 9.9009e-07, 1.7287e-07,\n",
      "         7.5140e-07]], device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
      "Decoder Output: tensor([[1.9703e-09, 2.1897e-09, 6.5013e-02,  ..., 3.0352e-08, 3.0272e-08,\n",
      "         7.9633e-09]], device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
      "Decoder Output: tensor([[7.5034e-10, 8.9649e-10, 8.2128e-01,  ..., 4.7447e-09, 3.1738e-09,\n",
      "         4.4919e-09]], device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
      "Decoder Output: tensor([[2.1517e-10, 2.0492e-10, 9.3125e-02,  ..., 9.5543e-09, 6.5340e-09,\n",
      "         1.8093e-08]], device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
      "Decoder Output: tensor([[5.6314e-11, 8.7253e-11, 7.5003e-01,  ..., 4.6189e-10, 1.3425e-09,\n",
      "         2.2576e-09]], device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
      "Decoder Output: tensor([[6.1469e-11, 5.2494e-11, 2.0878e-02,  ..., 1.2721e-09, 2.1497e-09,\n",
      "         9.8492e-09]], device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
      "Decoder Output: tensor([[5.2104e-12, 7.7013e-12, 9.7845e-01,  ..., 3.7753e-11, 1.2126e-10,\n",
      "         3.9125e-10]], device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
      "Decoder Output: tensor([[2.7913e-12, 2.1320e-12, 1.9542e-01,  ..., 1.1242e-10, 2.1284e-10,\n",
      "         5.3117e-10]], device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
      "The tokens: tensor([ 25, 200, 483,   4,   2,   4,   2,   4,   2,   4], device='cuda:0')\n",
      "Bot: i m sorry . . . .\n",
      "Human> q\n"
     ]
    }
   ],
   "source": [
    "searcher=GreedySearchDecoder(encoder,decoder)\n",
    "evaluateInput(encoder,decoder,searcher,voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
